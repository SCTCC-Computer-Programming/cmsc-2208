NewQuestion,MS,
QuestionText,"Which statements about learning curves are true? (Select all that apply)",
Option,1,"They show model performance as a function of training set size",
Option,1,"They help understand when more data would help",
Option,0,"They directly show feature importance",
Option,1,"They can reveal if regularization is needed with limited data",

NewQuestion,MS,
QuestionText,"Which statements about scikit-learn conventions are true? (Select all that apply)",
Option,1,"Attributes ending with _ are derived from training data",
Option,1,".fit() returns self, enabling method chaining",
Option,0,"All parameters must have trailing underscores",
Option,1,"This distinguishes fitted attributes from user-set parameters",

NewQuestion,MS,
QuestionText,"L1 regularization (Lasso) might be preferred over L2 when: (Select all that apply)",
Option,1,"You expect only a few features to be important",
Option,1,"Model interpretability is a priority",
Option,1,"You want automatic feature selection",
Option,0,"You want to use all features equally",

NewQuestion,MS,
QuestionText,"Which guidelines for Naive Bayes variant selection are mentioned? (Select all that apply)",
Option,1,"GaussianNB for continuous data",
Option,1,"BernoulliNB for binary features",
Option,1,"MultinomialNB for count data (like text)",
Option,0,"Always use GaussianNB regardless of data type",

NewQuestion,MS,
QuestionText,"For very large datasets, the text suggests: (Select all that apply)",
Option,1,"Consider solver='sag' for LogisticRegression/Ridge",
Option,1,"SGDClassifier and SGDRegressor are even more scalable",
Option,1,"Linear models scale well to large datasets",
Option,0,"Always use decision trees instead",
