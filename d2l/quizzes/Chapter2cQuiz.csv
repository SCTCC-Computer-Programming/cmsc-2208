NewQuestion,MC,
QuestionText,"What fundamental approach do decision trees use to make predictions?",
Option,0,"Learning a weighted formula that applies globally to all data",
Option,0,"Storing all training data and searching for similar examples",
Option,100,"Learning a hierarchy of if-else questions during training",
Option,0,"Calculating distances between data points",

NewQuestion,MC,
QuestionText,"In scikit-learn, which module contains DecisionTreeClassifier?",
Option,0,"sklearn.linear_model",
Option,100,"sklearn.tree",
Option,0,"sklearn.neighbors",
Option,0,"sklearn.ensemble",

NewQuestion,MC,
QuestionText,"What does the max_depth parameter control in a decision tree?",
Option,0,"The number of features the tree can use",
Option,100,"The maximum number of questions the tree can ask in sequence",
Option,0,"The number of training examples required per leaf",
Option,0,"The minimum accuracy the tree must achieve",

NewQuestion,MC,
QuestionText,"What happens when a DecisionTreeClassifier is created with default settings (no max_depth specified)?",
Option,0,"The tree depth is limited to 5",
Option,0,"The tree cannot be trained",
Option,0,"The tree depth is automatically optimized",
Option,100,"The tree grows until all leaves are pure, with no depth limit",

NewQuestion,MC,
QuestionText,"What does it mean for a leaf node to be 'pure' in a decision tree?",
Option,0,"The leaf contains exactly one training example",
Option,0,"The leaf has been pruned for better generalization",
Option,100,"All training examples in the leaf have the same outcome",
Option,0,"The leaf uses only one feature for its decision",

NewQuestion,MC,
QuestionText,"Which statement best describes feature importance in decision trees?",
Option,0,"It shows the direction of each feature's effect (positive or negative)",
Option,100,"It shows which features the tree used most for making decisions",
Option,0,"It shows which features have the highest average values",
Option,0,"It shows which features were excluded from the tree",

NewQuestion,MC,
QuestionText,"In scikit-learn, how do you access feature importance values after training a DecisionTreeClassifier?",
Option,100,"Using the .feature_importances_ attribute",
Option,0,"Using the .coef_ attribute",
Option,0,"Using the .weights_ attribute",
Option,0,"Using the .score() method",

NewQuestion,MC,
QuestionText,"What do feature importance values in a decision tree always sum to?",
Option,0,"0",
Option,100,"1.0",
Option,0,"100",
Option,0,"The number of features",

NewQuestion,MC,
QuestionText,"A decision tree achieves perfect training accuracy (1.0) but significantly lower test accuracy. What does this indicate?",
Option,0,"The model is underfitting",
Option,0,"The features are not scaled properly",
Option,100,"The model is overfitting",
Option,0,"The model needs more training data",

NewQuestion,MC,
QuestionText,"How do decision trees differ from k-Nearest Neighbors (kNN) in terms of what they learn?",
Option,0,"Trees and kNN both store all training data",
Option,0,"Trees store training data; kNN learns a formula",
Option,0,"Trees and kNN both learn weighted formulas",
Option,100,"Trees learn a hierarchy of rules; kNN stores all training data",

NewQuestion,MC,
QuestionText,"How do decision trees differ from linear models in terms of decision boundaries?",
Option,0,"Trees create curved boundaries; linear models create axis-parallel boxes",
Option,100,"Trees create axis-parallel rectangular regions; linear models create straight lines or planes",
Option,0,"Trees and linear models both create curved boundaries",
Option,0,"Trees and linear models both create axis-parallel boundaries",

NewQuestion,MC,
QuestionText,"Do decision trees require feature scaling (like standardization)?",
Option,0,"Yes, all features must be scaled to the same range",
Option,0,"Yes, but only for continuous features",
Option,100,"No, trees split on one feature at a time so scale doesn't matter",
Option,0,"Yes, but only when max_depth is not specified",

NewQuestion,MC,
QuestionText,"What typically happens to training accuracy as max_depth increases?",
Option,100,"Training accuracy increases as the tree can fit data more closely",
Option,0,"Training accuracy decreases as the tree becomes too simple",
Option,0,"Training accuracy stays constant regardless of depth",
Option,0,"Training accuracy first increases then decreases",

NewQuestion,MC,
QuestionText,"What typically happens to test accuracy as max_depth increases beyond a certain point?",
Option,0,"Test accuracy continues to increase indefinitely",
Option,0,"Test accuracy is unaffected by max_depth",
Option,100,"Test accuracy may decrease as the tree overfits the training data",
Option,0,"Test accuracy always matches training accuracy",

NewQuestion,MC,
QuestionText,"If max_depth=1, how many questions can the tree ask for any prediction?",
Option,0,"One question per feature",
Option,0,"Two questions",
Option,0,"As many as needed",
Option,100,"Exactly one question",

NewQuestion,MC,
QuestionText,"What does the number of leaves in a tree represent?",
Option,0,"The number of features used",
Option,0,"The number of training examples",
Option,100,"The number of different decision paths and final predictions possible",
Option,0,"The depth of the tree",

NewQuestion,MC,
QuestionText,"How does a decision tree handle feature interactions (like attendance AND homework_rate both being high)?",
Option,100,"It captures interactions automatically through the hierarchy of questions",
Option,0,"It requires you to manually create interaction features",
Option,0,"It cannot handle feature interactions",
Option,0,"It requires special parameters to enable interactions",

NewQuestion,MC,
QuestionText,"When comparing a tree with max_depth=1 to max_depth=5, which is more complex?",
Option,0,"max_depth=1 is more complex",
Option,0,"Complexity depends on the number of features, not depth",
Option,0,"They have the same complexity",
Option,100,"max_depth=5 is more complex",

NewQuestion,MC,
QuestionText,"What does it mean when feature importance for a feature is 0?",
Option,100,"The feature was not used anywhere in the tree",
Option,0,"The feature has negative impact on predictions",
Option,0,"The feature is the most important",
Option,0,"The feature needs to be removed from the dataset",

NewQuestion,MC,
QuestionText,"For tree-based feature importance, which type of splits tend to be more important?",
Option,0,"Splits at the bottom of the tree",
Option,0,"Splits using features with larger values",
Option,0,"All splits contribute equally",
Option,100,"Splits higher in the tree that affect more samples",

NewQuestion,MC,
QuestionText,"Which statement about single decision trees and clean synthetic data is most accurate?",
Option,0,"Single trees always perform poorly on clean data",
Option,100,"Single trees can perform well on clean data with clear patterns",
Option,0,"Single trees require ensemble methods even for clean data",
Option,0,"Clean data always requires deep trees for good performance",

NewQuestion,MC,
QuestionText,"What is the primary limitation of single decision trees on complex, noisy real-world data?",
Option,0,"They are too slow to train",
Option,100,"They tend to overfit and memorize training examples",
Option,0,"They cannot handle numeric features",
Option,0,"They require feature scaling",

NewQuestion,MC,
QuestionText,"Which advanced technique combines multiple decision trees to reduce overfitting?",
Option,100,"Random Forests or Gradient Boosting (ensemble methods)",
Option,0,"k-Nearest Neighbors",
Option,0,"Linear regression",
Option,0,"Naive Bayes",

NewQuestion,MC,
QuestionText,"During training, how does a decision tree decide which question to ask at each split?",
Option,100,"It evaluates all possible questions and picks the one that best separates the classes",
Option,0,"It randomly selects a feature and threshold",
Option,0,"It uses the feature with the highest variance",
Option,0,"It asks questions in alphabetical order by feature name",

NewQuestion,MC,
QuestionText,"What does the depth of a tree represent?",
Option,0,"The total number of questions in the tree",
Option,0,"The number of training examples",
Option,0,"The number of features used",
Option,100,"The longest path from root to any leaf (maximum questions in sequence)",

NewQuestion,M,
QuestionText,"Match each algorithm with its prediction approach.",
Choice,1,"Decision Tree",
Match,1,"Learns hierarchy of if-else questions",
Choice,2,"k-Nearest Neighbors",
Match,2,"Stores data and searches for similar examples",
Choice,3,"Linear Model (Ridge, Logistic)",
Match,3,"Learns weighted formula applied globally",

NewQuestion,M,
QuestionText,"Match each max_depth value with its typical behavior.",
Choice,1,"max_depth=1",
Match,1,"Very simple; asks only one question; may underfit complex data",
Choice,2,"max_depth=5",
Match,2,"Moderate complexity; can capture detailed patterns",
Choice,3,"max_depth=None (unlimited)",
Match,3,"Very complex; can perfectly memorize training data; prone to overfitting",
Choice,4,"Optimal max_depth",
Match,4,"Balances fitting training data with generalizing to test data",

NewQuestion,M,
QuestionText,"Match each tree characteristic with its correct description.",
Choice,1,"Tree depth",
Match,1,"Maximum number of questions asked in sequence for any prediction",
Choice,2,"Number of leaves",
Match,2,"Number of different final decision outcomes possible",
Choice,3,"Feature importance",
Match,3,"Measure of how much each feature contributed to decisions",
Choice,4,"Pure leaf",
Match,4,"A leaf where all training examples have the same outcome",

NewQuestion,M,
QuestionText,"Match each comparison with the correct pair of algorithms.",
Choice,1,"No feature scaling required",
Match,1,"Decision Trees",
Choice,2,"Shows feature importance through learned weights/coefficients",
Match,2,"Linear Models (Ridge, Logistic Regression)",
Choice,3,"Fast training but slow prediction",
Match,3,"k-Nearest Neighbors",
Choice,4,"Can capture feature interactions automatically",
Match,4,"Decision Trees",

NewQuestion,MS,
QuestionText,"Which statements about decision tree training are true? (Select all that apply)",
Option,1,"The algorithm automatically finds the best questions to ask",
Option,1,"Training builds the entire tree structure by recursive splitting",
Option,0,"You must manually specify which features to use at each split",
Option,1,"Each split is chosen to best separate the classes",

NewQuestion,MS,
QuestionText,"Which statements about max_depth are true? (Select all that apply)",
Option,1,"It is the primary parameter for controlling tree complexity",
Option,1,"Larger max_depth allows more detailed decision-making",
Option,1,"Larger max_depth increases overfitting risk",
Option,0,"max_depth must always be set to the number of features",

NewQuestion,MS,
QuestionText,"Which statements about feature importance in trees are true? (Select all that apply)",
Option,1,"Values are always between 0 and 1",
Option,1,"All importance values sum to 1.0",
Option,1,"Zero importance means the feature was not used in the tree",
Option,0,"Importance values can be negative for features with negative impact",

NewQuestion,MS,
QuestionText,"When would decision trees be a good choice? (Select all that apply)",
Option,1,"Features are on different scales and you want to avoid scaling",
Option,1,"You need to explain decisions to non-technical stakeholders",
Option,1,"You want the model to automatically find feature interactions",
Option,0,"You need the absolute best accuracy and ensemble methods are not allowed",

NewQuestion,MS,
QuestionText,"Which statements about overfitting in decision trees are true? (Select all that apply)",
Option,1,"Deep trees with no max_depth can memorize training data",
Option,1,"Perfect training accuracy with lower test accuracy suggests overfitting",
Option,1,"Limiting max_depth helps prevent overfitting",
Option,0,"Overfitting only occurs when features are not scaled",

NewQuestion,MS,
QuestionText,"Which statements comparing trees to other algorithms are true? (Select all that apply)",
Option,1,"Trees create axis-parallel decision boundaries unlike linear models",
Option,1,"Trees don't require feature scaling unlike kNN and linear models",
Option,1,"Trees learn rules while kNN stores and searches data",
Option,0,"Trees and linear models both learn global weighted formulas",

NewQuestion,MS,
QuestionText,"Which statements about tree leaves are true? (Select all that apply)",
Option,1,"Each leaf represents a final decision/prediction",
Option,1,"More leaves means more possible different predictions",
Option,1,"Pure leaves have all training examples with the same outcome",
Option,0,"Every tree must have exactly as many leaves as training examples",

NewQuestion,MS,
QuestionText,"Which statements about ensemble methods with trees are true? (Select all that apply)",
Option,1,"Random Forests combine many trees to reduce overfitting",
Option,1,"Gradient Boosting builds trees sequentially to correct mistakes",
Option,1,"Ensembles address the overfitting problem of single trees",
Option,0,"Ensembles are only used when single trees fail completely",

NewQuestion,MS,
QuestionText,"Which factors affect how important a split is in the tree? (Select all that apply)",
Option,1,"How much the split improves class separation",
Option,1,"How high in the tree the split occurs",
Option,1,"How many samples are affected by the split",
Option,0,"The alphabetical order of the feature name",
