NewQuestion,MC,
QuestionText,"What does predict_proba return?",
Option,0,"The same output as .predict() but in numeric form",
Option,0,"A single probability showing the overall model accuracy",
Option,0,"A confidence percentage from 0% to 100% for the predicted class only",
Option,100,"A probability for each class, showing how likely the model thinks each outcome is",

NewQuestion,MC,
QuestionText,"What is the shape of predict_proba output for 75 test students in a binary (2-class) problem?",
Option,0,"(75,) — one value per student",
Option,100,"(75, 2) — one row per student, one column per class",
Option,0,"(2, 75) — one row per class, one column per student",
Option,0,"(75, 1) — one row per student, one column",

NewQuestion,MC,
QuestionText,"How do you determine which column of predict_proba output corresponds to which class?",
Option,0,"The column with the higher values is always the positive class",
Option,0,"The columns are always in alphabetical order",
Option,100,"Check the classes_ attribute of the trained model",
Option,0,"The first column is always the predicted class",

NewQuestion,MC,
QuestionText,"What do the values in each row of predict_proba output always sum to?",
Option,100,"1.0",
Option,0,"0",
Option,0,"100",
Option,0,"It varies depending on the model's confidence",

NewQuestion,MC,
QuestionText,"How does .predict() relate to predict_proba?",
Option,0,".predict() returns the probability of the most likely class",
Option,100,".predict() returns whichever class has the higher probability from predict_proba",
Option,0,".predict() averages the probabilities across all classes",
Option,0,".predict() and predict_proba use completely different internal calculations",

NewQuestion,MC,
QuestionText,"What does decision_function return for a binary classifier?",
Option,0,"Two probabilities per sample, one for each class",
Option,0,"A class label for each sample",
Option,100,"A single score per sample that can be any positive or negative number",
Option,0,"A probability between 0 and 1 for the positive class",

NewQuestion,MC,
QuestionText,"In a binary classifier, what does a positive decision_function value indicate?",
Option,0,"The model is confident in its prediction",
Option,0,"The probability of the predicted class is above 0.5",
Option,0,"The prediction is correct",
Option,100,"The model favors the positive class (the second entry in classes_)",

NewQuestion,MC,
QuestionText,"What is the threshold value for decision_function that separates the two predicted classes?",
Option,0,"0.5",
Option,0,"-1.0",
Option,0,"1.0",
Option,100,"0",

NewQuestion,MC,
QuestionText,"Why does decision_function return only one value per sample in binary classification, while predict_proba returns two?",
Option,0,"Binary classification only needs one score because there is only one class to predict",
Option,100,"The two predict_proba columns always sum to 1.0, making one redundant. decision_function encodes everything in a single number.",
Option,0,"decision_function only measures confidence in the predicted class, not the other class",
Option,0,"decision_function cannot distinguish between classes, only confidence level",

NewQuestion,MC,
QuestionText,"Can you meaningfully compare decision_function scores across different models or datasets?",
Option,0,"Yes, as long as you normalize the scores to be between 0 and 1 first",
Option,0,"Yes, but only if both models are the same type (e.g., both LogisticRegression)",
Option,100,"No, the scale is arbitrary and depends on the specific model and data",
Option,0,"Yes, because the scores represent standardized confidence levels",

NewQuestion,MC,
QuestionText,"What is the value range of predict_proba output?",
Option,0,"-1 to 1",
Option,0,"0 to 100",
Option,0,"Any real number (positive or negative)",
Option,100,"0 to 1",

NewQuestion,MC,
QuestionText,"When both predict_proba and decision_function are available, which is generally easier to interpret and why?",
Option,0,"decision_function, because a single number is simpler than two columns",
Option,0,"decision_function, because larger values always mean higher accuracy",
Option,100,"predict_proba, because probabilities have a natural interpretation (e.g., 83% likely to pass)",
Option,0,"They are equally easy to interpret since both measure confidence",

NewQuestion,MC,
QuestionText,"Which scikit-learn classifier supports decision_function but does NOT support predict_proba?",
Option,100,"LinearSVC",
Option,0,"KNeighborsClassifier",
Option,0,"DecisionTreeClassifier",
Option,0,"LogisticRegression",

NewQuestion,MC,
QuestionText,"A student has P(pass) = 0.511 and a decision_function score of 0.046. Which statement best describes this prediction?",
Option,0,"The decision_function score disagrees with the predict_proba output",
Option,0,"The model is very confident this student will pass",
Option,100,"The model barely favors pass and is highly uncertain about this prediction",
Option,0,"The model is 51.1% accurate for this type of student",

NewQuestion,MC,
QuestionText,"What does it mean when predict_proba returns values near 0.5 for both classes in a binary problem?",
Option,100,"The model is uncertain and the prediction could go either way",
Option,0,"The data point is an outlier that doesn't belong to either class",
Option,0,"The model is well-calibrated",
Option,0,"The model has high accuracy for this prediction",

NewQuestion,MC,
QuestionText,"In the demo, the uncertain tier (50-70% confidence) had 61.5% accuracy across 13 students. What does this tell you?",
Option,0,"The model is broken and needs to be retrained",
Option,0,"These students should be removed from the dataset",
Option,0,"The model needs more training data to improve on these cases",
Option,100,"When the model is uncertain, its predictions are barely better than guessing",

NewQuestion,MC,
QuestionText,"You are building a system to flag at-risk students for additional support. You want to minimize incorrectly flagging students who would actually pass. Which approach makes the most sense?",
Option,0,"Flag all students where .predict() returns fail, regardless of confidence",
Option,100,"Only flag students where P(fail) is above a high threshold like 0.8, accepting that some at-risk students will be missed",
Option,0,"Flag students where decision_function is closest to 0, since those are the most uncertain",
Option,0,"Lower the threshold to 0.3 so that more students are flagged for support",

NewQuestion,MC,
QuestionText,"What is calibration in the context of classifier predictions?",
Option,0,"Adjusting the decision threshold from 0.5 to a different value",
Option,100,"Whether the model's reported confidence matches how often it is actually correct",
Option,0,"The process of tuning model parameters to improve accuracy",
Option,0,"Scaling features so they have equal influence on predictions",

NewQuestion,MC,
QuestionText,"In a 3-class classification problem, what shape does predict_proba output have for 50 test samples?",
Option,100,"(50, 3) — one row per sample, one column per class",
Option,0,"(50, 2) — same as binary, the extra class is ignored",
Option,0,"(50,) — one value per sample",
Option,0,"(3, 50) — one row per class, one column per sample",

NewQuestion,MC,
QuestionText,"A model reports 90% confidence on its predictions but is only correct 70% of the time. This model is:",
Option,100,"Overconfident",
Option,0,"Well-calibrated",
Option,0,"Underfitting",
Option,0,"Underconfident",

NewQuestion,M,
QuestionText,"Match each method with its output description.",
Choice,1,"predict_proba",
Match,1,"Values between 0 and 1, one per class",
Choice,2,"decision_function",
Match,2,"Values on an arbitrary scale with no fixed bounds",
Choice,3,".predict()",
Match,3,"Class labels only, with no confidence information",
Choice,4,".score()",
Match,4,"A single accuracy number for the entire test set",


NewQuestion,M,
QuestionText,"Match each probability output with its interpretation.",
Choice,1,"P(pass) = 0.999",
Match,1,"The model is nearly certain the student will pass",
Choice,2,"P(pass) = 0.511",
Match,2,"The model barely favors pass, essentially a coin flip",
Choice,3,"P(fail) = 0.829",
Match,3,"The model is fairly confident the student will fail",
Choice,4,"P(pass) = 0.537, actual = fail",
Match,4,"A low-confidence prediction that turned out to be wrong",


NewQuestion,M,
QuestionText,"Match each term with its definition.",
Choice,1,"Positive class",
Match,1,"The second entry in classes_",
Choice,2,"Negative class",
Match,2,"The first entry in classes_",
Choice,3,"classes_ attribute",
Match,3,"Maps column indices to actual class names",
Choice,4,"Calibration",
Match,4,"Whether reported confidence matches actual accuracy",


NewQuestion,M,
QuestionText,"Match each scenario with the correct output shape.",
Choice,1,"predict_proba, binary classification",
Match,1,"(n_samples, 2)",
Choice,2,"predict_proba, 3-class classification",
Match,2,"(n_samples, 3)",
Choice,3,"decision_function, binary classification",
Match,3,"(n_samples,) — a flat array",
Choice,4,"decision_function, 3-class classification",
Match,4,"(n_samples, 3) — one score per class",


NewQuestion,MS,
QuestionText,"Which statements about predict_proba are true? (Select all that apply)",
Option,1,"Each row of the output sums to 1.0",
Option,1,"The column order matches the classes_ attribute of the trained model",
Option,1,"It is available on LogisticRegression, DecisionTreeClassifier, and KNeighborsClassifier",
Option,0,"It returns values on an arbitrary scale that depends on the model",


NewQuestion,MS,
QuestionText,"Which statements about decision_function are true? (Select all that apply)",
Option,1,"For binary classification, it returns one score per sample",
Option,1,"Positive values indicate the positive class, negative values indicate the negative class",
Option,1,"Values farther from zero indicate higher confidence",
Option,0,"The output values are always between -1 and 1",


NewQuestion,MS,
QuestionText,"In which situations is predict_proba preferable to decision_function? (Select all that apply)",
Option,1,"When you need to explain the model's confidence to a non-technical audience",
Option,1,"When you want to set a specific probability threshold for decision-making",
Option,1,"When you need to compare confidence levels across different predictions from the same model",
Option,0,"When using LinearSVC as your classifier",


NewQuestion,MS,
QuestionText,"Which statements about calibration are true? (Select all that apply)",
Option,1,"A calibrated model's 90% confidence predictions are correct about 90% of the time",
Option,1,"Some models are systematically overconfident or underconfident",
Option,0,"All scikit-learn classifiers are automatically well-calibrated",
Option,0,"Calibration measures the overall accuracy of a model",


NewQuestion,MS,
QuestionText,"What information do predict_proba and decision_function reveal that .predict() hides? (Select all that apply)",
Option,1,"How confident the model is in each prediction",
Option,1,"Whether a prediction is a borderline case or a strong case",
Option,1,"That some predictions are more trustworthy than others",
Option,0,"Which features contributed most to the prediction",


NewQuestion,MS,
QuestionText,"Which statements about multiclass uncertainty estimates are true? (Select all that apply)",
Option,1,"predict_proba returns one column per class in multiclass problems",
Option,1,"decision_function returns one score per class in multiclass problems",
Option,1,"The classes_ attribute maps column indices to class names for both methods",
Option,0,"In multiclass problems, decision_function still returns only one value per sample",


NewQuestion,MS,
QuestionText,"What influences how confident a model is about a specific prediction? (Select all that apply)",
Option,1,"How clearly the input features point toward one class",
Option,1,"Whether the features give mixed signals (e.g., high quiz score but low exam score)",
Option,1,"How similar the input is to training examples the model learned from",
Option,0,"How many total features are in the dataset",


NewQuestion,MS,
QuestionText,"Which are practical uses of uncertainty estimates from classifiers? (Select all that apply)",
Option,1,"Deciding which predictions to trust enough to act on",
Option,1,"Flagging uncertain cases for human review instead of automatic action",
Option,1,"Setting confidence thresholds to control the tradeoff between coverage and reliability",
Option,0,"Improving the model's training accuracy",

