NewQuestion,MC,
QuestionText,"Learning curves (showing model performance vs. training set size) are primarily useful for:",
Option,0,"Displaying coefficient magnitudes",
Option,0,"Showing feature importance rankings",
Option,100,"Understanding how more training data affects model performance and when regularization matters",
Option,0,"Calculating the optimal number of classes",

NewQuestion,MC,
QuestionText,"In scikit-learn, why do attributes like coef_ and intercept_ have trailing underscores?",
Option,0,"To indicate they are private variables",
Option,100,"To distinguish attributes derived from training data from user-set parameters",
Option,0,"To show they are optional parameters",
Option,0,"To mark deprecated features",

NewQuestion,MC,
QuestionText,"What information is conveyed in a coefficient magnitude plot (like Figure 2-12)?",
Option,100,"How coefficient values for each feature change under different regularization settings",
Option,0,"The accuracy of predictions for each feature",
Option,0,"The correlation between each pair of features",
Option,0,"The number of samples in each class",

NewQuestion,MC,
QuestionText,"In the Boston Housing example, LinearRegression achieved training=0.95, test=0.61, while Ridge achieved training=0.89, test=0.75. What does this tell us?",
Option,0,"LinearRegression generalizes better",
Option,0,"Both models are equally good",
Option,100,"Ridge provides better generalization despite lower training score",
Option,0,"Ridge is severely underfitting",

NewQuestion,MC,
QuestionText,"According to Figure 2-13's learning curves, what happens as the training set becomes very large?",
Option,0,"Ridge always significantly outperforms LinearRegression",
Option,100,"Ridge and LinearRegression performance converges; regularization becomes less critical",
Option,0,"Test scores always decrease",
Option,0,"Overfitting becomes more severe",

NewQuestion,MC,
QuestionText,"The alpha parameter in MultinomialNB and BernoulliNB controls smoothing by:",
Option,100,"Adding virtual data points with positive values for all features",
Option,0,"Removing outlier samples",
Option,0,"Selecting the top features only",
Option,0,"Changing from L1 to L2 penalty",

NewQuestion,MC,
QuestionText,"According to the text, you should prefer L1 (Lasso) over L2 (Ridge) when:",
Option,0,"You have very few training samples",
Option,100,"You expect only a few features to be important and want an interpretable model",
Option,0,"You want all features to have equal influence",
Option,0,"Using Naive Bayes models",

NewQuestion,MC,
QuestionText,"Linear models are particularly advantageous for:",
Option,0,"Low-dimensional dense data only",
Option,100,"High-dimensional sparse data (many zeros)",
Option,0,"Time series exclusively",
Option,0,"Data with missing values only",

NewQuestion,MC,
QuestionText,"For very large datasets (millions of samples), which technique is mentioned for faster linear model training?",
Option,100,"Using solver='sag' option in LogisticRegression and Ridge",
Option,0,"Decreasing the regularization parameter",
Option,0,"Using more features",
Option,0,"Removing all regularization",

NewQuestion,MC,
QuestionText,"What is the key statistical difference between BernoulliNB and MultinomialNB?",
Option,100,"BernoulliNB counts how often features are nonzero; MultinomialNB uses average values",
Option,0,"They use identical statistics",
Option,0,"BernoulliNB uses L1 penalty; MultinomialNB uses L2",
Option,0,"BernoulliNB is for regression; MultinomialNB is for classification",

NewQuestion,MC,
QuestionText,"The text warns that in linear models with correlated features:",
Option,0,"Coefficients are always stable and reliable",
Option,0,"Only L1 coefficients can change",
Option,100,"Coefficients can change (even flip sign) with different regularization strengths",
Option,0,"This only occurs in Naive Bayes",

NewQuestion,MC,
QuestionText,"For a LinearSVC with 5 classes and 10 features, intercept_.shape would be:",
Option,0,"(10,)",
Option,100,"(5,)",
Option,0,"(5, 10)",
Option,0,"(10, 5)",

NewQuestion,MC,
QuestionText,"In one-vs-rest multiclass classification, the final predicted class is:",
Option,100,"The class whose binary classifier has the highest confidence score",
Option,0,"The first class alphabetically",
Option,0,"Randomly selected from classes with score > 0",
Option,0,"The class with the most training samples",
