NewQuestion,MC,
QuestionText,"In a linear regression model, a prediction is computed as:",
Option,100,"Weighted sum of features plus an intercept",
Option,0,"Majority vote across neighbors",
Option,0,"Sequence of if/else splits",
Option,0,"Probability from class counts only",

NewQuestion,MC,
QuestionText,"If training score is high but test score is much lower, this most strongly suggests:",
Option,0,"Perfect generalization",
Option,0,"Underfitting",
Option,0,"Guaranteed data leakage",
Option,100,"Overfitting",

NewQuestion,MC,
QuestionText,"In Ridge regression, increasing alpha generally makes the model:",
Option,0,"More complex (less regularization)",
Option,0,"Unchanged (alpha only affects intercept)",
Option,100,"Simpler (more regularization)",
Option,0,"Switch to L1 regularization",

NewQuestion,MC,
QuestionText,"Lasso is most directly associated with:",
Option,0,"L2 penalty that shrinks coefficients without driving them to zero",
Option,100,"L1 penalty that can set some coefficients exactly to zero",
Option,0,"No regularization",
Option,0,"Tree-based splitting rules",

NewQuestion,MC,
QuestionText,"ElasticNet is best described as:",
Option,0,"Only L1 regularization",
Option,0,"Only L2 regularization",
Option,0,"A method for nearest neighbors",
Option,100,"A combination of L1 and L2 regularization",

NewQuestion,MC,
QuestionText,"For LogisticRegression and LinearSVC, increasing C generally:",
Option,100,"Decreases regularization (more complex model)",
Option,0,"Increases regularization (simpler model)",
Option,0,"Has no effect on fit",
Option,0,"Forces one-vs-one multiclass",

NewQuestion,MC,
QuestionText,"A linear classifier has a decision boundary that is:",
Option,0,"A hierarchy of if/else statements",
Option,100,"Linear in the feature space (line/plane/hyperplane)",
Option,0,"Always curved",
Option,0,"Based on counting tokens only",

NewQuestion,MC,
QuestionText,"Despite its name, LogisticRegression is used for:",
Option,0,"Regression only",
Option,0,"Clustering",
Option,100,"Classification",
Option,0,"Dimensionality reduction",

NewQuestion,MC,
QuestionText,"The one-vs-rest (OvR) approach trains:",
Option,100,"A binary classifier per class vs all other classes",
Option,0,"A classifier per feature",
Option,0,"One model that directly predicts all classes without binarization",
Option,0,"A classifier per data point",

NewQuestion,MC,
QuestionText,"In scikit-learn, method chaining after .fit() is possible primarily because .fit():",
Option,0,"Returns predictions",
Option,0,"Returns coefficients only",
Option,0,"Returns the test set",
Option,100,"Returns self",

NewQuestion,MC,
QuestionText,"For a 3-class LinearSVC with 2 features, coef_.shape is most consistent with:",
Option,0,"(2, 3)",
Option,0,"(2,)",
Option,100,"(3, 2)",
Option,0,"(3, 1)",

NewQuestion,MC,
QuestionText,"Which is a common caution when interpreting linear model coefficients?",
Option,0,"Coefficients never change with different training data",
Option,100,"Coefficients can change (even sign) with correlated features and different regularization strengths",
Option,0,"Coefficients exist only for Naive Bayes",
Option,0,"Coefficients always equal feature importances",

NewQuestion,MC,
QuestionText,"Ridge regression is most strongly associated with:",
Option,100,"L2 penalty",
Option,0,"L1 penalty",
Option,0,"No penalty",
Option,0,"Tree pruning",

NewQuestion,MC,
QuestionText,"L1 regularization tends to produce models that are:",
Option,0,"Dense (many small nonzero coefficients)",
Option,0,"Identical to KNN",
Option,0,"Impossible to train",
Option,100,"Sparse (many coefficients exactly zero)",

NewQuestion,MC,
QuestionText,"When alpha is extremely large in Ridge, you most often expect:",
Option,0,"Very large coefficients",
Option,0,"Guaranteed perfect test accuracy",
Option,100,"Coefficients shrink strongly toward zero",
Option,0,"Multiclass behavior changes to OvR",

NewQuestion,MC,
QuestionText,"A decision tree is best described as:",
Option,0,"A single linear equation",
Option,100,"A hierarchy of if/else questions leading to a decision",
Option,0,"A probability table over words",
Option,0,"A distance-based vote",

NewQuestion,MC,
QuestionText,"Gaussian Naive Bayes is most appropriate when features are:",
Option,100,"Continuous (approximately normal per class)",
Option,0,"Binary indicators only",
Option,0,"Token counts only",
Option,0,"Always categorical strings",

NewQuestion,MC,
QuestionText,"Multinomial Naive Bayes is most naturally suited to:",
Option,0,"Continuous sensor readings",
Option,0,"Binary flags only",
Option,0,"Regression targets only",
Option,100,"Count-based features (common in text)",

NewQuestion,MC,
QuestionText,"Bernoulli Naive Bayes is most naturally suited to:",
Option,0,"Continuous features only",
Option,100,"Binary features (0/1)",
Option,0,"Regression targets only",
Option,0,"Tree splits only",

NewQuestion,MC,
QuestionText,""Smoothing" in Naive Bayes primarily helps with:",
Option,0,"Removing the need for train/test splits",
Option,0,"Guaranteeing linear decision boundaries",
Option,100,"Avoiding zero probabilities for unseen events",
Option,0,"Converting regression to classification",

NewQuestion,MC,
QuestionText,"If a model performs poorly on both training and test sets, the most likely diagnosis is:",
Option,100,"Underfitting",
Option,0,"Overfitting",
Option,0,"Perfect fit",
Option,0,"OvR failure",

NewQuestion,MC,
QuestionText,"In linear models, the intercept term primarily represents:",
Option,0,"The slope for the first feature",
Option,100,"The prediction when all features are zero (baseline offset)",
Option,0,"The test accuracy",
Option,0,"The number of classes",

NewQuestion,MS,
QuestionText,"Which statements about Ridge vs Lasso are true? (Select all that apply)",
Option,1,"Ridge uses L2 regularization",
Option,1,"Lasso uses L1 regularization",
Option,1,"Lasso can set coefficients exactly to zero",
Option,0,"Ridge typically sets many coefficients exactly to zero",

NewQuestion,MS,
QuestionText,"Increasing Ridge alpha generally: (Select all that apply)",
Option,1,"Increases regularization strength",
Option,1,"Shrinks coefficients more strongly",
Option,0,"Makes the model more flexible",
Option,1,"Can reduce overfitting",

NewQuestion,MS,
QuestionText,"Increasing C in LogisticRegression / LinearSVC generally: (Select all that apply)",
Option,1,"Decreases regularization strength",
Option,1,"Can increase risk of overfitting",
Option,0,"Forces coefficients to zero",
Option,1,"Can fit the training data more closely",

NewQuestion,MS,
QuestionText,"Which are valid reasons coefficient interpretation can be misleading? (Select all that apply)",
Option,1,"Correlated features can lead to unstable coefficient estimates",
Option,1,"Coefficients can change with different regularization strengths",
Option,0,"Coefficients are always identical across data splits",
Option,1,"Sign flips can occur in some settings",

NewQuestion,MS,
QuestionText,"Which are linear classification models discussed? (Select all that apply)",
Option,1,"LogisticRegression",
Option,1,"LinearSVC",
Option,0,"DecisionTreeClassifier",
Option,0,"MultinomialNB",

NewQuestion,MS,
QuestionText,"Which statements about OvR multiclass are true? (Select all that apply)",
Option,1,"It trains one classifier per class vs the rest",
Option,1,"It produces one set of coefficients per class (in linear models)",
Option,0,"It requires a separate model per feature",
Option,0,"It is only available for regression",

NewQuestion,MS,
QuestionText,"Which scikit-learn actions are part of a standard supervised workflow? (Select all that apply)",
Option,1,".fit(X_train, y_train)",
Option,1,".predict(X_test)",
Option,1,".score(X_test, y_test)",
Option,0,".cluster(X)",

NewQuestion,MS,
QuestionText,"Which Naive Bayes variant-to-feature mappings are correct? (Select all that apply)",
Option,1,"GaussianNB → continuous features",
Option,1,"BernoulliNB → binary features",
Option,1,"MultinomialNB → count features",
Option,0,"GaussianNB → binary features only",

NewQuestion,MS,
QuestionText,"Which are true about regularization in linear models? (Select all that apply)",
Option,1,"It discourages overly large coefficients",
Option,1,"It can reduce overfitting",
Option,0,"It always improves test performance",
Option,1,"It is controlled by alpha in Ridge/Lasso/ElasticNet",

NewQuestion,MS,
QuestionText,"Which are legitimate symptoms of overfitting? (Select all that apply)",
Option,1,"Training score much higher than test score",
Option,0,"Test score much higher than training score (consistently)",
Option,1,"High variance in performance across different splits/settings",
Option,1,"Very complex model relative to data",

