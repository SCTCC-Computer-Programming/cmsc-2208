{
  "hash": "4b1288ad10d63f14e39e0f9f928f0a14",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Week 2 Demo: Classification vs Regression Targets'\nformat:\n  html:\n    toc: true\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\n---\n\n<a href=\"week-2-demo.ipynb\" download=\"week-2-demo.ipynb\">\n  Download Jupyter Notebook\n</a>\n\n\nThis page builds a tiny student-performance dataset to demonstrate the core ideas Chapter 2 introduces.\n\n## Build a Tiny Dataset\n\nThis cell creates a tiny, fake dataset so we can focus on the idea of **inputs (features)** and **outputs (targets)** without needing a real file yet. By the end of the cell, we’ll have a table (`df`) where each row represents one student.\n\n::: {#3d84e5af .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\nrng = np.random.default_rng(0)\nn = 12\n\ndf = pd.DataFrame({\n    \"attendance\": rng.integers(60, 101, n),\n    \"homework_rate\": rng.integers(50, 101, n),\n    \"quiz_avg\": rng.integers(40, 101, n),\n    \"exam_avg\": rng.integers(40, 101, n),\n})\n\n# numeric target (regression)\ndf[\"final_score\"] = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"]).round(0)\n\n# categorical target (classification)\ndf[\"pass_fail\"] = np.where(df[\"final_score\"] >= 70, \"pass\", \"fail\")\n\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attendance</th>\n      <th>homework_rate</th>\n      <th>quiz_avg</th>\n      <th>exam_avg</th>\n      <th>final_score</th>\n      <th>pass_fail</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>94</td>\n      <td>75</td>\n      <td>64</td>\n      <td>44</td>\n      <td>56.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>86</td>\n      <td>80</td>\n      <td>92</td>\n      <td>58</td>\n      <td>73.0</td>\n      <td>pass</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80</td>\n      <td>99</td>\n      <td>73</td>\n      <td>69</td>\n      <td>76.0</td>\n      <td>pass</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71</td>\n      <td>87</td>\n      <td>42</td>\n      <td>65</td>\n      <td>62.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>72</td>\n      <td>82</td>\n      <td>86</td>\n      <td>64</td>\n      <td>74.0</td>\n      <td>pass</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>61</td>\n      <td>77</td>\n      <td>84</td>\n      <td>41</td>\n      <td>61.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>63</td>\n      <td>78</td>\n      <td>91</td>\n      <td>40</td>\n      <td>63.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>60</td>\n      <td>97</td>\n      <td>50</td>\n      <td>47</td>\n      <td>58.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>67</td>\n      <td>64</td>\n      <td>45</td>\n      <td>40</td>\n      <td>46.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>93</td>\n      <td>91</td>\n      <td>92</td>\n      <td>80</td>\n      <td>86.0</td>\n      <td>pass</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>86</td>\n      <td>84</td>\n      <td>41</td>\n      <td>72</td>\n      <td>65.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>97</td>\n      <td>50</td>\n      <td>73</td>\n      <td>79</td>\n      <td>71.0</td>\n      <td>pass</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Loading the tools we need**\n\nThe first two lines load two libraries:\n\n-   **NumPy (`np`)**: used here for generating random numbers and doing simple numeric operations.\n-   **pandas (`pd`)**: used to create and display a table called a **DataFrame** (think: spreadsheet in Python).\n\n**Making the randomness repeatable**\n\nNext, the cell creates a random number generator:\n\n-   `rng` is a “random number generator” object we’ll use to create fake attendance and scores.\n-   The `0` is a **seed**, which makes the results repeatable. That means if you run the notebook again, you’ll get the same random dataset each time (useful for teaching and debugging).\n\n**Choosing how many students to generate**\n\n-   `n = 12` means we’re generating **12 rows**, so our dataset will represent 12 students.\n-   If we changed `n` to 100, we’d generate 100 students instead.\n\n**Building the DataFrame (our dataset table)**\n\nThe `DataFrame` is created with four columns that represent **student features** (inputs):\n\n-   `attendance`\n-   `homework_rate`\n-   `quiz_avg`\n-   `exam_avg`\n\nEach column is filled with random whole numbers in a chosen range. Those ranges are just meant to look realistic (for example, attendance between about 60 and 100).\n\nSo at this point:\n\n-   each **row** = one student\n-   each **column** = one measured input about that student\n\n**Creating a numeric target for regression**\n\nNext the cell creates a new column: and \\* `final_score` is calculated as a **weighted combination** of homework, quizzes, and exams. \\* This is meant to imitate how a course grade might be computed (exams count more than quizzes, etc.). \\* This column is a **number**, so it can be used as a **regression target** (predict a numeric value).\n\n**Creating a label target for classification**\n\nThen the cell creates another new column:\n\n-   `pass_fail` turns the numeric `final_score` into a **category label**: `\"pass\"` or `\"fail\"`.\n-   This column is a **classification target**, because the output must be one of a fixed set of options.\n\n**Displaying the result**\n\nThe last line tells Jupyter/Quarto to display `df`, so you can see the dataset you just created as a table.\n\nIn supervised learning, we organize our data in a table:\n\n-   Each **row** is one example (here, one student).\n-   The columns split into:\n    -   **X (features):** the input information we use to make a prediction\\\n        (`attendance`, `homework_rate`, `quiz_avg`, `exam_avg`)\n    -   **y (target/label):** the output we want to predict\n\nIn our demo, we keeping **X** the same, and we show two choices for **y**:\n\n-   `pass_fail` is a **classification** target (the model chooses a label from {pass, fail})\n-   `final_score` is a **regression** target (the model predicts a numeric value)\n\n## Review the Difference in Targets\n\nThis section zooms in on the **target column** for classification: `pass_fail`. The goal is to make the idea of a *classification target* visible: it isn’t a wide range of numbers — it’s a **small set of labels**.\n\n### Classification target (labels)\n\n#### Counting how many of each label we have\n\nThis first line counts how many students fall into each class:\n\n-   `df[\"pass_fail\"]` selects the `pass_fail` column from the DataFrame.\n\n-   `.value_counts()` counts how many times each label appears.\n\n-   The output should look like a small summary table showing something like:\n\n    -   how many students are `\"pass\"`\n    -   how many students are `\"fail\"`\n\nThis is useful because classification targets often come in **classes**, and it helps to know whether your dataset has a reasonable spread of those classes (for example, not *all* pass and *none* fail).\n\n::: {#eca0f893 .cell execution_count=2}\n``` {.python .cell-code}\ndf[\"pass_fail\"].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\npass_fail\nfail    7\npass    5\nName: count, dtype: int64\n```\n:::\n:::\n\n\n#### Making the label counts visual with a bar chart\n\nThe next cell turns those same counts into a picture.\n\n-   `import matplotlib.pyplot as plt` loads the plotting library we’ll use to draw the chart.\n\n-   `df[\"pass_fail\"].value_counts().plot(kind=\"bar\")` does two steps at once:\n\n    1.  it recomputes the counts of `\"pass\"` vs `\"fail\"`\n    2.  it plots them as a **bar chart**\n\n-   `plt.title(...)` adds a title so the chart clearly communicates what it represents.\n\n-   `plt.xlabel(\"label\")` labels the horizontal axis: the category names (`pass` and `fail`).\n\n-   `plt.ylabel(\"count\")` labels the vertical axis: how many students are in each category.\n\n-   `plt.show()` displays the plot.\n\nThe key takeaway from this plot: a **classification target** is made up of **labels** from a fixed set of options, and we can summarize it by counting how many examples fall into each label.\n\n::: {#61809bb3 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\ndf[\"pass_fail\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Classification target: pass/fail\")\nplt.xlabel(\"label\")\nplt.ylabel(\"count\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](week-2-demo_files/figure-html/cell-4-output-1.png){width=576 height=467}\n:::\n:::\n\n\nNow that we’ve looked at the classification target as label counts, we’ll look at the regression target as numeric values.\n\n### Regression target (numeric)\n\nThis next cell looks at the regression target column: `final_score`. The goal is to show that regression targets are **numeric values**, so we summarize them using numeric statistics and distributions.\n\n#### A quick numeric summary (`describe`)\n\nThe first line:\n\n-   selects the `final_score` column\n-   produces a standard summary of numeric data\n\nThe output includes common summary statistics like:\n\n-   how many values there are (`count`)\n-   the average (`mean`)\n-   spread (`std`)\n-   minimum and maximum (`min`, `max`)\n-   and percentile cutoffs (like 25%, 50%, 75%)\n\n::: {#f5ccb8d7 .cell execution_count=4}\n``` {.python .cell-code}\ndf[\"final_score\"].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\ncount    12.000000\nmean     65.916667\nstd      10.672465\nmin      46.000000\n25%      60.250000\n50%      64.000000\n75%      73.250000\nmax      86.000000\nName: final_score, dtype: float64\n```\n:::\n:::\n\n\n#### A histogram to show the distribution of scores\n\nThe next cell visualizes the same column as a histogram.\n\n-   The histogram groups numeric values into bins (ranges) and counts how many students fall into each range.\n-   `bins=8` controls how many ranges the score axis is divided into. More bins means narrower ranges; fewer bins means wider ranges.\n\nThen the plot labels make it readable:\n\n-   the title explains what you’re looking at\n-   the x-axis label (“score”) tells you the values are numeric\n-   the y-axis label (“count”) tells you we’re counting how many students fall into each score range\n\n::: {#8c53691c .cell execution_count=5}\n``` {.python .cell-code}\ndf[\"final_score\"].plot(kind=\"hist\", bins=8)\nplt.title(\"Regression target: final score\")\nplt.xlabel(\"score\")\nplt.ylabel(\"count\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](week-2-demo_files/figure-html/cell-6-output-1.png){width=589 height=449}\n:::\n:::\n\n\n## Generalization: Train vs Test Split\n\nThis section sets up the train/test split so we can check whether a model would work on new students, not just the students in our dataset. We do that by holding back some rows as a test set and using the rest as a training set. By the end of this section, you’ll have X_train, X_test, y_train, and y_test—two groups of students that let us measure generalization later.\n\n#### Step 1: Choose X (features) and y (target)\n\n::: {#b572836e .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\n# X = input features (what we use to predict)\nX = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]]\n\n# y = target (what we want to predict)\ny = df[\"pass_fail\"]\n```\n:::\n\n\n**Importing the split tool**\n\n-   `from sklearn.model_selection import train_test_split` imports a helper function from scikit-learn.\n-   We’ll use it to split our rows into a training set and a test set in a consistent way.\n\n**Defining X (features)**\n\n-   `X = df[[...]]` selects the feature columns from the DataFrame.\n\n-   We are choosing the same four inputs we’ve been using throughout the demo:\n\n    -   `attendance`\n    -   `homework_rate`\n    -   `quiz_avg`\n    -   `exam_avg`\n\nThis creates a new table `X` that contains only the input information.\n\n**Defining y (target)**\n\n-   `y = df[\"pass_fail\"]` selects the target column we want to predict.\n-   Here we are choosing **classification** as our task, so `y` is the label column (`pass` or `fail`).\n\nAt this point, we have separated the dataset into:\n\n-   **X:** what we use to predict\n-   **y:** what we want to predict\n\n#### Step 2: Split the rows into training and test sets\n\n::: {#a9a0cd98 .cell execution_count=7}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=0\n)\n\nprint(\"Total rows:\", len(df))\nprint(\"Training rows:\", len(X_train))\nprint(\"Test rows:\", len(X_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal rows: 12\nTraining rows: 9\nTest rows: 3\n```\n:::\n:::\n\n\n**Splitting X and y together**\n\n-   `train_test_split(X, y, ...)` splits the dataset into two groups of rows.\n-   We split **X and y at the same time** so that each feature row stays matched with its correct label.\n\n**This gives us four outputs:**\n\n-   `X_train`: the feature rows in the training set\n-   `X_test`: the feature rows in the test set\n-   `y_train`: the labels that go with the training rows\n-   `y_test`: the labels that go with the test rows\n\n**Choosing how large the test set is**\n\n-   `test_size=0.25` means **25% of the rows** go into the test set.\n-   With 12 total students, this will usually mean about 3 students in the test set and 9 in training.\n\n**Making the split repeatable**\n\n-   `random_state=0` makes the split repeatable.\n-   Without it, the split could change each time you run the notebook, which makes it harder to compare results.\n\n**Printing the sizes**\n\nThe `print(...)` lines show how many rows ended up in each group so you can confirm the split happened.\n\n\n\n## kNN\n\nNow that we have a **training set** and a **test set**, we can train a model on the training students and see how well it generalizes to the test students.\n\nWe’ll use **k-nearest neighbors (kNN)** for classification. kNN predicts the label for a new student by finding the **k closest** students in the training set and using a majority vote. Changing **k** changes how flexible the model is.\n\n### Step 1: Train one kNN model and check train vs test performance\n\n::: {#f9e0ecd1 .cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\nprint(\"Training accuracy:\", knn.score(X_train, y_train))\nprint(\"Test accuracy:\", knn.score(X_test, y_test))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining accuracy: 1.0\nTest accuracy: 1.0\n```\n:::\n:::\n\n\n* `KNeighborsClassifier(...)` creates a kNN classifier.\n* `n_neighbors=3` sets **k = 3**, meaning the model looks at the 3 nearest training students when it predicts a label.\n* `knn.fit(X_train, y_train)` trains the model using the training set.\n\n  * For kNN, “training” mostly means storing the training data so the model can look up neighbors later.\n* `knn.score(...)` reports accuracy: the fraction of predictions that match the true labels.\n\n  * The **training accuracy** is how well it predicts the training students.\n  * The **test accuracy** is how well it predicts the held-back test students.\n\nThis is our first concrete look at **generalization**: test accuracy is the estimate of how well this model works on new students.\n\n### Step 2: Watch what happens when we change k\n\n::: {#aa57df3a .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmax_k = len(X_train)  # k can't be bigger than the number of training rows\nks = range(1, max_k + 1)\n\ntrain_scores = []\ntest_scores = []\n\nfor k in ks:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    train_scores.append(knn.score(X_train, y_train))\n    test_scores.append(knn.score(X_test, y_test))\n\n\nresults = pd.DataFrame({\n    \"k\": list(ks),\n    \"train_acc\": train_scores,\n    \"test_acc\": test_scores\n})\nresults.round(3)\n\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>k</th>\n      <th>train_acc</th>\n      <th>test_acc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1.000</td>\n      <td>0.667</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1.000</td>\n      <td>0.667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0.667</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0.667</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0.667</td>\n      <td>0.333</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>0.667</td>\n      <td>0.333</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Importing the model**\n\n* `from sklearn.neighbors import KNeighborsClassifier` imports the kNN classification model from scikit-learn.\n* We import it inside this cell so the cell works even if it’s run on its own.\n\n**Choosing the k values we’re allowed to test**\n\n* `max_k = len(X_train)` gets the number of rows in the training set.\n* For kNN, `k` cannot be larger than the number of training examples, because the model can’t look for “10 nearest neighbors” if you only have 9 training students.\n* `ks = range(1, max_k + 1)` creates the list of k values we’ll test.\n  If `max_k` is 9, then this produces k values from 1 through 9.\n\n**Creating containers to store our results**\n\n* `train_scores = []` creates an empty list that will store training accuracy for each k.\n* `test_scores = []` creates an empty list that will store test accuracy for each k.\n\nThese lists start empty, and we’ll fill them as we loop.\n\n**Looping over k values**\n\n* `for k in ks:` starts a loop.\n  That means we will run the indented code once for each k value.\n\nInside the loop:\n\n* `knn = KNeighborsClassifier(n_neighbors=k)` creates a new kNN model using the current k value.\n* `knn.fit(X_train, y_train)` trains the model on the training set. For kNN, this mainly means the model stores the training data so it can compare new points to it later.\n* `train_scores.append(knn.score(X_train, y_train))` calculates accuracy on the training set and adds it to the `train_scores` list.\n* `test_scores.append(knn.score(X_test, y_test))` calculates accuracy on the test set and adds it to the `test_scores` list.\n\nSo after the loop finishes:\n\n* `train_scores[i]` is the training accuracy for `k = ks[i]`\n* `test_scores[i]` is the test accuracy for `k = ks[i]`\n\n**Displaying the results**\n\n*  We build a small table called `results` with three columns: `k`, `train_acc`, and `test_acc`.\n* `pd.DataFrame({...})` creates the table by lining up each k value with its training accuracy and test accuracy.\n* `results.round(3)` rounds the accuracy values to three decimals so the table is easier to read.\n* The final line displays the table so we can quickly compare training vs test accuracy for each k before plotting.\n\nAt this stage, the important thing is not the exact numbers—it’s that we now have a way to compare training performance vs test performance as k changes.\n\n\n### Step 3: Plot train vs test accuracy as k changes\n\n::: {#27a0c01b .cell execution_count=10}\n``` {.python .cell-code}\nplt.plot(list(ks), train_scores, label=\"training accuracy\")\nplt.plot(list(ks), test_scores, label=\"test accuracy\")\nplt.xlabel(\"k (number of neighbors)\")\nplt.ylabel(\"accuracy\")\nplt.title(\"kNN: training vs test accuracy as k changes\")\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](week-2-demo_files/figure-html/cell-11-output-1.png){width=589 height=449}\n:::\n:::\n\n\n**Plotting the training accuracy line**\n\n* `plt.plot(list(ks), train_scores, label=\"training accuracy\")` draws a line graph of training accuracy.\n* `list(ks)` provides the x-values (the k values we tested).\n* `train_scores` provides the y-values (the training accuracy for each k).\n* `label=\"training accuracy\"` gives this line a name so it can appear in the legend.\n\n**Plotting the test accuracy line**\n\n* `plt.plot(list(ks), test_scores, label=\"test accuracy\")` draws a second line on the same plot for test accuracy.\n* It uses the same x-values (`k`), but a different set of y-values (`test_scores`).\n* This lets us compare training and test performance at the same k values.\n\n**Labeling the axes**\n\n* `plt.xlabel(\"k (number of neighbors)\")` labels the horizontal axis so it’s clear what the x-values represent.\n* `plt.ylabel(\"accuracy\")` labels the vertical axis so it’s clear we’re measuring accuracy (from 0 to 1).\n\n**Adding a title**\n\n* `plt.title(\"kNN: training vs test accuracy as k changes\")` adds a title that summarizes what the plot is showing.\n\n**Adding a legend**\n\n* `plt.legend()` displays a legend box that matches each line to its label (“training accuracy” vs “test accuracy”).\n* Without this, you’d see two lines but wouldn’t know which is which.\n\n**Displaying the plot**\n\n* `plt.show()` tells Python to render the chart.\n\nThis plot is valuable because it shows how changing **k** changes model behavior:\n\n* When **k is small**, the model can fit the training data very closely, so training accuracy is often high.\n* As **k increases**, the model becomes less flexible, so training accuracy usually drops.\n* The test accuracy is the estimate of **generalization**, and we watch how it changes as k changes.\n\nBecause our dataset is tiny, the test line may jump around, but the key idea still holds: **k controls how complex the model is**, and training vs test accuracy helps us see overfitting vs underfitting.\n\n",
    "supporting": [
      "week-2-demo_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}