{
  "hash": "652b990048433a334230592f64ec2bbc",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Week 3 Demo: Ridge and Logistic Regression\"\nformat: \n  html:\n    toc: true\njupyter: python3\nexecute:\n  echo: true\n  warning: false\n  message: false\n  freeze: auto\n---\n\nLast week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).\n\nThis week, we're adding two completely different algorithms to our toolkit. While they work in very different ways from kNN, we'll use the same train/test framework to evaluate and tune them.\n\n## Introduction\n\n### Ridge Regression: Predictions Through Weighted Combinations\n\nRidge Regression makes predictions using a formula. Unlike kNN, which stores all the training data and looks up neighbors, Ridge learns these weights once and then uses the same formula for every prediction. This makes it fast and the predictions easy to interpret.\n\n**What makes Ridge different from kNN:**\n\n- kNN stores training data and searches through it; Ridge learns a formula\n- kNN has no \"training\" phase (just stores data); Ridge solves for optimal weights\n- kNN predictions depend on nearby points; Ridge uses the same weights for everyone\n\n**The complexity control:** Ridge has a parameter called `alpha` that controls regularization. Higher alpha forces the weights to be smaller, creating a simpler model. We'll explore how different alpha values affect performance.\n\n### Logistic Regression: Classification Through Weighted Probabilities\n\nLogistic Regression also uses weighted combinations of features, but instead of predicting a number directly, it predicts the probability of belonging to a class (like pass or fail). Despite the name \"regression,\" this is actually a classification algorithm.\n\nThe model learns weights just like Ridge, but converts the weighted sum into a probability between 0 and 1. If the probability is above 0.5, it predicts one class; otherwise, it predicts the other class.\n\n**What makes Logistic Regression different from kNN:**\n\n- kNN finds similar examples and votes; Logistic calculates a probability from a formula\n- kNN has no concept of feature importance; Logistic learns which features matter through weights\n- kNN works the same for any number of classes; Logistic is designed for binary (two-class) problems\n\n**The complexity control:** Logistic Regression has a parameter called `C` that controls regularization, but it works opposite to Ridge's alpha. Higher C means less regularization (more complex model), while higher alpha meant more regularization (simpler model).\n\n### What You'll Learn This Week\n\nBy the end of this demo, you'll be able to:\n\n- Train Ridge Regression models for predicting numeric values and interpret their weights\n- Train Logistic Regression models for predicting categories\n- Understand the relationship between regularization and model complexity\n- Compare different linear models and know when to use each one\n\n## Part 1: Ridge Regression - Basic Workflow\n\n### Understanding Ridge Regression\n\nRidge Regression is a linear model, which means it makes predictions by combining features with learned weights. The model learns the best weights during training, then uses those same weights for every prediction. \n\nThe prediction formula looks like this:\n```\npredicted_score = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b\n```\n\nEach feature gets multiplied by its weight (w₁, w₂, etc.), and then we add a constant (b, called the intercept). The model's job is to find the best values for these weights.\n\n#### What are weights?\n\nWeights tell us how much each feature contributes to the prediction. For example, if the model learns that w₃ (the weight for quiz_avg) is 0.5 and w₁ (the weight for attendance) is 0.1, it means quiz average has five times more influence on the final score than attendance does. A weight of 0 would mean that feature doesn't matter at all. Negative weights mean that as the feature increases, the prediction decreases.\n\nThink of weights like importance scores: larger absolute values (whether positive or negative) mean that feature has a bigger impact on predictions. If exam_avg has a weight of 0.8 and attendance has a weight of 0.1, the model is saying \"exam average matters a lot more than attendance for predicting final scores.\"\n\n#### What is regularization? \n\nRidge uses something called regularization, which means it prefers smaller weights. Why does this matter? When weights get very large (like [3.2, -2.8, 5.1, -4.3]), the model becomes very sensitive to small changes in the input data. A model with smaller weights (like [0.3, 0.2, 0.5, 0.4]) is more stable and less likely to be thrown off by random variation in the data. Think of it like the difference between a recipe that says \"add exactly 247 grains of salt\" versus \"add a pinch of salt.\" The first is overly precise and fragile, the second is more robust.\n\nRidge tries to keep weights small and reasonable, which helps prevent overfitting. It does this by penalizing large weights during training, forcing the model to find a solution that both fits the data well AND keeps weights modest.\n\n#### The complexity control (alpha)\n\nThe parameter `alpha` controls how large the weights in the prediction formula can be. The formula structure is always the same (a weighted sum of features), but alpha determines whether those weights can be large or must stay small.\n\nImagine you're creating a formula to predict how long it takes to commute to campus. Your formula always uses the same factors (distance, time of day, weather, route), but alpha controls how much weight each factor gets:\n\n- **With small alpha (like 0.1):** Ridge can assign large weights. Your formula might use weights like \"add 15.7 minutes per mile of distance, subtract 23.2 minutes if you leave before 7am, add 18.9 minutes if it's raining.\" These large, specific weights make your predictions very sensitive to each factor. This might fit your personal commute data perfectly, but one unusual day can throw the predictions way off.\n\n- **With large alpha (like 100):** Ridge must keep weights small. Your formula might use weights like \"add 2 minutes per mile of distance, add 5 minutes if it's rush hour, add 3 minutes if it's raining.\" These smaller weights make your predictions less sensitive to any single factor. The predictions might be less precise for your specific situation, but they are more stable and work better in new situations.\n\nWhen we create a Ridge model, we pass alpha as an argument:\n\n```python\nridge = Ridge(alpha=0.1)   # Small alpha: allows large weights\nridge = Ridge(alpha=100)   # Large alpha: forces small weights\n```\n\nThe `.fit()` method then finds the best weights it can within the constraint set by alpha. For this demo, we'll use alpha=1.0, which is scikit-learn's default and represents moderate regularization.\n\n#### What is R²?\n\nWhen we evaluate a regression model in scikit-learn, we use a method called `.score()` that returns a number called R² (R-squared). This is the metric we use to measure how well the model performs. You'll see it calculated like this in our code:\n\n```python\ntrain_r2 = ridge.score(X_train, y_train)\ntest_r2 = ridge.score(X_test, y_test)\n```\n\nR² ranges from 0 to 1 and tells us how much of the variation in the data our model explains:\n- **1.0** means perfect predictions (every prediction exactly matches the true value)\n- **0.5** means okay predictions (the model captures some patterns but misses others)\n- **0.0** means the model is no better than just guessing the average every time\n\nFor example, if we get `train_r2 = 0.85`, it means our model explains 85% of the variation in the training data. Higher R² values are better, and we want both training and test R² to be reasonably high and close together.\n\n### Step 1: Create a student dataset\n\nLet's start with a student performance dataset. We'll create data for 300 students, each with four measured features: attendance percentage, homework completion rate, quiz average, and exam average. We'll calculate a final score based on a weighted combination of these features, plus some random noise to represent the unpredictable factors we can't measure (like how much sleep a student got, or whether they were having a bad day).\n\n::: {#ae0ddb39 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Create reproducible random data\nrng = np.random.default_rng(42)\nn = 300\n\ndf = pd.DataFrame({\n    \"attendance\": rng.integers(60, 101, n),\n    \"homework_rate\": rng.integers(50, 101, n),\n    \"quiz_avg\": rng.integers(40, 101, n),\n    \"exam_avg\": rng.integers(40, 101, n),\n})\n\n# Create final score with some realistic noise\nbase_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"])\nnoise = rng.normal(0, 5, n)\ndf[\"final_score\"] = (base_score + noise).round(0)\n\n# Add classification target for Logistic Regression (coming later)\ndf[\"pass_fail\"] = np.where(df[\"final_score\"] >= 70, \"pass\", \"fail\")\n\nprint(f\"Dataset size: {len(df)} students\")\nprint(f\"Number of features: {len(df.columns) - 2}\")  # -2 for final_score and pass_fail\ndf.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset size: 300 students\nNumber of features: 4\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>attendance</th>\n      <th>homework_rate</th>\n      <th>quiz_avg</th>\n      <th>exam_avg</th>\n      <th>final_score</th>\n      <th>pass_fail</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>68</td>\n      <td>77</td>\n      <td>40</td>\n      <td>63.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>91</td>\n      <td>92</td>\n      <td>87</td>\n      <td>42</td>\n      <td>64.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>86</td>\n      <td>50</td>\n      <td>85</td>\n      <td>53</td>\n      <td>64.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>77</td>\n      <td>95</td>\n      <td>48</td>\n      <td>93</td>\n      <td>68.0</td>\n      <td>fail</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>77</td>\n      <td>77</td>\n      <td>90</td>\n      <td>67</td>\n      <td>70.0</td>\n      <td>pass</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Understanding the code:**\n\n**Importing libraries:**\n- `numpy` (abbreviated as `np`) provides tools for generating random numbers and doing numerical operations\n- `pandas` (abbreviated as `pd`) lets us work with tables of data called DataFrames\n- `matplotlib.pyplot` (abbreviated as `plt`) is for creating visualizations\n- `train_test_split` from scikit-learn will help us split data into training and test sets\n\n**Creating reproducible random data:**\n- `rng = np.random.default_rng(42)` creates a random number generator with a fixed seed (42). Using a seed means we get the same \"random\" numbers every time we run this code, making results reproducible.\n- `n = 300` sets our dataset size to 300 students\n\n**Building the DataFrame:**\n- `pd.DataFrame({...})` creates a table with four columns\n- `rng.integers(60, 101, n)` generates 300 random whole numbers between 60 and 100 (inclusive)\n- Each column represents one feature: attendance percentage, homework completion rate, quiz average, and exam average\n- After this step, we have a table where each row is one student and each column is one measurement\n\n**Creating a realistic final score:**\n- `base_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"])` calculates a weighted average where exams count for 50%, quizzes for 30%, and homework for 20%. Notice we didn't use attendance in the formula.\n- `noise = rng.normal(0, 5, n)` generates random variation from a bell curve centered at 0 with a spread of 5 points. Think of this as the unpredictable factors we can't measure: maybe a student had a headache during the exam, or got lucky on multiple choice questions, or studied extra hard one week. These small random variations make our data more like the real world.\n- `df[\"final_score\"] = (base_score + noise).round(0)` adds the random variation to the base score and rounds to whole numbers\n\n**Displaying the results:**\n- `print(f\"Dataset size: {len(df)} students\")` shows how many students we created\n- `df.head()` displays the first 5 rows of our dataset so we can see what it looks like.\n\n**Creating a classification target:**\n- `df[\"pass_fail\"] = np.where(df[\"final_score\"] >= 70, \"pass\", \"fail\")` creates a binary label where students with final_score ≥ 70 are marked as \"pass\", otherwise \"fail\"\n- We create this now because we'll use it later for Logistic Regression (classification)\n- For now, we'll focus on predicting the numeric `final_score` with Ridge Regression\n\n### Step 2: Prepare the data\n\nBefore we train any model, we need to separate our features from our target and split into training and test sets. This is the same process we used last week with kNN: we separate X (the inputs we use to make predictions) from y (what we're trying to predict), then split both into training and test portions.\n\n::: {#a3c195de .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\n\n# Features (X) and target (y)\nX = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]]\ny = df[\"final_score\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=0\n)\n\nprint(f\"Training set: {len(X_train)} students\")\nprint(f\"Test set: {len(X_test)} students\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set: 225 students\nTest set: 75 students\n```\n:::\n:::\n\n\n**Understanding the code:**\n\n**Importing the model:**\n- `from sklearn.linear_model import Ridge` imports the Ridge Regression model from scikit-learn\n\n**Separating features and target:**\n- `X = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]]` creates a table with just our input features (the four columns we'll use to make predictions). By convention, we use capital X for features.\n- `y = df[\"final_score\"]` extracts just the target column (what we want to predict). By convention, we use lowercase y for the target.\n\n**Splitting the data:**\n- `train_test_split(X, y, test_size=0.25, random_state=0)` splits our data into four pieces\n- `test_size=0.25` means 25% of the data goes to testing, 75% to training\n- `random_state=0` makes the split reproducible (same split every time)\n- `X_train, X_test, y_train, y_test` captures the four results: training features, test features, training targets, and test targets\n- With 300 students total, this gives us about 225 students for training and 75 for testing\n\n### Step 3: Train a Ridge model\n\nNow let's train a single Ridge model to see how the basic workflow works. We'll use alpha=1.0, which is scikit-learn's default value. This represents moderate regularization: not too weak (which would allow overfitting) and not too strong (which would oversimplify the model).\n\n::: {#26cc1878 .cell execution_count=3}\n``` {.python .cell-code}\n# Create and train a Ridge model\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\n# Evaluate on both training and test sets\ntrain_r2 = ridge.score(X_train, y_train)\ntest_r2 = ridge.score(X_test, y_test)\n\nprint(f\"Training R²: {train_r2:.3f}\")\nprint(f\"Test R²: {test_r2:.3f}\")\nprint(f\"\\nLearned weights: {ridge.coef_}\")\nprint(f\"Intercept: {ridge.intercept_:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining R²: 0.791\nTest R²: 0.776\n\nLearned weights: [0.03404973 0.19636421 0.2986387  0.5035881 ]\nIntercept: -2.90\n```\n:::\n:::\n\n\n**Understanding the code:**\n\n**Creating and training the model:**\n- `ridge = Ridge(alpha=1.0)` creates a Ridge model with alpha=1.0 (moderate regularization)\n- `ridge.fit(X_train, y_train)` trains the model on the training data. This is where the model learns the optimal weights.\n\n**Evaluating the model:**\n- `ridge.score(X_train, y_train)` calculates the R² score on the training data (how well does it fit the data it learned from?)\n- `ridge.score(X_test, y_test)` calculates the R² score on the test data (how well does it work on new students it has never seen?)\n\n**Examining what was learned:**\n- `ridge.coef_` shows the weights learned for each feature\n- `ridge.intercept_` shows the constant (b) added to predictions\n\n**Understanding the output:**\n\n**R² scores:**\n- `Training R²: 0.791` means the model explains 79.1% of the variation in the training data\n- `Test R²: 0.776` means the model explains 77.6% of the variation in the test data\n- Both scores are reasonably high (closer to 1.0 is better)\n- The scores are close together (difference of only 0.015), which indicates the model generalizes well\n\n**Learned weights:**\n- The four weights correspond to our four features: attendance, homework_rate, quiz_avg, and exam_avg\n- `[0.034, 0.196, 0.299, 0.504]` shows how much each feature contributes to the prediction\n- Notice these roughly match our true formula (0.2 homework, 0.3 quiz, 0.5 exam)\n- Attendance has a small weight (0.034) even though we did not use it in the formula. This happens because Ridge finds small correlations in the data.\n\n**Intercept:**\n- `-2.90` is the constant added to every prediction\n- This adjusts the baseline of the predictions to match the scale of the final scores\n\n**What this tells us:** The model is working well. It has learned weights that are close to the true relationship, and it performs similarly on both training and test data (no overfitting).\n\n## Part 2: Logistic Regression for Classification\n\n### Understanding Logistic Regression\n\nLogistic Regression, despite its name, is actually a classification algorithm, not a regression algorithm. It's called \"regression\" for historical reasons, but it predicts categories (like pass/fail) rather than continuous numbers. Like Ridge Regression, Logistic Regression is a linear model that learns weights for each feature, but it uses these weights differently to make categorical predictions.\n\n**How does it work?** Logistic Regression combines features with weights just like Ridge:\n```\nscore = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b\n```\n\nBut instead of using this score directly as a prediction, it converts the score into a probability between 0 and 1. If the probability is above 0.5, it predicts \"pass\"; otherwise, it predicts \"fail\".\n\n**What makes Logistic Regression different from Ridge:**\n- Ridge predicts numbers (regression); Logistic predicts categories (classification)\n- Ridge uses R² as its performance metric; Logistic uses accuracy (percentage of correct predictions)\n- Ridge returns the weighted sum directly; Logistic converts it to a probability first\n\n#### The complexity control (C)\n\nLogistic Regression has a parameter called `C` that controls regularization, similar to Ridge's alpha. However, C works in the opposite direction. The formula structure is always the same (a weighted sum of features), but C determines whether those weights can be large or must stay small.\n\nUsing the same driving analogy: Imagine predicting whether your commute will be \"fast\" or \"slow\" (a classification problem).\n\n**With small C (like 0.1):** Logistic must keep weights small. Your formula might use weights like \"add 2 points per mile of distance, add 3 points if it's rush hour.\" Strong regularization forces simple, stable predictions.\n\n**With large C (like 100):** Logistic can assign large weights. Your formula might use weights like \"add 15.7 points per mile of distance, subtract 23.2 points if you leave before 7am.\" Weak regularization allows complex, sensitive predictions.\n\nWhen we create a Logistic Regression model, we pass C as an argument:\n\n```python\nlogreg = LogisticRegression(C=0.1)    # Small C: strong regularization, forces small weights\nlogreg = LogisticRegression(C=100)    # Large C: weak regularization, allows large weights\n```\n\nThe `.fit()` method then finds the best weights within the constraint set by C. Notice that C works backwards from Ridge's alpha: higher C means less regularization (more complex), while higher alpha meant more regularization (simpler). For this demo, we'll use C=1.0, which is scikit-learn's default and represents moderate regularization.\n\n### Step 1: Prepare classification data\n\nWe'll use the same dataset and features as Part 1, but this time we're predicting pass/fail categories instead of numeric scores. This is the same train/test split process we've used before.\n\n::: {#3c34b3cd .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LogisticRegression\n\n# Use pass/fail as the target\ny_class = df[\"pass_fail\"]\n\n# Features stay the same\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X, y_class, test_size=0.25, random_state=0\n)\n\nprint(f\"Training set: {len(X_train_class)} students\")\nprint(f\"Test set: {len(X_test_class)} students\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set: 225 students\nTest set: 75 students\n```\n:::\n:::\n\n\n**Understanding the code:**\n\n**Importing the model:**\n- `from sklearn.linear_model import LogisticRegression` imports the Logistic Regression classifier from scikit-learn\n- Notice it's in the `linear_model` module, just like Ridge, because it's also a linear model\n\n**Selecting the classification target:**\n- `y_class = df[\"pass_fail\"]` extracts the pass/fail column as our target\n- This is categorical data (two classes: \"pass\" and \"fail\") rather than numeric data\n\n**Splitting the data:**\n- We use the same `train_test_split` function with the same parameters\n- The only difference from Part 1 is we're using `y_class` (categories) instead of `y` (numbers)\n- We add `_class` to our variable names to keep track of which data is for classification\n\n### Step 2: Train a Logistic Regression model\n\nNow let's train a single Logistic Regression model to see how the basic workflow works. We'll use C=1.0, which is scikit-learn's default value and represents moderate regularization.\n\n::: {#355868c7 .cell execution_count=5}\n``` {.python .cell-code}\n# Create and train a Logistic Regression model\nlogreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0)\nlogreg.fit(X_train_class, y_train_class)\n\n# Evaluate on both training and test sets\ntrain_acc = logreg.score(X_train_class, y_train_class)\ntest_acc = logreg.score(X_test_class, y_test_class)\n\nprint(f\"Training accuracy: {train_acc:.3f}\")\nprint(f\"Test accuracy: {test_acc:.3f}\")\nprint(f\"\\nLearned weights: {logreg.coef_[0]}\")\nprint(f\"Intercept: {logreg.intercept_[0]:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining accuracy: 0.867\nTest accuracy: 0.907\n\nLearned weights: [-0.00720698  0.05896261  0.10066204  0.19035106]\nIntercept: -24.03\n```\n:::\n:::\n\n\n**Understanding the code:**\n\n**Creating and training the model:**\n- `logreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0)` creates a Logistic Regression model\n- `C=1.0` uses the default regularization strength\n- `max_iter=1000` sets the maximum number of iterations for the optimization algorithm. The model will stop earlier if it finds the best weights, but this ensures it has enough time for difficult problems.\n- `random_state=0` ensures reproducible results\n- `logreg.fit(X_train_class, y_train_class)` trains the model on the training data. This is where the model learns the optimal weights.\n\n**Evaluating the model:**\n- `logreg.score(X_train_class, y_train_class)` calculates accuracy on the training data (what fraction of training predictions are correct?)\n- `logreg.score(X_test_class, y_test_class)` calculates accuracy on the test data (what fraction of test predictions are correct?)\n- Accuracy is simpler than R². It's just the percentage of correct predictions.\n\n**Examining what was learned:**\n- `logreg.coef_[0]` shows the weights learned for each feature (we use [0] because Logistic stores weights in a 2D array for multi-class problems)\n- `logreg.intercept_[0]` shows the constant (b) added to the score\n\n**Understanding the output:**\n\n**Accuracy scores:**\n- Training accuracy of 0.XXX means the model correctly predicted XX% of the training students\n- Test accuracy of 0.XXX means the model correctly predicted XX% of the test students\n- Both scores should be reasonably high (closer to 1.0 is better)\n- If the scores are close together, the model generalizes well\n\n**Learned weights:**\n- The four weights correspond to our four features: attendance, homework_rate, quiz_avg, and exam_avg\n- Positive weights mean higher values of that feature increase the probability of \"pass\"\n- Negative weights mean higher values of that feature increase the probability of \"fail\"\n- Larger magnitude weights (positive or negative) indicate features that have more influence on the classification\n\n**Intercept:**\n- The intercept shifts the decision boundary between \"pass\" and \"fail\"\n- It adjusts where the 0.5 probability threshold falls\n\n**What this tells us:** The model has learned a linear decision boundary that separates pass from fail based on the four features. The weights show which features matter most for the classification decision.\n\n## Part 3: Comparing Linear Models\n\nWe've now explored two different linear models for two different tasks: Ridge Regression for predicting numbers and Logistic Regression for predicting categories. Let's compare them to understand when to use each one.\n\n### Quick comparison: Linear Regression vs Ridge Regression\n\nRidge Regression is Linear Regression with regularization added. Let's see the difference side by side.\n\n::: {#3e2de9f3 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\n\n# Train both models\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\n# Compare performance\nprint(\"Linear Regression (no regularization):\")\nprint(f\"  Training R²: {lr.score(X_train, y_train):.3f}\")\nprint(f\"  Test R²: {lr.score(X_test, y_test):.3f}\")\nprint()\nprint(\"Ridge Regression (alpha=1.0):\")\nprint(f\"  Training R²: {ridge.score(X_train, y_train):.3f}\")\nprint(f\"  Test R²: {ridge.score(X_test, y_test):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression (no regularization):\n  Training R²: 0.791\n  Test R²: 0.776\n\nRidge Regression (alpha=1.0):\n  Training R²: 0.791\n  Test R²: 0.776\n```\n:::\n:::\n\n\n**Understanding the code:**\n\n**Training Linear Regression:**\n- `lr = LinearRegression()` creates a model with no regularization at all\n- It has no complexity parameter to tune (no alpha or C)\n- `lr.fit(X_train, y_train)` trains it to find the weights that best fit the training data\n\n**Training Ridge:**\n- `ridge = Ridge(alpha=1.0)` creates a model with moderate regularization\n- We use alpha=1.0 as a reasonable default value\n\n**Comparing the scores:**\n- We print both training and test R² for each model\n- Linear Regression often gets slightly higher training scores (it's not constrained)\n- Ridge often gets better test scores (regularization helps with generalization)\n\n**What the comparison shows:**\n- If both models have similar test scores, Linear Regression is simpler (no parameter to tune)\n- If Ridge has a notably better test score, the regularization is helping prevent overfitting\n- The gap between training and test scores tells us about overfitting\n\n### When to use each linear model\n\nNow that we've seen all three models in action, here's how to choose between them:\n\n| Model | Task Type | Use When | Complexity Parameter |\n|-------|-----------|----------|---------------------|\n| **Linear Regression** | Regression (predict numbers) | Dataset is small or you want the simplest possible model | None (no tuning needed) |\n| **Ridge Regression** | Regression (predict numbers) | You have many features or want to prevent overfitting | `alpha` (higher = simpler) |\n| **Logistic Regression** | Classification (predict categories) | You want a simple, interpretable classifier | `C` (higher = more complex) |\n\n**Key pattern across all three:**\n- All three are linear models (they learn weights for features)\n- All three are fast to train and make predictions\n- All three create interpretable models (you can examine the weights)\n- Ridge and Logistic both use regularization to prevent overfitting\n\n**The confusing part:** Ridge uses `alpha` where higher means more regularization, while Logistic uses `C` where higher means less regularization. They work in opposite directions, which takes practice to remember.\n\n**When you have a choice:** \n- For regression tasks, start with Ridge (it's rarely worse than Linear Regression and often better)\n- For classification tasks, Logistic Regression is usually a good first choice\n- Always use train/test scores to check if your regularization parameter is set well\n\n## Summary: Comparing Three Algorithms\n\nWe've now seen three different supervised learning algorithms across two weeks. Each has a \"complexity parameter\" that we tune using train/test performance:\n\n| Algorithm | Task | Complexity Parameter | Simpler Model | More Complex Model |\n|-----------|------|---------------------|---------------|-------------------|\n| **kNN** (Week 2) | Classification or Regression | `n_neighbors` | Large k | Small k |\n| **Ridge Regression** (Week 3) | Regression | `alpha` | Large alpha | Small alpha |\n| **Logistic Regression** (Week 3) | Classification | `C` | Small C | Large C |\n\n**Notice the pattern (and the confusing part):**\n- kNN: larger k = simpler (more neighbors = smoother decisions)\n- Ridge: larger alpha = simpler (more regularization = smaller weights)\n- Logistic: larger C = more complex (less regularization = larger weights)\n\nRidge's `alpha` and Logistic's `C` work in **opposite directions**. This takes practice to remember.\n\n**The universal pattern across all three:**\n- **Too simple** both train and test scores are low (underfitting)\n- **Too complex** train score high, test score low (overfitting)\n- **Just right** train and test scores are close and both reasonably high\n\n**How we find \"just right\":** Try different parameter values and pick the one with the best test score.\n\n### Key Takeaways\n\n1. **Train vs test scores** tell us if our model is too simple, too complex, or just right\n2. **kNN** uses `n_neighbors` where larger k means simpler model\n3. **Ridge** uses `alpha` where larger alpha means simpler model (higher = more regularization)\n4. **Logistic Regression** uses `C` where larger C means more complex model (higher = less regularization)\n5. All three algorithms use the same evaluation framework: train a model, check train score, check test score, adjust complexity\n6. Linear models (Ridge and Logistic) learn weights that show feature importance; kNN doesn't learn weights at all\n7. Always tune your complexity parameter using the **test set**, not the training set\n\n### Looking Ahead\n\nNext week (Week 4), we'll explore Decision Trees and ensemble methods like Random Forests. These algorithms work completely differently from kNN and linear models, but we'll use the same train/test framework to evaluate and tune them.\n\n",
    "supporting": [
      "week-3-demo-fixed_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}