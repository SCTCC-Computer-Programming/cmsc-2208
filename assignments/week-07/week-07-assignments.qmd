---
title: "Week 7 Assignments"
subtitle: "Textbook Chapter 2 (Uncertainty Estimates), Uncertainty Estimates Demo, and D2L Quiz"
format:
  html:
    toc: true
---

- Here is the link to the **[Week 7 demo](../../assignments/week-07/week-7-demo.html)**. 
- <a href="week-7-demo.ipynb" download="week-7-demo.ipynb"> Download Jupyter Notebook </a>

## Part 1: Reading

Read **Chapter 2, Uncertainty Estimates from Classifiers section** (pages 119-128, through the Chapter 2 Summary and Outlook).

This section introduces methods for measuring how confident a classifier is in each individual prediction. The textbook demonstrates `predict_proba` and `decision_function` using `GradientBoostingClassifier` and extends both methods to multiclass problems. The demo provides a hands-on walkthrough of both methods using `LogisticRegression` on the student performance dataset, with detailed comparisons and an analysis of how confidence connects to accuracy.

## Part 2: Work through the Week 7 Demo

The Week 7 demo covers:

1. Understanding what uncertainty estimates are and why `.predict()` alone hides important information
2. Using `predict_proba` to get probability estimates for each class
3. Understanding output shape and the `classes_` attribute for determining column order
4. How `predict_proba` connects to `.predict()` (the class with the higher probability is the prediction)
5. Using `decision_function` to get raw confidence scores for binary classification
6. Understanding positive class, negative class, and how the sign of the score determines the prediction
7. Comparing `predict_proba` and `decision_function` side by side
8. Analyzing prediction accuracy by confidence level to see which predictions are trustworthy

## Part 3: D2L Quiz

Complete the **Week 7 D2L quiz** (Uncertainty Estimates concepts).

The quiz covers:

* `predict_proba`: output shape, probability values between 0 and 1, `classes_` attribute for column order, rows sum to 1.0
* `decision_function`: single raw score for binary classification, positive/negative class terminology, threshold of 0, arbitrary scale
* Comparing methods: bounded vs unbounded values, when to use each, `LinearSVC` as a decision_function-only classifier
* Connection to `.predict()`: how both methods relate to class label predictions, thresholds
* Confidence and accuracy: high-confidence vs uncertain predictions, practical applications like flagging uncertain cases
* Calibration: what it means for a model to be calibrated, overconfident vs underconfident
* Advanced awareness: multiclass extensions, `GradientBoostingClassifier` examples, uncertainty visualizations

## D2L submission checklist

Complete the following in D2L:

* **Week 7 D2L quiz** (no file uploads required this week)

**Note:** This week focuses on reading Chapter 2 (Uncertainty Estimates section and the Chapter 2 Summary), understanding the demo, and demonstrating comprehension through the quiz. There are no screenshot or coding submissions this week. The emphasis is on understanding how classifiers express confidence in their predictions, how `predict_proba` and `decision_function` differ, and why not all predictions are equally trustworthy.
