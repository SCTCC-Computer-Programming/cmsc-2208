---
title: "Week 3 Demo: Ridge and Logistic Regression"
format: 
  html:
    toc: true
jupyter: python3
execute:
  echo: true
  warning: false
  message: false
  freeze: auto
---

## Introduction

Last week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).

This week, we're learning about **linear models**, a completely different family of algorithms. 

### What Are Linear Models?

Linear models make predictions using a formula that combines your input features. The key characteristic that makes them "linear" is that they multiply each feature by a weight and add them together. For example, if you're predicting a student's final score based on their homework, quiz, and exam grades, a linear model learns a formula like:


```
predicted_score = (w1 × homework) + (w2 × quiz) + (w3 × exam) + constant
```

During training, the model tries many different combinations of weights to see which ones make the most accurate predictions on the training data. For example, it might try:

- Attempt 1: w1=0.1, w2=0.1, w3=0.1 → predictions are terrible
- Attempt 2: w1=0.5, w2=0.3, w3=0.8 → predictions are better
- Attempt 3: w1=0.2, w2=0.3, w3=0.5 → predictions are even better

The model keeps adjusting the weights until it finds the combination that minimizes the difference between its predictions and the actual values in the training data. This process happens automatically when you call `.fit()` - you don't have to manually try different weights yourself.

In our student data example in Part 1, if exam scores actually matter most for the final grade, the model should learn that w3 (for exam) should be larger than w1 (for homework). The training process figures this out by looking at the patterns in the data.

**What makes linear models fundamentally different from kNN:**
- **kNN stores** all training data and searches through it; **linear models learn** a formula
- **kNN has no "training"** phase (just stores data); **linear models solve** for optimal weights during training
- **kNN predictions** depend on nearby examples; **linear models** use the same formula for everyone
- **kNN can't tell you** which features matter; **linear models** show importance through weights

This week, we'll explore two specific linear models: Ridge Regression (for predicting numbers) and Logistic Regression (for predicting categories).

### Ridge Regression: Predictions Through Weighted Combinations

Ridge Regression is a linear model for regression tasks (predicting numeric values like final scores). It learns weights for each feature, then uses those weights in a formula to make predictions.

**The complexity control:** Ridge has a parameter called `alpha` that controls regularization. Regularization keeps the weights from getting too large, which helps prevent overfitting. Higher alpha forces the weights to be smaller, creating a simpler model.

### Logistic Regression: Classification Through Weighted Probabilities

Logistic Regression is a linear model for classification tasks (predicting categories like pass/fail). Despite its name containing "regression," it predicts categories, not numbers.

It works similarly to Ridge by learning weights and combining features, but instead of outputting the weighted sum directly, it converts the sum into a probability between 0 and 1. If the probability is above 0.5, it predicts one class; otherwise, it predicts the other class.

**The complexity control:** Logistic Regression has a parameter called `C` that controls regularization, but it works opposite to Ridge's alpha. Higher C means less regularization (more complex model), while higher alpha meant more regularization (simpler model).

### What You'll Learn This Week

By the end of this demo, you'll be able to:
- Understand what linear models are and how they differ from kNN
- Train Ridge Regression models for predicting numeric values
- Train Logistic Regression models for predicting categories
- Interpret the learned weights to understand feature importance
- Use the same train/test evaluation framework across different algorithms

## Part 1: Ridge Regression - Basic Workflow

### Understanding Ridge Regression

As mentioned in the introduction, Ridge Regression makes predictions by combining features with learned weights. Let's explore this in detail

The prediction formula looks like this:
```
predicted_score = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b
```

Each feature gets multiplied by its weight (w₁, w₂, etc.), and then we add a constant (b, called the intercept). 

Weights tell us how much each feature contributes to the prediction. For example, if the model learns that w₃ (the weight for quiz_avg) is 0.5 and w₁ (the weight for attendance) is 0.1, it means quiz average has five times more influence on the final score than attendance does. A weight of 0 would mean that feature doesn't matter at all. Negative weights mean that as the feature increases, the prediction decreases.

Think of weights like importance scores: larger absolute values (whether positive or negative) mean that feature has a bigger impact on predictions. If exam_avg has a weight of 0.8 and attendance has a weight of 0.1, the model is saying "exam average matters a lot more than attendance for predicting final scores."

#### What is regularization? 

Ridge uses something called regularization, which means it prefers smaller weights. Why does this matter? When weights get very large (like [3.2, -2.8, 5.1, -4.3]), the model becomes very sensitive to small changes in the input data. A model with smaller weights (like [0.3, 0.2, 0.5, 0.4]) is more stable and less likely to be thrown off by random variation in the data. Think of it like the difference between a recipe that says "add exactly 247 grains of salt" versus "add a pinch of salt." The first is overly precise and fragile, the second is more robust.

Ridge tries to keep weights small and reasonable, which helps prevent overfitting. It does this by penalizing large weights during training, forcing the model to find a solution that both fits the data well AND keeps weights modest.

#### The complexity control (alpha)

The parameter `alpha` controls how large the weights in the prediction formula can be. The formula structure is always the same (a weighted sum of features), but alpha determines whether those weights can be large or must stay small.

Imagine you're creating a formula to predict how long it takes to commute to campus. Your formula always uses the same factors (distance, time of day, weather, route), but alpha controls how much weight each factor gets:

- **With small alpha (like 0.1):** Ridge can assign large weights. Your formula might use weights like "add 15.7 minutes per mile of distance, subtract 23.2 minutes if you leave before 7am, add 18.9 minutes if it's raining." These large, specific weights make your predictions very sensitive to each factor. This might fit your personal commute data perfectly, but one unusual day can throw the predictions way off.

- **With large alpha (like 100):** Ridge must keep weights small. Your formula might use weights like "add 2 minutes per mile of distance, add 5 minutes if it's rush hour, add 3 minutes if it's raining." These smaller weights make your predictions less sensitive to any single factor. The predictions might be less precise for your specific situation, but they are more stable and work better in new situations.

When we create a Ridge model, we pass alpha as an argument:

```python
ridge = Ridge(alpha=0.1)   # Small alpha: allows large weights
ridge = Ridge(alpha=100)   # Large alpha: forces small weights
```

The `.fit()` method then finds the best weights it can within the constraint set by alpha. For this demo, we'll use alpha=1.0, which is scikit-learn's default and represents moderate regularization.

#### What is R²?

When we evaluate a regression model in scikit-learn, we use a method called `.score()` that returns a number called R² (R-squared). This is the metric we use to measure how well the model performs. You'll see it calculated like this in our code:

```python
train_r2 = ridge.score(X_train, y_train)
test_r2 = ridge.score(X_test, y_test)
```

R² ranges from 0 to 1 and tells us how much of the variation in the data our model explains:
- **1.0** means perfect predictions (every prediction exactly matches the true value)
- **0.5** means okay predictions (the model captures some patterns but misses others)
- **0.0** means the model is no better than just guessing the average every time

For example, if we get `train_r2 = 0.85`, it means our model explains 85% of the variation in the training data. Higher R² values are better, and we want both training and test R² to be reasonably high and close together.

### Step 1: Create a student dataset

Let's start with a student performance dataset. We'll create data for 300 students, each with four measured features: attendance percentage, homework completion rate, quiz average, and exam average. We'll calculate a final score based on a weighted combination of these features, plus some random noise to represent the unpredictable factors we can't measure (like how much sleep a student got, or whether they were having a bad day).

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Create reproducible random data
rng = np.random.default_rng(42)
n = 300

df = pd.DataFrame({
    "attendance": rng.integers(60, 101, n),
    "homework_rate": rng.integers(50, 101, n),
    "quiz_avg": rng.integers(40, 101, n),
    "exam_avg": rng.integers(40, 101, n),
})

# Create final score with some realistic noise
base_score = (0.2*df["homework_rate"] + 0.3*df["quiz_avg"] + 0.5*df["exam_avg"])
noise = rng.normal(0, 5, n)
df["final_score"] = (base_score + noise).round(0)

# Add classification target for Logistic Regression (coming later)
df["pass_fail"] = np.where(df["final_score"] >= 70, "pass", "fail")

print(f"Dataset size: {len(df)} students")
print(f"Number of features: {len(df.columns) - 2}")  # -2 for final_score and pass_fail
df.head()
```

**Understanding the code:**

**Importing libraries:**
- `numpy` (abbreviated as `np`) provides tools for generating random numbers and doing numerical operations
- `pandas` (abbreviated as `pd`) lets us work with tables of data called DataFrames
- `matplotlib.pyplot` (abbreviated as `plt`) is for creating visualizations
- `train_test_split` from scikit-learn will help us split data into training and test sets

**Creating reproducible random data:**
- `rng = np.random.default_rng(42)` creates a random number generator with a fixed seed (42). Using a seed means we get the same "random" numbers every time we run this code, making results reproducible.
- `n = 300` sets our dataset size to 300 students

**Building the DataFrame:**
- `pd.DataFrame({...})` creates a table with four columns
- `rng.integers(60, 101, n)` generates 300 random whole numbers between 60 and 100 (inclusive)
- Each column represents one feature: attendance percentage, homework completion rate, quiz average, and exam average
- After this step, we have a table where each row is one student and each column is one measurement

**Creating a realistic final score:**
- `base_score = (0.2*df["homework_rate"] + 0.3*df["quiz_avg"] + 0.5*df["exam_avg"])` calculates a weighted average where exams count for 50%, quizzes for 30%, and homework for 20%. Notice we didn't use attendance in the formula.
- `noise = rng.normal(0, 5, n)` generates random variation from a bell curve centered at 0 with a spread of 5 points. Think of this as the unpredictable factors we can't measure: maybe a student had a headache during the exam, or got lucky on multiple choice questions, or studied extra hard one week. These small random variations make our data more like the real world.
- `df["final_score"] = (base_score + noise).round(0)` adds the random variation to the base score and rounds to whole numbers

**Displaying the results:**
- `print(f"Dataset size: {len(df)} students")` shows how many students we created
- `df.head()` displays the first 5 rows of our dataset so we can see what it looks like.

**Creating a classification target:**
- `df["pass_fail"] = np.where(df["final_score"] >= 70, "pass", "fail")` creates a binary label where students with final_score ≥ 70 are marked as "pass", otherwise "fail"
- We create this now because we'll use it later for Logistic Regression (classification)
- For now, we'll focus on predicting the numeric `final_score` with Ridge Regression

### Step 2: Prepare the data

Before we train any model, we need to separate our features from our target and split into training and test sets. This is the same process we used last week with kNN: we separate X (the inputs we use to make predictions) from y (what we're trying to predict), then split both into training and test portions.

```{python}
from sklearn.linear_model import Ridge

# Features (X) and target (y)
X = df[["attendance", "homework_rate", "quiz_avg", "exam_avg"]]
y = df["final_score"]

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.25, random_state=0
)

print(f"Training set: {len(X_train)} students")
print(f"Test set: {len(X_test)} students")
```

**Understanding the code:**

**Importing the model:**
- `from sklearn.linear_model import Ridge` imports the Ridge Regression model from scikit-learn

**Separating features and target:**
- `X = df[["attendance", "homework_rate", "quiz_avg", "exam_avg"]]` creates a table with just our input features (the four columns we'll use to make predictions). By convention, we use capital X for features.
- `y = df["final_score"]` extracts just the target column (what we want to predict). By convention, we use lowercase y for the target.

**Splitting the data:**
- `train_test_split(X, y, test_size=0.25, random_state=0)` splits our data into four pieces
- `test_size=0.25` means 25% of the data goes to testing, 75% to training
- `random_state=0` makes the split reproducible (same split every time)
- `X_train, X_test, y_train, y_test` captures the four results: training features, test features, training targets, and test targets
- With 300 students total, this gives us about 225 students for training and 75 for testing

### Step 3: Train a Ridge model

Now let's train a single Ridge model to see how the basic workflow works. We'll use alpha=1.0, which is scikit-learn's default value. This represents moderate regularization: not too weak (which would allow overfitting) and not too strong (which would oversimplify the model).

```{python}
# Create and train a Ridge model
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Evaluate on both training and test sets
train_r2 = ridge.score(X_train, y_train)
test_r2 = ridge.score(X_test, y_test)

print(f"Training R²: {train_r2:.3f}")
print(f"Test R²: {test_r2:.3f}")
print(f"\nLearned weights: {ridge.coef_}")
print(f"Intercept: {ridge.intercept_:.2f}")
```

**Understanding the code:**

**Creating and training the model:**
- `ridge = Ridge(alpha=1.0)` creates a Ridge model with alpha=1.0 (moderate regularization)
- `ridge.fit(X_train, y_train)` trains the model on the training data. This is where the model learns the optimal weights.

**Evaluating the model:**
- `ridge.score(X_train, y_train)` calculates the R² score on the training data (how well does it fit the data it learned from?)
- `ridge.score(X_test, y_test)` calculates the R² score on the test data (how well does it work on new students it has never seen?)

**Examining what was learned:**
- `ridge.coef_` shows the weights learned for each feature
- `ridge.intercept_` shows the constant (b) added to predictions

**Understanding the output:**

**R² scores:**
- `Training R²: 0.791` means the model explains 79.1% of the variation in the training data
- `Test R²: 0.776` means the model explains 77.6% of the variation in the test data
- Both scores are reasonably high (closer to 1.0 is better)
- The scores are close together (difference of only 0.015), which indicates the model generalizes well

**Learned weights:**
- The four weights correspond to our four features: attendance, homework_rate, quiz_avg, and exam_avg
- `[0.034, 0.196, 0.299, 0.504]` shows how much each feature contributes to the prediction
- Notice these roughly match our true formula (0.2 homework, 0.3 quiz, 0.5 exam)
- Attendance has a small weight (0.034) even though we did not use it in the formula. This happens because Ridge finds small correlations in the data.

**Intercept:**
- `-2.90` is the constant added to every prediction
- This adjusts the baseline of the predictions to match the scale of the final scores

**What this tells us:** The model is working well. It has learned weights that are close to the true relationship, and it performs similarly on both training and test data (no overfitting).

## Part 2: Logistic Regression for Classification

### Understanding Logistic Regression

As we discussed in the introduction, Logistic Regression is a linear model for classification tasks. Despite its name, it's actually a classification algorithm, not a regression algorithm. It's called "regression" for historical reasons, but it predicts categories (like pass/fail) rather than continuous numbers.
Like Ridge Regression, Logistic Regression learns weights for each feature and combines them in a formula. However, it uses these weights differently to make categorical predictions.

**How does it work?** Logistic Regression combines features with weights just like Ridge:
```
score = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b
```

But instead of using this score directly as a prediction, it converts the score into a probability between 0 and 1. If the probability is above 0.5, it predicts "pass"; otherwise, it predicts "fail".

**What makes Logistic Regression different from Ridge:**
- Ridge predicts numbers (regression); Logistic predicts categories (classification)
- Ridge uses R² as its performance metric; Logistic uses accuracy (percentage of correct predictions)
- Ridge returns the weighted sum directly; Logistic converts it to a probability first

#### The complexity control (C)

Logistic Regression has a parameter called `C` that controls regularization, similar to Ridge's alpha. However, C works in the opposite direction. The formula structure is always the same (a weighted sum of features), but C determines whether those weights can be large or must stay small.

**What are we predicting and why do weights matter?** We're predicting pass/fail, where a student passes if their final score is 70 or above. Remember from our data creation that final scores are based on: 0.2 × homework + 0.3 × quiz + 0.5 × exam. This means exam scores matter most for passing, quizzes matter moderately, and homework matters least. Attendance wasn't even in the formula, so it shouldn't matter at all.

A well-trained Logistic Regression model should learn weights that reflect this reality:
- exam_avg should get the largest weight (most important for passing)
- quiz_avg should get a moderate weight
- homework_rate should get a small weight
- attendance should get a weight close to zero (it doesn't actually affect passing)

**How does C affect these weights?**

**With small C (like 0.1):** Strong regularization forces all weights to be small and similar in size. The model might learn weights like [0.01, 0.02, 0.03, 0.02], where all features have roughly equal, small influence. This means the model can't distinguish that exam scores matter more than attendance, even though they clearly do. The model is too simple to capture the true pattern.

**With large C (like 100):** Weak regularization allows weights to vary widely. The model might learn weights like [0.1, 0.8, 2.3, 5.1], correctly recognizing that exam (5.1) matters much more than attendance (0.1). But if the weights get too large, the model becomes overly confident and sensitive to small changes, which can cause problems with new data.

**With moderate C (like 1.0):** The model balances these extremes, learning weights that reflect the true importance of features without becoming overly sensitive.

When we create a Logistic Regression model, we pass C as an argument:

```python
logreg = LogisticRegression(C=0.1)    # Small C: strong regularization, forces small weights
logreg = LogisticRegression(C=100)    # Large C: weak regularization, allows large weights
```

The `.fit()` method finds the best weights within the constraint set by C. Notice that C works backwards from Ridge's alpha: higher C means less regularization (more complex), while higher alpha meant more regularization (simpler). For this demo, we'll use C=1.0, which is scikit-learn's default and represents moderate regularization.

### Step 1: Prepare classification data

We'll use the same dataset and features as Part 1, but this time we're predicting pass/fail categories instead of numeric scores. This is the same train/test split process we've used before.

```{python}
from sklearn.linear_model import LogisticRegression

# Use pass/fail as the target
y_class = df["pass_fail"]

# Features stay the same
X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(
    X, y_class, test_size=0.25, random_state=0
)

print(f"Training set: {len(X_train_class)} students")
print(f"Test set: {len(X_test_class)} students")
```

**Understanding the code:**

**Importing the model:**
- `from sklearn.linear_model import LogisticRegression` imports the Logistic Regression classifier from scikit-learn
- Notice it's in the `linear_model` module, just like Ridge, because it's also a linear model

**Selecting the classification target:**
- `y_class = df["pass_fail"]` extracts the pass/fail column as our target
- This is categorical data (two classes: "pass" and "fail") rather than numeric data

**Splitting the data:**
- We use the same `train_test_split` function with the same parameters
- The only difference from Part 1 is we're using `y_class` (categories) instead of `y` (numbers)
- We add `_class` to our variable names to keep track of which data is for classification

### Step 2: Train a Logistic Regression model

Now let's train a single Logistic Regression model to see how the basic workflow works. We'll use C=1.0, which is scikit-learn's default value and represents moderate regularization.

```{python}
# Create and train a Logistic Regression model
logreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0)
logreg.fit(X_train_class, y_train_class)

# Evaluate on both training and test sets
train_acc = logreg.score(X_train_class, y_train_class)
test_acc = logreg.score(X_test_class, y_test_class)

print(f"Training accuracy: {train_acc:.3f}")
print(f"Test accuracy: {test_acc:.3f}")
print(f"\nLearned weights: {logreg.coef_[0]}")
print(f"Intercept: {logreg.intercept_[0]:.2f}")
```

**Understanding the code:**

**Creating and training the model:**
- `logreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0)` creates a Logistic Regression model
- `C=1.0` uses the default regularization strength
- `max_iter=1000` sets the maximum number of iterations for the optimization algorithm. The model will stop earlier if it finds the best weights, but this ensures it has enough time for difficult problems.
- `random_state=0` ensures reproducible results
- `logreg.fit(X_train_class, y_train_class)` trains the model on the training data. This is where the model learns the optimal weights.

**Evaluating the model:**
- `logreg.score(X_train_class, y_train_class)` calculates accuracy on the training data (what fraction of training predictions are correct?)
- `logreg.score(X_test_class, y_test_class)` calculates accuracy on the test data (what fraction of test predictions are correct?)
- Accuracy is simpler than R². It's just the percentage of correct predictions.

**Examining what was learned:**
- `logreg.coef_[0]` shows the weights learned for each feature (we use [0] because Logistic stores weights in a 2D array for multi-class problems)
- `logreg.intercept_[0]` shows the constant (b) added to the score

**Understanding the output:**

**Accuracy scores:**
- Training accuracy: 0.867 means the model correctly predicted pass/fail for 86.7% of the training students
- Test accuracy: 0.907 means the model correctly predicted pass/fail for 90.7% of the test students
- Both scores are high (closer to 1.0 is better)
- The test accuracy is actually slightly higher than training accuracy, which is unusual but can happen with smaller datasets. This suggests the model is not overfitting.

**Learned weights:**
- The four weights are: [-0.007, 0.059, 0.101, 0.190]
- These correspond to: attendance, homework_rate, quiz_avg, and exam_avg
- Notice that exam_avg has the largest weight (0.190), which makes sense because exam scores are worth 50% of the final score in our data
- quiz_avg has the second largest weight (0.101), matching its 30% contribution
- homework_rate has a smaller weight (0.059), matching its 20% contribution
- attendance has a weight near zero (-0.007), which makes sense because we did not use attendance when creating final scores

The model correctly learned which features actually matter for predicting pass/fail.

**Intercept:**
- -24.03 is the constant added to the weighted sum before converting to a probability
- This large negative value shifts the decision boundary. It means the weighted sum needs to be fairly positive (above 24) before the model predicts "pass" with high confidence.

**What this tells us:** The model has learned a sensible decision boundary. The weights reflect the true importance of each feature in determining pass/fail, with exam scores mattering most and attendance mattering least.

## Summary: Three Supervised Learning Algorithms

We've now seen three different supervised learning algorithms across two weeks: kNN, Ridge Regression, and Logistic Regression.

### What We Learned This Week

In this demo, we focused on the basic workflow for Ridge and Logistic Regression:
- How to train the models with default parameters
- How to interpret the output (R² scores, accuracy, learned weights)
- That Ridge learns weights showing feature importance for regression tasks
- That Logistic learns weights showing feature importance for classification tasks
- That both use the same train/test evaluation framework as kNN

### Algorithm Comparison

Each algorithm has a "complexity parameter" that controls how simple or complex the model is:

| Algorithm | Task | Complexity Parameter | Default Value | Simpler Model | More Complex Model |
|-----------|------|---------------------|---------------|---------------|-------------------|
| **kNN** (Week 2) | Classification or Regression | `n_neighbors` | 5 | Large k | Small k |
| **Ridge Regression** (Week 3) | Regression | `alpha` | 1.0 | Large alpha | Small alpha |
| **Logistic Regression** (Week 3) | Classification | `C` | 1.0 | Small C | Large C |

**Note:** Ridge's `alpha` and Logistic's `C` work in opposite directions. Higher alpha means more regularization (simpler), but higher C means less regularization (more complex). This takes practice to remember.

### Key Takeaways

1. All three algorithms use the same workflow: split data into train/test, train the model, evaluate performance
2. Ridge and Logistic learn weights that show which features are important; kNN does not
3. Ridge predicts numbers (regression), Logistic predicts categories (classification)
4. Default parameters (alpha=1.0, C=1.0) often work well for initial exploration
5. You can examine learned weights to understand what the model thinks is important

### Further Reading in the Textbook

This demo showed you the basic workflow for Ridge and Logistic Regression. The textbook (pages 45-67) covers important topics we didn't demonstrate here:

- **Parameter tuning:** How to systematically try different values of alpha and C to find the best model
- **Overfitting and underfitting:** Examples showing what happens when regularization is too weak or too strong
- **Learning curves:** Visualizations showing how model performance changes with different parameter values
- **Lasso Regression:** Another regularized linear model that can automatically select important features
- **When to use each model:** Detailed guidance on choosing between Linear Regression, Ridge, and Lasso

### Looking Ahead

Next week (Week 4), we'll explore Decision Trees and ensemble methods like Random Forests. These algorithms work completely differently from kNN and linear models, but we'll use the same train/test framework to evaluate them.


