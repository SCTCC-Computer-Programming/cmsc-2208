<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Week 3 Demo: Ridge and Logistic Regression – CMSC 2208 : Introduction to Machine Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" integrity="sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-b337f537912f5ab074b938eebad4c5d7.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark-feb0c0ed7aac57d66ec8dce0f6a48183.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../../site_libs/bootstrap/bootstrap-b337f537912f5ab074b938eebad4c5d7.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js" integrity="sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../shared/assets/course.css">
<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar docked nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CMSC 2208 : Introduction to Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Week 3 Demo: Ridge and Logistic Regression</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
        <div class="sidebar-tools-collapse">
    <a href="https://discord.gg/az8cmeqJdc" title="" class="quarto-navigation-tool px-1" aria-label="Discord"><i class="bi bi-chat-dots"></i></a>
    <a href="https://sctcc.learn.minnstate.edu/d2l/login" title="" class="quarto-navigation-tool px-1" aria-label="D2L"><i class="bi bi-book-half"></i></a>
</div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/syllabus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Syllabus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../course/schedule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Schedule</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/policies/office-hours.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Office Hours</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/policies/homework-coursework.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment Policies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/policies/general-policies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">General Policies</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/policies/ai-policy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">AI Policy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Weekly Overview</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../weekly-overview/week-02/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../weekly-overview/week-03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3 (Current Week)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Weekly Assignments</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../assignments/week-02/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 2</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../assignments/week-03/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Week 3 (Current Week)</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Guides</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false">
 <span class="menu-text">Week 1</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/guides/setup/miniconda.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Miniconda Setup</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/guides/setup/jupyter-learning-links.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Jupyter Learning Links</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false">
 <span class="menu-text">Week 2</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2 Week 2 Reference Videos</span></a>
  </div>
</li>
      </ul>
  </li>
          <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false">
 <span class="menu-text">Week 3</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" role="navigation" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth2 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Chapter 2 Week 3 Reference Videos</span></a>
  </div>
</li>
      </ul>
  </li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#what-are-linear-models" id="toc-what-are-linear-models" class="nav-link" data-scroll-target="#what-are-linear-models">What Are Linear Models?</a></li>
  <li><a href="#ridge-regression-predictions-through-weighted-combinations" id="toc-ridge-regression-predictions-through-weighted-combinations" class="nav-link" data-scroll-target="#ridge-regression-predictions-through-weighted-combinations">Ridge Regression: Predictions Through Weighted Combinations</a></li>
  <li><a href="#logistic-regression-classification-through-weighted-probabilities" id="toc-logistic-regression-classification-through-weighted-probabilities" class="nav-link" data-scroll-target="#logistic-regression-classification-through-weighted-probabilities">Logistic Regression: Classification Through Weighted Probabilities</a></li>
  <li><a href="#what-youll-learn-this-week" id="toc-what-youll-learn-this-week" class="nav-link" data-scroll-target="#what-youll-learn-this-week">What You’ll Learn This Week</a></li>
  </ul></li>
  <li><a href="#part-1-ridge-regression---basic-workflow" id="toc-part-1-ridge-regression---basic-workflow" class="nav-link" data-scroll-target="#part-1-ridge-regression---basic-workflow">Part 1: Ridge Regression - Basic Workflow</a>
  <ul class="collapse">
  <li><a href="#understanding-ridge-regression" id="toc-understanding-ridge-regression" class="nav-link" data-scroll-target="#understanding-ridge-regression">Understanding Ridge Regression</a></li>
  <li><a href="#step-1-create-a-student-dataset" id="toc-step-1-create-a-student-dataset" class="nav-link" data-scroll-target="#step-1-create-a-student-dataset">Step 1: Create a student dataset</a></li>
  <li><a href="#step-2-prepare-the-data" id="toc-step-2-prepare-the-data" class="nav-link" data-scroll-target="#step-2-prepare-the-data">Step 2: Prepare the data</a></li>
  <li><a href="#step-3-train-a-ridge-model" id="toc-step-3-train-a-ridge-model" class="nav-link" data-scroll-target="#step-3-train-a-ridge-model">Step 3: Train a Ridge model</a></li>
  </ul></li>
  <li><a href="#part-2-logistic-regression-for-classification" id="toc-part-2-logistic-regression-for-classification" class="nav-link" data-scroll-target="#part-2-logistic-regression-for-classification">Part 2: Logistic Regression for Classification</a>
  <ul class="collapse">
  <li><a href="#understanding-logistic-regression" id="toc-understanding-logistic-regression" class="nav-link" data-scroll-target="#understanding-logistic-regression">Understanding Logistic Regression</a></li>
  <li><a href="#step-1-prepare-classification-data" id="toc-step-1-prepare-classification-data" class="nav-link" data-scroll-target="#step-1-prepare-classification-data">Step 1: Prepare classification data</a></li>
  <li><a href="#step-2-train-a-logistic-regression-model" id="toc-step-2-train-a-logistic-regression-model" class="nav-link" data-scroll-target="#step-2-train-a-logistic-regression-model">Step 2: Train a Logistic Regression model</a></li>
  </ul></li>
  <li><a href="#summary-three-supervised-learning-algorithms" id="toc-summary-three-supervised-learning-algorithms" class="nav-link" data-scroll-target="#summary-three-supervised-learning-algorithms">Summary: Three Supervised Learning Algorithms</a>
  <ul class="collapse">
  <li><a href="#what-we-learned-this-week" id="toc-what-we-learned-this-week" class="nav-link" data-scroll-target="#what-we-learned-this-week">What We Learned This Week</a></li>
  <li><a href="#algorithm-comparison" id="toc-algorithm-comparison" class="nav-link" data-scroll-target="#algorithm-comparison">Algorithm Comparison</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#further-reading-in-the-textbook" id="toc-further-reading-in-the-textbook" class="nav-link" data-scroll-target="#further-reading-in-the-textbook">Further Reading in the Textbook</a></li>
  <li><a href="#looking-ahead" id="toc-looking-ahead" class="nav-link" data-scroll-target="#looking-ahead">Looking Ahead</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Week 3 Demo: Ridge and Logistic Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">January 22, 2026</p>
    </div>
  </div>
    
  </div>
  


</header>


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Last week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).</p>
<p>This week, we’re learning about <strong>linear models</strong>, a completely different family of algorithms.</p>
<section id="what-are-linear-models" class="level3">
<h3 class="anchored" data-anchor-id="what-are-linear-models">What Are Linear Models?</h3>
<p>Linear models make predictions using a formula that combines your input features. The key characteristic that makes them “linear” is that they multiply each feature by a weight and add them together. For example, if you’re predicting a student’s final score based on their homework, quiz, and exam grades, a linear model learns a formula like:</p>
<pre><code>predicted_score = (w1 × homework) + (w2 × quiz) + (w3 × exam) + constant</code></pre>
<p>During training, the model tries many different combinations of weights to see which ones make the most accurate predictions on the training data. For example, it might try:</p>
<ul>
<li>Attempt 1: w1=0.1, w2=0.1, w3=0.1 → predictions are terrible</li>
<li>Attempt 2: w1=0.5, w2=0.3, w3=0.8 → predictions are better</li>
<li>Attempt 3: w1=0.2, w2=0.3, w3=0.5 → predictions are even better</li>
</ul>
<p>The model keeps adjusting the weights until it finds the combination that minimizes the difference between its predictions and the actual values in the training data. This process happens automatically when you call <code>.fit()</code> - you don’t have to manually try different weights yourself.</p>
<p>In our student data example in Part 1, if exam scores actually matter most for the final grade, the model should learn that w3 (for exam) should be larger than w1 (for homework). The training process figures this out by looking at the patterns in the data.</p>
<p><strong>What makes linear models fundamentally different from kNN:</strong> - <strong>kNN stores</strong> all training data and searches through it; <strong>linear models learn</strong> a formula - <strong>kNN has no “training”</strong> phase (just stores data); <strong>linear models solve</strong> for optimal weights during training - <strong>kNN predictions</strong> depend on nearby examples; <strong>linear models</strong> use the same formula for everyone - <strong>kNN can’t tell you</strong> which features matter; <strong>linear models</strong> show importance through weights</p>
<p>This week, we’ll explore two specific linear models: Ridge Regression (for predicting numbers) and Logistic Regression (for predicting categories).</p>
</section>
<section id="ridge-regression-predictions-through-weighted-combinations" class="level3">
<h3 class="anchored" data-anchor-id="ridge-regression-predictions-through-weighted-combinations">Ridge Regression: Predictions Through Weighted Combinations</h3>
<p>Ridge Regression is a linear model for regression tasks (predicting numeric values like final scores). It learns weights for each feature, then uses those weights in a formula to make predictions.</p>
<p><strong>The complexity control:</strong> Ridge has a parameter called <code>alpha</code> that controls regularization. Regularization keeps the weights from getting too large, which helps prevent overfitting. Higher alpha forces the weights to be smaller, creating a simpler model.</p>
</section>
<section id="logistic-regression-classification-through-weighted-probabilities" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-classification-through-weighted-probabilities">Logistic Regression: Classification Through Weighted Probabilities</h3>
<p>Logistic Regression is a linear model for classification tasks (predicting categories like pass/fail). Despite its name containing “regression,” it predicts categories, not numbers.</p>
<p>It works similarly to Ridge by learning weights and combining features, but instead of outputting the weighted sum directly, it converts the sum into a probability between 0 and 1. If the probability is above 0.5, it predicts one class; otherwise, it predicts the other class.</p>
<p><strong>The complexity control:</strong> Logistic Regression has a parameter called <code>C</code> that controls regularization, but it works opposite to Ridge’s alpha. Higher C means less regularization (more complex model), while higher alpha meant more regularization (simpler model).</p>
</section>
<section id="what-youll-learn-this-week" class="level3">
<h3 class="anchored" data-anchor-id="what-youll-learn-this-week">What You’ll Learn This Week</h3>
<p>By the end of this demo, you’ll be able to: - Understand what linear models are and how they differ from kNN - Train Ridge Regression models for predicting numeric values - Train Logistic Regression models for predicting categories - Interpret the learned weights to understand feature importance - Use the same train/test evaluation framework across different algorithms</p>
</section>
</section>
<section id="part-1-ridge-regression---basic-workflow" class="level2">
<h2 class="anchored" data-anchor-id="part-1-ridge-regression---basic-workflow">Part 1: Ridge Regression - Basic Workflow</h2>
<section id="understanding-ridge-regression" class="level3">
<h3 class="anchored" data-anchor-id="understanding-ridge-regression">Understanding Ridge Regression</h3>
<p>As mentioned in the introduction, Ridge Regression makes predictions by combining features with learned weights. Let’s explore this in detail</p>
<p>The prediction formula looks like this:</p>
<pre><code>predicted_score = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b</code></pre>
<p>Each feature gets multiplied by its weight (w₁, w₂, etc.), and then we add a constant (b, called the intercept).</p>
<p>Weights tell us how much each feature contributes to the prediction. For example, if the model learns that w₃ (the weight for quiz_avg) is 0.5 and w₁ (the weight for attendance) is 0.1, it means quiz average has five times more influence on the final score than attendance does. A weight of 0 would mean that feature doesn’t matter at all. Negative weights mean that as the feature increases, the prediction decreases.</p>
<p>Think of weights like importance scores: larger absolute values (whether positive or negative) mean that feature has a bigger impact on predictions. If exam_avg has a weight of 0.8 and attendance has a weight of 0.1, the model is saying “exam average matters a lot more than attendance for predicting final scores.”</p>
<section id="what-is-regularization" class="level4">
<h4 class="anchored" data-anchor-id="what-is-regularization">What is regularization?</h4>
<p>Ridge uses something called regularization, which means it prefers smaller weights. Why does this matter? When weights get very large (like [3.2, -2.8, 5.1, -4.3]), the model becomes very sensitive to small changes in the input data. A model with smaller weights (like [0.3, 0.2, 0.5, 0.4]) is more stable and less likely to be thrown off by random variation in the data. Think of it like the difference between a recipe that says “add exactly 247 grains of salt” versus “add a pinch of salt.” The first is overly precise and fragile, the second is more robust.</p>
<p>Ridge tries to keep weights small and reasonable, which helps prevent overfitting. It does this by penalizing large weights during training, forcing the model to find a solution that both fits the data well AND keeps weights modest.</p>
</section>
<section id="the-complexity-control-alpha" class="level4">
<h4 class="anchored" data-anchor-id="the-complexity-control-alpha">The complexity control (alpha)</h4>
<p>The parameter <code>alpha</code> controls how large the weights in the prediction formula can be. The formula structure is always the same (a weighted sum of features), but alpha determines whether those weights can be large or must stay small.</p>
<p>Imagine you’re creating a formula to predict how long it takes to commute to campus. Your formula always uses the same factors (distance, time of day, weather, route), but alpha controls how much weight each factor gets:</p>
<ul>
<li><p><strong>With small alpha (like 0.1):</strong> Ridge can assign large weights. Your formula might use weights like “add 15.7 minutes per mile of distance, subtract 23.2 minutes if you leave before 7am, add 18.9 minutes if it’s raining.” These large, specific weights make your predictions very sensitive to each factor. This might fit your personal commute data perfectly, but one unusual day can throw the predictions way off.</p></li>
<li><p><strong>With large alpha (like 100):</strong> Ridge must keep weights small. Your formula might use weights like “add 2 minutes per mile of distance, add 5 minutes if it’s rush hour, add 3 minutes if it’s raining.” These smaller weights make your predictions less sensitive to any single factor. The predictions might be less precise for your specific situation, but they are more stable and work better in new situations.</p></li>
</ul>
<p>When we create a Ridge model, we pass alpha as an argument:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.1</span>)   <span class="co"># Small alpha: allows large weights</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="dv">100</span>)   <span class="co"># Large alpha: forces small weights</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>.fit()</code> method then finds the best weights it can within the constraint set by alpha. For this demo, we’ll use alpha=1.0, which is scikit-learn’s default and represents moderate regularization.</p>
</section>
<section id="what-is-r²" class="level4">
<h4 class="anchored" data-anchor-id="what-is-r²">What is R²?</h4>
<p>When we evaluate a regression model in scikit-learn, we use a method called <code>.score()</code> that returns a number called R² (R-squared). This is the metric we use to measure how well the model performs. You’ll see it calculated like this in our code:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>train_r2 <span class="op">=</span> ridge.score(X_train, y_train)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>test_r2 <span class="op">=</span> ridge.score(X_test, y_test)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>R² ranges from 0 to 1 and tells us how much of the variation in the data our model explains: - <strong>1.0</strong> means perfect predictions (every prediction exactly matches the true value) - <strong>0.5</strong> means okay predictions (the model captures some patterns but misses others) - <strong>0.0</strong> means the model is no better than just guessing the average every time</p>
<p>For example, if we get <code>train_r2 = 0.85</code>, it means our model explains 85% of the variation in the training data. Higher R² values are better, and we want both training and test R² to be reasonably high and close together.</p>
</section>
</section>
<section id="step-1-create-a-student-dataset" class="level3">
<h3 class="anchored" data-anchor-id="step-1-create-a-student-dataset">Step 1: Create a student dataset</h3>
<p>Let’s start with a student performance dataset. We’ll create data for 300 students, each with four measured features: attendance percentage, homework completion rate, quiz average, and exam average. We’ll calculate a final score based on a weighted combination of these features, plus some random noise to represent the unpredictable factors we can’t measure (like how much sleep a student got, or whether they were having a bad day).</p>
<div id="dd5311d6" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create reproducible random data</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"attendance"</span>: rng.integers(<span class="dv">60</span>, <span class="dv">101</span>, n),</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"homework_rate"</span>: rng.integers(<span class="dv">50</span>, <span class="dv">101</span>, n),</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"quiz_avg"</span>: rng.integers(<span class="dv">40</span>, <span class="dv">101</span>, n),</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"exam_avg"</span>: rng.integers(<span class="dv">40</span>, <span class="dv">101</span>, n),</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Create final score with some realistic noise</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>base_score <span class="op">=</span> (<span class="fl">0.2</span><span class="op">*</span>df[<span class="st">"homework_rate"</span>] <span class="op">+</span> <span class="fl">0.3</span><span class="op">*</span>df[<span class="st">"quiz_avg"</span>] <span class="op">+</span> <span class="fl">0.5</span><span class="op">*</span>df[<span class="st">"exam_avg"</span>])</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> rng.normal(<span class="dv">0</span>, <span class="dv">5</span>, n)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"final_score"</span>] <span class="op">=</span> (base_score <span class="op">+</span> noise).<span class="bu">round</span>(<span class="dv">0</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Add classification target for Logistic Regression (coming later)</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"pass_fail"</span>] <span class="op">=</span> np.where(df[<span class="st">"final_score"</span>] <span class="op">&gt;=</span> <span class="dv">70</span>, <span class="st">"pass"</span>, <span class="st">"fail"</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Dataset size: </span><span class="sc">{</span><span class="bu">len</span>(df)<span class="sc">}</span><span class="ss"> students"</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of features: </span><span class="sc">{</span><span class="bu">len</span>(df.columns) <span class="op">-</span> <span class="dv">2</span><span class="sc">}</span><span class="ss">"</span>)  <span class="co"># -2 for final_score and pass_fail</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Dataset size: 300 students
Number of features: 4</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">attendance</th>
<th data-quarto-table-cell-role="th">homework_rate</th>
<th data-quarto-table-cell-role="th">quiz_avg</th>
<th data-quarto-table-cell-role="th">exam_avg</th>
<th data-quarto-table-cell-role="th">final_score</th>
<th data-quarto-table-cell-role="th">pass_fail</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th data-quarto-table-cell-role="th">0</th>
<td>63</td>
<td>68</td>
<td>77</td>
<td>40</td>
<td>63.0</td>
<td>fail</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">1</th>
<td>91</td>
<td>92</td>
<td>87</td>
<td>42</td>
<td>64.0</td>
<td>fail</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">2</th>
<td>86</td>
<td>50</td>
<td>85</td>
<td>53</td>
<td>64.0</td>
<td>fail</td>
</tr>
<tr class="even">
<th data-quarto-table-cell-role="th">3</th>
<td>77</td>
<td>95</td>
<td>48</td>
<td>93</td>
<td>68.0</td>
<td>fail</td>
</tr>
<tr class="odd">
<th data-quarto-table-cell-role="th">4</th>
<td>77</td>
<td>77</td>
<td>90</td>
<td>67</td>
<td>70.0</td>
<td>pass</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p><strong>Understanding the code:</strong></p>
<p><strong>Importing libraries:</strong> - <code>numpy</code> (abbreviated as <code>np</code>) provides tools for generating random numbers and doing numerical operations - <code>pandas</code> (abbreviated as <code>pd</code>) lets us work with tables of data called DataFrames - <code>matplotlib.pyplot</code> (abbreviated as <code>plt</code>) is for creating visualizations - <code>train_test_split</code> from scikit-learn will help us split data into training and test sets</p>
<p><strong>Creating reproducible random data:</strong> - <code>rng = np.random.default_rng(42)</code> creates a random number generator with a fixed seed (42). Using a seed means we get the same “random” numbers every time we run this code, making results reproducible. - <code>n = 300</code> sets our dataset size to 300 students</p>
<p><strong>Building the DataFrame:</strong> - <code>pd.DataFrame({...})</code> creates a table with four columns - <code>rng.integers(60, 101, n)</code> generates 300 random whole numbers between 60 and 100 (inclusive) - Each column represents one feature: attendance percentage, homework completion rate, quiz average, and exam average - After this step, we have a table where each row is one student and each column is one measurement</p>
<p><strong>Creating a realistic final score:</strong> - <code>base_score = (0.2*df["homework_rate"] + 0.3*df["quiz_avg"] + 0.5*df["exam_avg"])</code> calculates a weighted average where exams count for 50%, quizzes for 30%, and homework for 20%. Notice we didn’t use attendance in the formula. - <code>noise = rng.normal(0, 5, n)</code> generates random variation from a bell curve centered at 0 with a spread of 5 points. Think of this as the unpredictable factors we can’t measure: maybe a student had a headache during the exam, or got lucky on multiple choice questions, or studied extra hard one week. These small random variations make our data more like the real world. - <code>df["final_score"] = (base_score + noise).round(0)</code> adds the random variation to the base score and rounds to whole numbers</p>
<p><strong>Displaying the results:</strong> - <code>print(f"Dataset size: {len(df)} students")</code> shows how many students we created - <code>df.head()</code> displays the first 5 rows of our dataset so we can see what it looks like.</p>
<p><strong>Creating a classification target:</strong> - <code>df["pass_fail"] = np.where(df["final_score"] &gt;= 70, "pass", "fail")</code> creates a binary label where students with final_score ≥ 70 are marked as “pass”, otherwise “fail” - We create this now because we’ll use it later for Logistic Regression (classification) - For now, we’ll focus on predicting the numeric <code>final_score</code> with Ridge Regression</p>
</section>
<section id="step-2-prepare-the-data" class="level3">
<h3 class="anchored" data-anchor-id="step-2-prepare-the-data">Step 2: Prepare the data</h3>
<p>Before we train any model, we need to separate our features from our target and split into training and test sets. This is the same process we used last week with kNN: we separate X (the inputs we use to make predictions) from y (what we’re trying to predict), then split both into training and test portions.</p>
<div id="09715da0" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Features (X) and target (y)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">"attendance"</span>, <span class="st">"homework_rate"</span>, <span class="st">"quiz_avg"</span>, <span class="st">"exam_avg"</span>]]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">"final_score"</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Split into train and test sets</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span><span class="bu">len</span>(X_train)<span class="sc">}</span><span class="ss"> students"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set: </span><span class="sc">{</span><span class="bu">len</span>(X_test)<span class="sc">}</span><span class="ss"> students"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training set: 225 students
Test set: 75 students</code></pre>
</div>
</div>
<p><strong>Understanding the code:</strong></p>
<p><strong>Importing the model:</strong> - <code>from sklearn.linear_model import Ridge</code> imports the Ridge Regression model from scikit-learn</p>
<p><strong>Separating features and target:</strong> - <code>X = df[["attendance", "homework_rate", "quiz_avg", "exam_avg"]]</code> creates a table with just our input features (the four columns we’ll use to make predictions). By convention, we use capital X for features. - <code>y = df["final_score"]</code> extracts just the target column (what we want to predict). By convention, we use lowercase y for the target.</p>
<p><strong>Splitting the data:</strong> - <code>train_test_split(X, y, test_size=0.25, random_state=0)</code> splits our data into four pieces - <code>test_size=0.25</code> means 25% of the data goes to testing, 75% to training - <code>random_state=0</code> makes the split reproducible (same split every time) - <code>X_train, X_test, y_train, y_test</code> captures the four results: training features, test features, training targets, and test targets - With 300 students total, this gives us about 225 students for training and 75 for testing</p>
</section>
<section id="step-3-train-a-ridge-model" class="level3">
<h3 class="anchored" data-anchor-id="step-3-train-a-ridge-model">Step 3: Train a Ridge model</h3>
<p>Now let’s train a single Ridge model to see how the basic workflow works. We’ll use alpha=1.0, which is scikit-learn’s default value. This represents moderate regularization: not too weak (which would allow overfitting) and not too strong (which would oversimplify the model).</p>
<div id="ac0ac856" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train a Ridge model</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>ridge <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>ridge.fit(X_train, y_train)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on both training and test sets</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>train_r2 <span class="op">=</span> ridge.score(X_train, y_train)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>test_r2 <span class="op">=</span> ridge.score(X_test, y_test)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training R²: </span><span class="sc">{</span>train_r2<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test R²: </span><span class="sc">{</span>test_r2<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Learned weights: </span><span class="sc">{</span>ridge<span class="sc">.</span>coef_<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Intercept: </span><span class="sc">{</span>ridge<span class="sc">.</span>intercept_<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training R²: 0.791
Test R²: 0.776

Learned weights: [0.03404973 0.19636421 0.2986387  0.5035881 ]
Intercept: -2.90</code></pre>
</div>
</div>
<p><strong>Understanding the code:</strong></p>
<p><strong>Creating and training the model:</strong> - <code>ridge = Ridge(alpha=1.0)</code> creates a Ridge model with alpha=1.0 (moderate regularization) - <code>ridge.fit(X_train, y_train)</code> trains the model on the training data. This is where the model learns the optimal weights.</p>
<p><strong>Evaluating the model:</strong> - <code>ridge.score(X_train, y_train)</code> calculates the R² score on the training data (how well does it fit the data it learned from?) - <code>ridge.score(X_test, y_test)</code> calculates the R² score on the test data (how well does it work on new students it has never seen?)</p>
<p><strong>Examining what was learned:</strong> - <code>ridge.coef_</code> shows the weights learned for each feature - <code>ridge.intercept_</code> shows the constant (b) added to predictions</p>
<p><strong>Understanding the output:</strong></p>
<p><strong>R² scores:</strong> - <code>Training R²: 0.791</code> means the model explains 79.1% of the variation in the training data - <code>Test R²: 0.776</code> means the model explains 77.6% of the variation in the test data - Both scores are reasonably high (closer to 1.0 is better) - The scores are close together (difference of only 0.015), which indicates the model generalizes well</p>
<p><strong>Learned weights:</strong> - The four weights correspond to our four features: attendance, homework_rate, quiz_avg, and exam_avg - <code>[0.034, 0.196, 0.299, 0.504]</code> shows how much each feature contributes to the prediction - Notice these roughly match our true formula (0.2 homework, 0.3 quiz, 0.5 exam) - Attendance has a small weight (0.034) even though we did not use it in the formula. This happens because Ridge finds small correlations in the data.</p>
<p><strong>Intercept:</strong> - <code>-2.90</code> is the constant added to every prediction - This adjusts the baseline of the predictions to match the scale of the final scores</p>
<p><strong>What this tells us:</strong> The model is working well. It has learned weights that are close to the true relationship, and it performs similarly on both training and test data (no overfitting).</p>
</section>
</section>
<section id="part-2-logistic-regression-for-classification" class="level2">
<h2 class="anchored" data-anchor-id="part-2-logistic-regression-for-classification">Part 2: Logistic Regression for Classification</h2>
<section id="understanding-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="understanding-logistic-regression">Understanding Logistic Regression</h3>
<p>As we discussed in the introduction, Logistic Regression is a linear model for classification tasks. Despite its name, it’s actually a classification algorithm, not a regression algorithm. It’s called “regression” for historical reasons, but it predicts categories (like pass/fail) rather than continuous numbers. Like Ridge Regression, Logistic Regression learns weights for each feature and combines them in a formula. However, it uses these weights differently to make categorical predictions.</p>
<p><strong>How does it work?</strong> Logistic Regression combines features with weights just like Ridge:</p>
<pre><code>score = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b</code></pre>
<p>But instead of using this score directly as a prediction, it converts the score into a probability between 0 and 1. If the probability is above 0.5, it predicts “pass”; otherwise, it predicts “fail”.</p>
<p><strong>What makes Logistic Regression different from Ridge:</strong> - Ridge predicts numbers (regression); Logistic predicts categories (classification) - Ridge uses R² as its performance metric; Logistic uses accuracy (percentage of correct predictions) - Ridge returns the weighted sum directly; Logistic converts it to a probability first</p>
<section id="the-complexity-control-c" class="level4">
<h4 class="anchored" data-anchor-id="the-complexity-control-c">The complexity control (C)</h4>
<p>Logistic Regression has a parameter called <code>C</code> that controls regularization, similar to Ridge’s alpha. However, C works in the opposite direction. The formula structure is always the same (a weighted sum of features), but C determines whether those weights can be large or must stay small.</p>
<p><strong>What are we predicting and why do weights matter?</strong> We’re predicting pass/fail, where a student passes if their final score is 70 or above. Remember from our data creation that final scores are based on: 0.2 × homework + 0.3 × quiz + 0.5 × exam. This means exam scores matter most for passing, quizzes matter moderately, and homework matters least. Attendance wasn’t even in the formula, so it shouldn’t matter at all.</p>
<p>A well-trained Logistic Regression model should learn weights that reflect this reality: - exam_avg should get the largest weight (most important for passing) - quiz_avg should get a moderate weight - homework_rate should get a small weight - attendance should get a weight close to zero (it doesn’t actually affect passing)</p>
<p><strong>How does C affect these weights?</strong></p>
<p><strong>With small C (like 0.1):</strong> Strong regularization forces all weights to be small and similar in size. The model might learn weights like [0.01, 0.02, 0.03, 0.02], where all features have roughly equal, small influence. This means the model can’t distinguish that exam scores matter more than attendance, even though they clearly do. The model is too simple to capture the true pattern.</p>
<p><strong>With large C (like 100):</strong> Weak regularization allows weights to vary widely. The model might learn weights like [0.1, 0.8, 2.3, 5.1], correctly recognizing that exam (5.1) matters much more than attendance (0.1). But if the weights get too large, the model becomes overly confident and sensitive to small changes, which can cause problems with new data.</p>
<p><strong>With moderate C (like 1.0):</strong> The model balances these extremes, learning weights that reflect the true importance of features without becoming overly sensitive.</p>
<p>When we create a Logistic Regression model, we pass C as an argument:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="fl">0.1</span>)    <span class="co"># Small C: strong regularization, forces small weights</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="dv">100</span>)    <span class="co"># Large C: weak regularization, allows large weights</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p>The <code>.fit()</code> method finds the best weights within the constraint set by C. Notice that C works backwards from Ridge’s alpha: higher C means less regularization (more complex), while higher alpha meant more regularization (simpler). For this demo, we’ll use C=1.0, which is scikit-learn’s default and represents moderate regularization.</p>
</section>
</section>
<section id="step-1-prepare-classification-data" class="level3">
<h3 class="anchored" data-anchor-id="step-1-prepare-classification-data">Step 1: Prepare classification data</h3>
<p>We’ll use the same dataset and features as Part 1, but this time we’re predicting pass/fail categories instead of numeric scores. This is the same train/test split process we’ve used before.</p>
<div id="2ed51e6a" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Use pass/fail as the target</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>y_class <span class="op">=</span> df[<span class="st">"pass_fail"</span>]</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Features stay the same</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>X_train_class, X_test_class, y_train_class, y_test_class <span class="op">=</span> train_test_split(</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    X, y_class, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training set: </span><span class="sc">{</span><span class="bu">len</span>(X_train_class)<span class="sc">}</span><span class="ss"> students"</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test set: </span><span class="sc">{</span><span class="bu">len</span>(X_test_class)<span class="sc">}</span><span class="ss"> students"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training set: 225 students
Test set: 75 students</code></pre>
</div>
</div>
<p><strong>Understanding the code:</strong></p>
<p><strong>Importing the model:</strong> - <code>from sklearn.linear_model import LogisticRegression</code> imports the Logistic Regression classifier from scikit-learn - Notice it’s in the <code>linear_model</code> module, just like Ridge, because it’s also a linear model</p>
<p><strong>Selecting the classification target:</strong> - <code>y_class = df["pass_fail"]</code> extracts the pass/fail column as our target - This is categorical data (two classes: “pass” and “fail”) rather than numeric data</p>
<p><strong>Splitting the data:</strong> - We use the same <code>train_test_split</code> function with the same parameters - The only difference from Part 1 is we’re using <code>y_class</code> (categories) instead of <code>y</code> (numbers) - We add <code>_class</code> to our variable names to keep track of which data is for classification</p>
</section>
<section id="step-2-train-a-logistic-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="step-2-train-a-logistic-regression-model">Step 2: Train a Logistic Regression model</h3>
<p>Now let’s train a single Logistic Regression model to see how the basic workflow works. We’ll use C=1.0, which is scikit-learn’s default value and represents moderate regularization.</p>
<div id="f145278a" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create and train a Logistic Regression model</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="fl">1.0</span>, max_iter<span class="op">=</span><span class="dv">1000</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>logreg.fit(X_train_class, y_train_class)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate on both training and test sets</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>train_acc <span class="op">=</span> logreg.score(X_train_class, y_train_class)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>test_acc <span class="op">=</span> logreg.score(X_test_class, y_test_class)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training accuracy: </span><span class="sc">{</span>train_acc<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test accuracy: </span><span class="sc">{</span>test_acc<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Learned weights: </span><span class="sc">{</span>logreg<span class="sc">.</span>coef_[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Intercept: </span><span class="sc">{</span>logreg<span class="sc">.</span>intercept_[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training accuracy: 0.867
Test accuracy: 0.907

Learned weights: [-0.00720698  0.05896261  0.10066204  0.19035106]
Intercept: -24.03</code></pre>
</div>
</div>
<p><strong>Understanding the code:</strong></p>
<p><strong>Creating and training the model:</strong> - <code>logreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0)</code> creates a Logistic Regression model - <code>C=1.0</code> uses the default regularization strength - <code>max_iter=1000</code> sets the maximum number of iterations for the optimization algorithm. The model will stop earlier if it finds the best weights, but this ensures it has enough time for difficult problems. - <code>random_state=0</code> ensures reproducible results - <code>logreg.fit(X_train_class, y_train_class)</code> trains the model on the training data. This is where the model learns the optimal weights.</p>
<p><strong>Evaluating the model:</strong> - <code>logreg.score(X_train_class, y_train_class)</code> calculates accuracy on the training data (what fraction of training predictions are correct?) - <code>logreg.score(X_test_class, y_test_class)</code> calculates accuracy on the test data (what fraction of test predictions are correct?) - Accuracy is simpler than R². It’s just the percentage of correct predictions.</p>
<p><strong>Examining what was learned:</strong> - <code>logreg.coef_[0]</code> shows the weights learned for each feature (we use [0] because Logistic stores weights in a 2D array for multi-class problems) - <code>logreg.intercept_[0]</code> shows the constant (b) added to the score</p>
<p><strong>Understanding the output:</strong></p>
<p><strong>Accuracy scores:</strong> - Training accuracy: 0.867 means the model correctly predicted pass/fail for 86.7% of the training students - Test accuracy: 0.907 means the model correctly predicted pass/fail for 90.7% of the test students - Both scores are high (closer to 1.0 is better) - The test accuracy is actually slightly higher than training accuracy, which is unusual but can happen with smaller datasets. This suggests the model is not overfitting.</p>
<p><strong>Learned weights:</strong> - The four weights are: [-0.007, 0.059, 0.101, 0.190] - These correspond to: attendance, homework_rate, quiz_avg, and exam_avg - Notice that exam_avg has the largest weight (0.190), which makes sense because exam scores are worth 50% of the final score in our data - quiz_avg has the second largest weight (0.101), matching its 30% contribution - homework_rate has a smaller weight (0.059), matching its 20% contribution - attendance has a weight near zero (-0.007), which makes sense because we did not use attendance when creating final scores</p>
<p>The model correctly learned which features actually matter for predicting pass/fail.</p>
<p><strong>Intercept:</strong> - -24.03 is the constant added to the weighted sum before converting to a probability - This large negative value shifts the decision boundary. It means the weighted sum needs to be fairly positive (above 24) before the model predicts “pass” with high confidence.</p>
<p><strong>What this tells us:</strong> The model has learned a sensible decision boundary. The weights reflect the true importance of each feature in determining pass/fail, with exam scores mattering most and attendance mattering least.</p>
</section>
</section>
<section id="summary-three-supervised-learning-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="summary-three-supervised-learning-algorithms">Summary: Three Supervised Learning Algorithms</h2>
<p>We’ve now seen three different supervised learning algorithms across two weeks: kNN, Ridge Regression, and Logistic Regression.</p>
<section id="what-we-learned-this-week" class="level3">
<h3 class="anchored" data-anchor-id="what-we-learned-this-week">What We Learned This Week</h3>
<p>In this demo, we focused on the basic workflow for Ridge and Logistic Regression: - How to train the models with default parameters - How to interpret the output (R² scores, accuracy, learned weights) - That Ridge learns weights showing feature importance for regression tasks - That Logistic learns weights showing feature importance for classification tasks - That both use the same train/test evaluation framework as kNN</p>
</section>
<section id="algorithm-comparison" class="level3">
<h3 class="anchored" data-anchor-id="algorithm-comparison">Algorithm Comparison</h3>
<p>Each algorithm has a “complexity parameter” that controls how simple or complex the model is:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 6%">
<col style="width: 24%">
<col style="width: 17%">
<col style="width: 17%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Algorithm</th>
<th>Task</th>
<th>Complexity Parameter</th>
<th>Default Value</th>
<th>Simpler Model</th>
<th>More Complex Model</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>kNN</strong> (Week 2)</td>
<td>Classification or Regression</td>
<td><code>n_neighbors</code></td>
<td>5</td>
<td>Large k</td>
<td>Small k</td>
</tr>
<tr class="even">
<td><strong>Ridge Regression</strong> (Week 3)</td>
<td>Regression</td>
<td><code>alpha</code></td>
<td>1.0</td>
<td>Large alpha</td>
<td>Small alpha</td>
</tr>
<tr class="odd">
<td><strong>Logistic Regression</strong> (Week 3)</td>
<td>Classification</td>
<td><code>C</code></td>
<td>1.0</td>
<td>Small C</td>
<td>Large C</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> Ridge’s <code>alpha</code> and Logistic’s <code>C</code> work in opposite directions. Higher alpha means more regularization (simpler), but higher C means less regularization (more complex). This takes practice to remember.</p>
</section>
<section id="key-takeaways" class="level3">
<h3 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h3>
<ol type="1">
<li>All three algorithms use the same workflow: split data into train/test, train the model, evaluate performance</li>
<li>Ridge and Logistic learn weights that show which features are important; kNN does not</li>
<li>Ridge predicts numbers (regression), Logistic predicts categories (classification)</li>
<li>Default parameters (alpha=1.0, C=1.0) often work well for initial exploration</li>
<li>You can examine learned weights to understand what the model thinks is important</li>
</ol>
</section>
<section id="further-reading-in-the-textbook" class="level3">
<h3 class="anchored" data-anchor-id="further-reading-in-the-textbook">Further Reading in the Textbook</h3>
<p>This demo showed you the basic workflow for Ridge and Logistic Regression. The textbook (pages 45-67) covers important topics we didn’t demonstrate here:</p>
<ul>
<li><strong>Parameter tuning:</strong> How to systematically try different values of alpha and C to find the best model</li>
<li><strong>Overfitting and underfitting:</strong> Examples showing what happens when regularization is too weak or too strong</li>
<li><strong>Learning curves:</strong> Visualizations showing how model performance changes with different parameter values</li>
<li><strong>Lasso Regression:</strong> Another regularized linear model that can automatically select important features</li>
<li><strong>When to use each model:</strong> Detailed guidance on choosing between Linear Regression, Ridge, and Lasso</li>
</ul>
</section>
<section id="looking-ahead" class="level3">
<h3 class="anchored" data-anchor-id="looking-ahead">Looking Ahead</h3>
<p>Next week (Week 4), we’ll explore Decision Trees and ensemble methods like Random Forests. These algorithms work completely differently from kNN and linear models, but we’ll use the same train/test framework to evaluate them.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/SCTCC-Computer-Programming\.github\.io\/cmsc-2208\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<script type="text/javascript">
  document.addEventListener("DOMContentLoaded", function() {
    var elements = document.querySelectorAll('[aria-label]');
    for (var i = 0; i < elements.length; i++) {
      var ariaLabel = elements[i].getAttribute('aria-label');
      elements[i].setAttribute('data-bs-title', ariaLabel);
      elements[i].setAttribute('data-bs-toggle', 'tooltip');
      elements[i].setAttribute('data-bs-placement', 'bottom');
    }
    const tooltipTriggerList = document.querySelectorAll('[data-bs-toggle="tooltip"]');
    const tooltipList = [...tooltipTriggerList].map(tooltipTriggerEl => new bootstrap.Tooltip(tooltipTriggerEl));
  });
</script>




</body></html>