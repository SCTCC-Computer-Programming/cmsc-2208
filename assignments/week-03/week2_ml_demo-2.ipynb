{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Supervised Learning Algorithms Demo\n",
    "\n",
    "This notebook demonstrates new algorithms from Chapter 2:\n",
    "- Linear Models (Linear Regression and Ridge Regression)\n",
    "- Logistic Regression for Classification\n",
    "- Decision Trees\n",
    "- Model complexity and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Building a Student Performance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create reproducible random data\n",
    "rng = np.random.default_rng(42)\n",
    "n = 100  # more students for better demonstrations\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"attendance\": rng.integers(60, 101, n),\n",
    "    \"homework_rate\": rng.integers(50, 101, n),\n",
    "    \"quiz_avg\": rng.integers(40, 101, n),\n",
    "    \"exam_avg\": rng.integers(40, 101, n),\n",
    "})\n",
    "\n",
    "# Regression target: final score\n",
    "df[\"final_score\"] = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"]).round(0)\n",
    "\n",
    "# Classification target: pass/fail\n",
    "df[\"pass_fail\"] = np.where(df[\"final_score\"] >= 70, \"pass\", \"fail\")\n",
    "\n",
    "print(f\"Dataset size: {len(df)} students\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Linear Models for Regression\n",
    "\n",
    "Linear models predict outputs as weighted sums of features: ŷ = w[0]×x[0] + w[1]×x[1] + ... + b\n",
    "\n",
    "We'll compare:\n",
    "- **LinearRegression**: no regularization\n",
    "- **Ridge**: L2 regularization (shrinks coefficients toward zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "# Use final_score as regression target\n",
    "y_reg = df[\"final_score\"]\n",
    "X = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_reg, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} students\")\n",
    "print(f\"Test set: {len(X_test)} students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Linear Regression vs Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (no regularization)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_train = lr.score(X_train, y_train)\n",
    "lr_test = lr.score(X_test, y_test)\n",
    "\n",
    "print(\"Linear Regression:\")\n",
    "print(f\"  Training R²: {lr_train:.3f}\")\n",
    "print(f\"  Test R²: {lr_test:.3f}\")\n",
    "print()\n",
    "\n",
    "# Ridge with different alpha values\n",
    "alphas = [0.1, 1.0, 10.0, 100.0]\n",
    "ridge_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    train_r2 = ridge.score(X_train, y_train)\n",
    "    test_r2 = ridge.score(X_test, y_test)\n",
    "    \n",
    "    ridge_results.append({\n",
    "        \"alpha\": alpha,\n",
    "        \"train_R2\": train_r2,\n",
    "        \"test_R2\": test_r2\n",
    "    })\n",
    "    \n",
    "    print(f\"Ridge (alpha={alpha:5.1f}) → train: {train_r2:.3f}, test: {test_r2:.3f}\")\n",
    "\n",
    "ridge_df = pd.DataFrame(ridge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regularization effect\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ridge_df[\"alpha\"], ridge_df[\"train_R2\"], 'o-', label=\"Training R²\")\n",
    "plt.plot(ridge_df[\"alpha\"], ridge_df[\"test_R2\"], 's-', label=\"Test R²\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"alpha (regularization strength)\")\n",
    "plt.ylabel(\"R² score\")\n",
    "plt.title(\"Ridge Regression: Effect of Regularization\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: Higher alpha means stronger regularization (simpler model). This reduces overfitting but may increase underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coefficients for different regularization strengths\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Linear Regression coefficients\n",
    "feature_names = [\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]\n",
    "ax.plot(range(len(lr.coef_)), lr.coef_, 'o', label=\"LinearRegression\", markersize=8)\n",
    "\n",
    "# Ridge coefficients for different alphas\n",
    "for alpha in [1.0, 10.0, 100.0]:\n",
    "    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n",
    "    ax.plot(range(len(ridge.coef_)), ridge.coef_, 's', label=f\"Ridge alpha={alpha}\", alpha=0.7)\n",
    "\n",
    "ax.set_xticks(range(len(feature_names)))\n",
    "ax.set_xticklabels(feature_names, rotation=45)\n",
    "ax.set_ylabel(\"Coefficient magnitude\")\n",
    "ax.set_title(\"Effect of Regularization on Coefficients\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: Ridge regularization shrinks coefficients toward zero. Stronger regularization (higher alpha) means smaller coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Logistic Regression for Classification\n",
    "\n",
    "Despite the name, Logistic Regression is a **classification** algorithm. The parameter **C** controls regularization (higher C = less regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Use pass/fail classification target\n",
    "y_class = df[\"pass_fail\"]\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X, y_class, test_size=0.25, random_state=0\n",
    ")\n",
    "\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "logreg_results = []\n",
    "\n",
    "for C in C_values:\n",
    "    logreg = LogisticRegression(C=C, max_iter=1000, random_state=0)\n",
    "    logreg.fit(X_train_class, y_train_class)\n",
    "    \n",
    "    train_acc = logreg.score(X_train_class, y_train_class)\n",
    "    test_acc = logreg.score(X_test_class, y_test_class)\n",
    "    \n",
    "    logreg_results.append({\n",
    "        \"C\": C,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_accuracy\": test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"C={C:6.2f} → train: {train_acc:.3f}, test: {test_acc:.3f}\")\n",
    "\n",
    "logreg_df = pd.DataFrame(logreg_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(logreg_df[\"C\"], logreg_df[\"train_accuracy\"], 'o-', label=\"Training accuracy\")\n",
    "plt.plot(logreg_df[\"C\"], logreg_df[\"test_accuracy\"], 's-', label=\"Test accuracy\")\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"C (inverse regularization strength)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Logistic Regression: Effect of C Parameter\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: For Logistic Regression, **higher C = less regularization = more complex model**. This is opposite to Ridge's alpha parameter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Decision Trees\n",
    "\n",
    "Decision trees learn hierarchies of if/else questions. The parameter **max_depth** controls how deep the tree can grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "depths = [1, 2, 3, 5, 10, None]  # None = unlimited depth\n",
    "tree_results = []\n",
    "\n",
    "for depth in depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=0)\n",
    "    tree.fit(X_train_class, y_train_class)\n",
    "    \n",
    "    train_acc = tree.score(X_train_class, y_train_class)\n",
    "    test_acc = tree.score(X_test_class, y_test_class)\n",
    "    \n",
    "    depth_str = \"unlimited\" if depth is None else str(depth)\n",
    "    tree_results.append({\n",
    "        \"max_depth\": depth_str,\n",
    "        \"train_accuracy\": train_acc,\n",
    "        \"test_accuracy\": test_acc\n",
    "    })\n",
    "    \n",
    "    print(f\"max_depth={depth_str:9s} → train: {train_acc:.3f}, test: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a Simple Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Train a small tree for visualization\n",
    "simple_tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "simple_tree.fit(X_train_class, y_train_class)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plot_tree(simple_tree, \n",
    "          feature_names=feature_names, \n",
    "          class_names=[\"fail\", \"pass\"],\n",
    "          filled=True, \n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title(\"Decision Tree (max_depth=3)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: Each node asks a yes/no question about one feature. The tree splits data until leaves are pure or max_depth is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "tree.fit(X_train_class, y_train_class)\n",
    "\n",
    "# Plot feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": tree.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(importance_df[\"feature\"], importance_df[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance from Decision Tree\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(importance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insight**: Feature importance shows which features the tree used most for splitting. Higher values mean the feature was more useful for making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Model Complexity Across Algorithms\n",
    "\n",
    "Different algorithms have different complexity parameters:\n",
    "\n",
    "| Algorithm | Parameter | More Complex | Less Complex |\n",
    "|-----------|-----------|--------------||--------------|\n",
    "| Ridge | `alpha` | Small alpha | Large alpha |\n",
    "| Logistic Regression | `C` | Large C | Small C |\n",
    "| Decision Tree | `max_depth` | Large depth | Small depth |\n",
    "\n",
    "**General pattern**: \n",
    "- More complex models fit training data better but may overfit\n",
    "- Less complex models are more restricted but may underfit\n",
    "- The best model complexity depends on your dataset and is found using the test set\n",
    "\n",
    "**Note**: Ridge uses `alpha` where larger = simpler, while Logistic Regression uses `C` where larger = more complex. They're inverses of each other!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
