[
  {
    "objectID": "weekly-overview/week-04/index.html",
    "href": "weekly-overview/week-04/index.html",
    "title": "Week 4 Guide",
    "section": "",
    "text": "This week is about applying the linear model concepts you learned in Week 3. Rather than introducing new algorithms, you will work through two case studies that require you to interpret model outputs, identify overfitting and underfitting patterns, and make recommendations based on the results.\nIn Week 3, you learned how linear models work: the weighted prediction formula, regularization, and the parameters that control model complexity (alpha for Ridge, C for Logistic Regression). This week, you will practice using that knowledge to analyze realistic scenarios where a data scientist has already trained models and recorded the results.\nThe primary deliverable this week is a video reflection in which you discuss both case studies and demonstrate your understanding of linear models. This format gives you practice explaining technical concepts verbally—a skill that matters in real-world data science work.\nWeek 4 Assignment:\n\nHere is the link to the Week 4 Assignment.",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 4 (Current Week)"
    ]
  },
  {
    "objectID": "weekly-overview/week-04/index.html#week-4-focus",
    "href": "weekly-overview/week-04/index.html#week-4-focus",
    "title": "Week 4 Guide",
    "section": "",
    "text": "This week is about applying the linear model concepts you learned in Week 3. Rather than introducing new algorithms, you will work through two case studies that require you to interpret model outputs, identify overfitting and underfitting patterns, and make recommendations based on the results.\nIn Week 3, you learned how linear models work: the weighted prediction formula, regularization, and the parameters that control model complexity (alpha for Ridge, C for Logistic Regression). This week, you will practice using that knowledge to analyze realistic scenarios where a data scientist has already trained models and recorded the results.\nThe primary deliverable this week is a video reflection in which you discuss both case studies and demonstrate your understanding of linear models. This format gives you practice explaining technical concepts verbally—a skill that matters in real-world data science work.\nWeek 4 Assignment:\n\nHere is the link to the Week 4 Assignment.",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 4 (Current Week)"
    ]
  },
  {
    "objectID": "weekly-overview/week-04/index.html#what-youre-practicing-this-week",
    "href": "weekly-overview/week-04/index.html#what-youre-practicing-this-week",
    "title": "Week 4 Guide",
    "section": "What you’re practicing this week",
    "text": "What you’re practicing this week\n\nInterpreting model outputs\nIn the case studies, you will see tables of training and test scores (R² for regression, accuracy for classification) across multiple models with different parameter settings. Your job is to:\n\nIdentify which model is overfitting (high training score, lower test score)\nIdentify which model is underfitting (both scores low)\nRecommend which model to use based on generalization (test performance)\n\nThis is the same skill you practiced in the Week 3 demo, but now applied to new scenarios.\n\n\nConnecting parameters to patterns\nEach case study includes models trained with different values of alpha (Ridge) or C (Logistic Regression). You should be able to explain:\n\nHow alpha affects Ridge Regression: larger alpha → smaller weights → simpler model\nHow C affects Logistic Regression: larger C → less regularization → more complex model\nWhy these parameters work in opposite directions\n\n\n\nReading coefficients\nThe case studies include learned coefficients from trained models. You should be able to:\n\nIdentify which features have the strongest influence (largest absolute weight)\nInterpret what a negative coefficient means (as that feature increases, the prediction decreases)\nExplain why this interpretability is an advantage of linear models over kNN\n\n\n\nComparing linear models to kNN\nThe video reflection asks you to compare linear models to the kNN algorithm from Week 2. You should be able to explain:\n\nWhy linear models are more interpretable (you can examine coefficients)\nWhy linear models handle high-dimensional data better than kNN\nWhen you might choose one approach over the other",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 4 (Current Week)"
    ]
  },
  {
    "objectID": "weekly-overview/week-04/index.html#key-concepts-to-review-from-week-3",
    "href": "weekly-overview/week-04/index.html#key-concepts-to-review-from-week-3",
    "title": "Week 4 Guide",
    "section": "Key concepts to review from Week 3",
    "text": "Key concepts to review from Week 3\nIf any of these feel unclear, revisit the Week 3 demo and Chapter 2 (pages 45-70) before completing the assignment.\n\nThe linear prediction formula\nLinear models predict using: ŷ = w₁×x₁ + w₂×x₂ + ... + wₚ×xₚ + b\n\nWeights (w): show how much each feature contributes to the prediction\nIntercept (b): shifts the prediction baseline up or down\nThe model learns these values during training\n\n\n\nRegression vs classification\n\nRegression: predict a continuous number (use Ridge Regression, measure with R²)\nClassification: predict a category label (use Logistic Regression, measure with accuracy)\n\n\n\nOverfitting vs underfitting\n\nOverfitting: model is too complex, fits training data well but generalizes poorly (training score &gt;&gt; test score)\nUnderfitting: model is too simple, performs poorly on both training and test data (both scores low)\nGood generalization: training and test scores are reasonably close, with acceptable test performance\n\n\n\nRegularization and complexity parameters\n\nRidge (alpha): larger alpha → more regularization → simpler model → smaller coefficients\nLogistic Regression (C): larger C → less regularization → more complex model → larger coefficients\nThese work in opposite directions, which takes practice to remember\n\n\n\nR² vs accuracy\n\nR² (regression): proportion of variance explained, ranges 0 to 1, higher is better\nAccuracy (classification): proportion of correct predictions, ranges 0 to 1, higher is better",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 4 (Current Week)"
    ]
  },
  {
    "objectID": "weekly-overview/week-04/index.html#week-4-tasks",
    "href": "weekly-overview/week-04/index.html#week-4-tasks",
    "title": "Week 4 Guide",
    "section": "Week 4 tasks",
    "text": "Week 4 tasks\n\nReview Chapter 2, pages 45-70 as needed (focus on Ridge and Logistic Regression sections).\nRead through Case Study 1 and Case Study 2 in the Week 4 assignment.\nReview Scenario C (email spam detection) for the comparison discussion.\nRecord and submit your video reflection addressing all four sections.",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 4 (Current Week)"
    ]
  },
  {
    "objectID": "weekly-overview/week-02/index.html",
    "href": "weekly-overview/week-02/index.html",
    "title": "Week 2 Guide",
    "section": "",
    "text": "This week focuses learning the core structure that Chapter 2 uses for the rest of supervised learning:\n\nhow we name the inputs (X, the feature columns) and the output (y, the target/label),\nhow we decide whether a task is classification or regression,\nwhy generalization is the real goal (not “doing well on the training rows”),\nand how overfitting vs underfitting connects to model complexity.\n\nThen the chapter uses k-nearest neighbors (kNN) as the first algorithm where you can see these ideas clearly."
  },
  {
    "objectID": "weekly-overview/week-02/index.html#week-2-focus",
    "href": "weekly-overview/week-02/index.html#week-2-focus",
    "title": "Week 2 Guide",
    "section": "",
    "text": "This week focuses learning the core structure that Chapter 2 uses for the rest of supervised learning:\n\nhow we name the inputs (X, the feature columns) and the output (y, the target/label),\nhow we decide whether a task is classification or regression,\nwhy generalization is the real goal (not “doing well on the training rows”),\nand how overfitting vs underfitting connects to model complexity.\n\nThen the chapter uses k-nearest neighbors (kNN) as the first algorithm where you can see these ideas clearly."
  },
  {
    "objectID": "weekly-overview/week-02/index.html#chapter-2-concepts",
    "href": "weekly-overview/week-02/index.html#chapter-2-concepts",
    "title": "Week 2 Guide",
    "section": "Chapter 2 Concepts",
    "text": "Chapter 2 Concepts\n\nClassification vs regression\nChapter 2 starts by defining the two major supervised learning task types:\n\nClassification: predict a class label from a fixed list of possibilities (binary or multiclass).\nRegression: predict a numeric value where “in-between” outcomes make sense (continuity).\n\nA good reading habit here: ask “Does the output have continuity (regression) or distinct categories (classification)?”\n\n\nGeneralization as the goal\nThe book is explicit: we train on one set of examples, but we care about performance on new, unseen data. That ability is generalization.\nIt also emphasizes why training accuracy can be misleading using a “boat buyer” example: you can make a complex rule that matches the training rows perfectly, but that doesn’t mean it will work for new customers.\n\n\nOverfitting vs underfitting\nChapter 2 frames these as the two common failure modes against the generalization goal:\n\nOverfitting: model fits the training set “too closely,” performs well on training but worse on new data.\nUnderfitting: model is too simple to capture the pattern; does poorly even on training.\n\n\n\nModel complexity vs dataset size\nA distinct concept (separate from “overfitting is bad”): how complex a model you can use depends on how much variety you have in your training data—more variety/larger datasets can support more complex models without overfitting."
  },
  {
    "objectID": "weekly-overview/week-02/index.html#knn-the-first-algorithm",
    "href": "weekly-overview/week-02/index.html#knn-the-first-algorithm",
    "title": "Week 2 Guide",
    "section": "kNN: the first algorithm",
    "text": "kNN: the first algorithm\n\nWhat the algorithm is\nThe book describes kNN as “arguably the simplest” algorithm: building the model is storing the training set. Predictions happen by finding the closest training points (“neighbors”).\nk is the knob that changes complexity:\n\nsmall k → more flexible / can overfit\nlarger k → smoother / can underfit"
  },
  {
    "objectID": "weekly-overview/week-02/index.html#reading-expectations-for-week-2",
    "href": "weekly-overview/week-02/index.html#reading-expectations-for-week-2",
    "title": "Week 2 Guide",
    "section": "Reading expectations for Week 2",
    "text": "Reading expectations for Week 2\nAs you read, keep a running checklist:\n\nCan you identify X and y in each example?\nIs it classification or regression, and why?\nWhat would it mean for the model to generalize here?\nIf performance differs between training and test, is that pointing toward overfitting or underfitting?\nFor kNN specifically: how does changing k move you along the complexity scale?"
  },
  {
    "objectID": "weekly-overview/week-02/index.html#week-2-tasks",
    "href": "weekly-overview/week-02/index.html#week-2-tasks",
    "title": "Week 2 Guide",
    "section": "Week 2 Tasks",
    "text": "Week 2 Tasks\n\nWatch the Week 2 demo overview in your cmsc-1217 YouTube playsist.\n\n\n\nHere is the link to the Week 2 demo.\n Download Jupyter Notebook \n\n\nRead Chapter 2 through the kNN section, and stop before Linear Models.\nComplete the Chapter 2 D2L quiz"
  },
  {
    "objectID": "weekly-overview/week-02/index.html#knowledge-goals-for-week-2",
    "href": "weekly-overview/week-02/index.html#knowledge-goals-for-week-2",
    "title": "Week 2 Guide",
    "section": "Knowledge goals for Week 2",
    "text": "Knowledge goals for Week 2\nBy the end of the week, you should be able to:\n\ndecide whether a task is classification or regression and explain why\ndefine generalization in plain terms and explain why test performance matters\nexplain overfitting vs underfitting using the training/test pattern\ndescribe what kNN does at prediction time (neighbors + voting; mean for regression)\nexplain how changing k changes complexity and can shift generalization performance"
  },
  {
    "objectID": "shared/policies/office-hours.html",
    "href": "shared/policies/office-hours.html",
    "title": "Office Hours",
    "section": "",
    "text": "Mondays - 3:00–4:00 PM and 6:00–7:00 PM\nPersistent Zoom link: Join Zoom meeting",
    "crumbs": [
      "Home",
      "Course information",
      "Office Hours"
    ]
  },
  {
    "objectID": "shared/policies/office-hours.html#no-office-hours",
    "href": "shared/policies/office-hours.html#no-office-hours",
    "title": "Office Hours",
    "section": "No office hours",
    "text": "No office hours\nOffice hours are not held on:\nJan 19 — Martin Luther King — No Classes — College Closed\nFeb 16 — President’s Day — No Classes — College Closed\nMar 9–13 — Spring Break — No Classes\nMay 11–15 — Final Exams",
    "crumbs": [
      "Home",
      "Course information",
      "Office Hours"
    ]
  },
  {
    "objectID": "shared/policies/general-policies.html",
    "href": "shared/policies/general-policies.html",
    "title": "General Course Policies",
    "section": "",
    "text": "Academic integrity\n\nStudents must maintain academic integrity (no cheating, plagiarism, unauthorized materials, falsification, etc.).\nViolations follow SCTCC Academic Integrity Policy/Procedure and may be recorded; disciplinary action may apply.\nAcademic integrity violations result in a grade reduction.\n\n\n\nProfessional conduct and behavior\n\nOnline discussions must be civil, respectful, and relevant.\nDisruptive behavior on any course communication platform is not tolerated; may result in grade penalties and/or removal per the Code of Student Conduct.\nRespect for differing viewpoints is required; maintain professional language and conduct.\nIf disruptive behavior causes a student to miss an assignment, they receive no credit for that assignment.\nIf sanctioned for disruptive behavior, students waive the right to a warning on a second occurrence.\nSubstance policy for online sessions: inebriation/being under the influence, displaying alcohol, or drug paraphernalia results in removal from the session and/or course; no smoking during online school activites.\n\n\n\nStudent privacy / third-party communication\n\nCourse progress is discussed only with the student, not parents/guardians/third parties, unless an Information Release is on file.\nIf a third party contacts the instructor, the instructor will defer communication to the student.\nStudents may not invite third parties to attend any online activity (office hours, group meetings).\nRequests to release progress information to a third party should go through the Dean’s office.\n\n\n\nAttendance / participation\n\nAttendance is expected and monitored; students are responsible for tracking absences.\nAttendance is determined by D2L logins and weekly assignment submissions.\nNot logging in and/or not submitting work for a week counts as an absence (including weeks without due assignments unless it’s a scheduled holiday/break).\nStudents must log in and complete an activity by 11:30 PM on the first Wednesday of week 1 or they may be dropped.\nMissing two consecutive weeks → dropped.\nMissing two total weeks → one-letter-grade reduction (10% of total points).\nMissing three total weeks → an additional one-letter-grade reduction.\nAdditional absences beyond three → additional letter-grade reduction per occurrence.\nAttendance is handled per class (one class doesn’t cover another).\n\n\n\nTechnology and platform expectations\n\nStudents are responsible for having required computer/technology for the course.\nStudents are responsible for learning and using D2L (primary channel for schedules, grades, and announcements).\n\n\n\nEmail communication\n\nEmail is the primary method to contact the instructor; students must use their college email account.\nEmails must include a subject line with course name and section/class time.\nStudents are expected to check email daily and read instructor messages/attachments.\nOn nights homework is due, only administrative email items will be answered; technical homework questions will not be addressed.\nResponse time: by end of next business day; Friday emails answered Monday.",
    "crumbs": [
      "Home",
      "Course information",
      "General Policies"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html",
    "href": "shared/guides/setup/miniconda.html",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "",
    "text": "You will use the same core setup in both of these courses:\nThe difference between the courses is which environment/packages you install and which tools are emphasized. This page includes small callouts to help you follow the right parts at the right time.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#overview",
    "href": "shared/guides/setup/miniconda.html#overview",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "Overview",
    "text": "Overview\nIn these courses, you will work inside a conda environment. An environment is an isolated Python installation (Python + packages) stored in a folder on your computer. This helps everyone use consistent tools and reduces setup conflicts.\nYou will use the following tools:\n\nMiniconda Miniconda is a small installer that provides Conda, a system for managing Python installations and packages. Miniconda does not include many data-science packages by default; instead, it gives you a controlled way to install exactly what a course requires.\nConda (the environment manager) Conda is software that can create, activate, and manage environments. It can also install packages (libraries) in a way that keeps them consistent and reduces compatibility problems, especially for scientific packages.\nConda environment A conda environment is a self-contained folder that contains:\n\na specific version of Python, and\na specific set of installed packages. Each environment is separate from other environments on your computer. This means one course can use one set of packages (and versions) without affecting another course or your personal Python installation.\n\nJupyterLab (notebook workspace) JupyterLab is a browser-based programming environment used for data analysis and machine learning. It allows you to write code in cells, run those cells, and view output (tables, plots, printed results) directly below the code. JupyterLab is used in both courses because it supports step-by-step work and makes it easier to review results.\nIPython (interactive Python terminal — CMSC-1217 only) IPython is an enhanced interactive Python terminal. It provides features that support exploratory work, such as command history, tab completion, and built-in help. In CMSC-1217, IPython is used to practice interactive exploration and reinforce core Python concepts in a terminal-based workflow.\n\nHow these pieces fit together\nMiniconda installs Conda, and Conda is the tool you use to create a course environment. When you activate that environment, your terminal temporarily switches so that the python command (and tools like jupyter) run from the environment’s folder. You then start Jupyter (JupyterLab or Jupyter Notebook), and the notebook runs Python from the active environment, using the packages installed there. This sequence—install Miniconda → create environment → activate environment → start Jupyter → run notebooks—is the standard workflow you will use throughout the course.\n\nImportant: During Miniconda installation, do not add Miniconda to PATH. We will use the Miniconda/Anaconda Prompt when working for these courses.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#install-miniconda",
    "href": "shared/guides/setup/miniconda.html#install-miniconda",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "1) Install Miniconda",
    "text": "1) Install Miniconda\n\nDownload and install Miniconda (64-bit) for Windows. https://www.anaconda.com/download\nDuring installation, do not add Miniconda to PATH.\n\nWhy not add to PATH: Many of you already have a Python installation from python.org. Not adding Miniconda to PATH prevents Miniconda from changing which python runs in non-course terminals.\nMiniconda is a lightweight installer that provides:\n\nConda (a package and environment manager), and\na basic Python installation and its dependencies.\n\nMiniconda is intentionally minimal: it does not install a large collection of data-science packages by default. Instead, you install only the packages required for the course.\nRelationship to Anaconda\nMiniconda and Anaconda are closely related:\n\nAnaconda Distribution is a larger installer that comes with hundreds of packages pre-installed, plus optional tools like Anaconda Navigator.\nMiniconda is a smaller installer that includes only conda, Python, and a small core set of packages, and then you add what you need.\n\nFor these courses, Miniconda is a good fit because it helps us keep your course environment clean, consistent, and easier to troubleshoot.\nWhy Miniconda/Conda is common in data analysis and machine learning\nData analysis and machine learning often rely on libraries that include compiled components. Conda is widely used in these areas because it can install many scientific packages as prebuilt binaries, which reduces installation difficulties (especially on Windows).\nConda also supports creating separate environments so different projects can use different versions of Python/packages without interfering with each other.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#open-the-conda-terminal",
    "href": "shared/guides/setup/miniconda.html#open-the-conda-terminal",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "2) Open the conda terminal",
    "text": "2) Open the conda terminal\nOpen Anaconda Command Prompt from the Start menu.\nYou should see something like:\n(base) C:\\Users\\yourname&gt;\nWhy we use this terminal\nAnaconda/Miniconda Prompt is a terminal that is already configured to recognize conda commands such as conda createand conda activate. Using this prompt reduces setup errors because it starts with the correct conda settings.\nWhat (base) means\n(base) indicates that you are currently in conda’s default environment. This environment exists mainly so that conda can run and manage other environments. In this course, you will usually not do your work in (base). Instead, you will create and activate a course environment (for example, pydata-book or cmsc-2208) and work there.\nHow to interpret the prompt\nThe name in parentheses is your current environment:\n\n(base) means you are in the default conda environment\n(pydata-book) or (cmsc-2208) means you are in the course environment\n\nA quick habit that prevents many problems: before running course commands, look at the parentheses and confirm you are in the correct environment.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#configure-conda-package-channels",
    "href": "shared/guides/setup/miniconda.html#configure-conda-package-channels",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "3) Configure conda package channels",
    "text": "3) Configure conda package channels\nRun the following two commands, one at a time, pressing Enter after each.\nFirst, run:\nconda config --add channels conda-forge\nThen, run:\nconda config --set channel_priority strict\nConda downloads packages from sources called channels. A channel is a repository that hosts packaged software (similar in purpose to an “app store,” but for programming libraries). When you install a package with conda, conda searches the configured channels to find a compatible version of that package and its dependencies.\nWe use conda-forge because it is a widely used community-maintained channel that provides current, well-tested builds of many scientific and data-analysis libraries (for example, NumPy, pandas, matplotlib, and scikit-learn). Using a consistent channel helps ensure that most students receive the same package builds and reduces platform-specific installation problems.\nSetting strict channel priority means conda will prefer packages from the highest-priority channel (in this case, conda-forge) rather than mixing packages from multiple channels. This matters because mixing package sources can sometimes lead to version conflicts or dependency mismatches. Strict priority reduces those problems by keeping package selection consistent.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#create-the-course-environment",
    "href": "shared/guides/setup/miniconda.html#create-the-course-environment",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "4) Create the course environment",
    "text": "4) Create the course environment\nYou will create one conda environment for the course you are taking currently.\nWhat an environment is:\nIn programming, an environment is the set of software used to run your code, such as:\n\nthe language runtime (for us, Python),\ninstalled libraries/packages,\nand related tools (such as Jupyter).\n\nMany languages and toolchains support environments (for example, Python can use virtual environments, and other ecosystems have their own dependency managers). The goal is the same: keep project dependencies organized and consistent.\nWhat a conda environment is:\nIn this course we use a conda environment, which is a self-contained folder on your computer that includes:\n\na specific version of Python, and\na specific set of installed packages.\n\nWe use conda because it provides a consistent way to install scientific and data-analysis libraries (especially on Windows) and makes it straightforward to reproduce the same setup across different computers.\nDo not create both environments.\nIf you are in CMSC-1217 Introduction to Data Analytics\nRun this command (then press Enter):\nconda create -n pydata-book python=3.11\nIf prompted, type y.\nThis creates an environment named pydata-book. The name is just a label for this course’s Python setup.\nIf you are in CMSC-2208 Introduction to Machine Learning\nRun this command (then press Enter):\nconda create -n cmsc-2208 python=3.11\nIf prompted, type y.\nThis creates an environment named cmsc-2208. The name is just a label for this course’s Python setup.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#activate-the-environment-choose-your-course",
    "href": "shared/guides/setup/miniconda.html#activate-the-environment-choose-your-course",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "5) Activate the environment (choose your course)",
    "text": "5) Activate the environment (choose your course)\nBefore installing packages or starting JupyterLab, you must activate your course environment.\nWhat “activate” means: It tells the terminal to use the Python and packages inside your course environment. When the environment is active, commands like python and jupyter will run from the correct course setup.\nYou will activate one environment: the one for the course you are taking now.\n\nIf you are in CMSC-1217 Introduction to Data Analytics\nRun:\nconda activate pydata-book\n\n\nIf you are in CMSC-2208 Introduction to Machine Learning\nRun:\nconda activate cmsc-2208\nAfter you activate, your prompt will begin with the environment name, for example:\n\n(pydata-book) ...\n(cmsc-2208) ...\n\nRule: Before you do course work, confirm that the prompt shows your course environment name.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#install-packages-inside-the-active-environment",
    "href": "shared/guides/setup/miniconda.html#install-packages-inside-the-active-environment",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "6) Install packages (inside the active environment)",
    "text": "6) Install packages (inside the active environment)\n\nA) CMSC-1217 packages\nWith (pydata-book) active, run:\nconda install -c conda-forge numpy pandas=2.0.3 matplotlib jupyterlab notebook ipykernel\nThis installs:\n\nnumpy: arrays and numerical computing\npandas: data tables (DataFrames)\nmatplotlib: plotting\njupyterlab: notebook interface\nipykernel: the component that allows this environment to run notebooks as a Jupyter kernel\n\n\nCMSC-1217 only: You will also use IPython in this course. IPython will work once the environment is active.\n\n\n\nB) CMSC-2208 packages (required)\nWith (cmsc-2208) active, run:\nconda install -c conda-forge numpy pandas matplotlib jupyterlab notebook ipykernel scikit-learn scipy\nThis installs the data stack plus the machine learning libraries used in CMSC-2208.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#verify-your-installation-imports-version-confirmation",
    "href": "shared/guides/setup/miniconda.html#verify-your-installation-imports-version-confirmation",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "7) Verify your installation (imports + version confirmation)",
    "text": "7) Verify your installation (imports + version confirmation)\nRun this command in the active environment:\npython -c \"import sys; import numpy as np; import pandas as pd; import matplotlib; print('python', sys.version.split()[0]); print('numpy', np.__version__); print('pandas', pd.__version__); print('matplotlib', matplotlib.__version__)\"\nWhat this step is doing (and why it can take a moment):\nThis command does two things:\n\nImports the core libraries (numpy, pandas, matplotlib). Importing is the real verification step. It confirms the packages are installed correctly in this environment and that Python can load them without errors.\nPrints version numbers after the imports succeed. This helps confirm you are using the expected tool versions for the course.\n\nBecause these libraries are large and include compiled components, the first successful import in a new environment may take longer than you expect. That is normal.\n\nCMSC-2208 only\nFor CMSC-2208, also verify:\npython -c \"import sklearn, scipy; print('sklearn', sklearn.__version__); print('scipy', scipy.__version__)\"\nThis confirms that the machine learning libraries required for the course can be imported correctly in the CMSC-2208 environment.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#create-the-course-folder-structure-choose-your-course",
    "href": "shared/guides/setup/miniconda.html#create-the-course-folder-structure-choose-your-course",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "8) Create the course folder structure (choose your course)",
    "text": "8) Create the course folder structure (choose your course)\nYou will create a small folder structure on your C: drive so your files stay organized and easy to find.\nRun the commands for your course only. Run them one at a time, pressing Enter after each command.\n\nIf you are in CMSC-1217 Introduction to Data Analytics\nRun the following four commands:\nFirst:\nmkdir C:\\cmsc-1217\nThen:\nmkdir C:\\cmsc-1217\\course-work\nThen:\nmkdir C:\\cmsc-1217\\book\nFinally:\nmkdir C:\\cmsc-1217\\data\n\n\nIf you are in CMSC-2208 Introduction to Machine Learning\nRun the following four commands:\nFirst:\nmkdir C:\\cmsc-2208\nThen:\nmkdir C:\\cmsc-2208\\course-work\nThen:\nmkdir C:\\cmsc-2208\\book\nFinally:\nmkdir C:\\cmsc-2208\\data\n\n\nWhat these folders are for (both courses)\n\ncourse-work: your weekly work and submissions\nbook: companion notebooks (reference materials)\ndata: datasets used in course work\n\nNote: If you see a message like “A subdirectory or file already exists,” that usually means you already created the folder. That is fine—continue to the next command.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#install-git-if-needed",
    "href": "shared/guides/setup/miniconda.html#install-git-if-needed",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "9) Install Git (if needed)",
    "text": "9) Install Git (if needed)\nCheck:\ngit --version\nIf Git is not installed, install Git for Windows, then re-check git --version.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#download-the-book-companion-files-choose-your-course",
    "href": "shared/guides/setup/miniconda.html#download-the-book-companion-files-choose-your-course",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "10) Download the book companion files (choose your course)",
    "text": "10) Download the book companion files (choose your course)\nEach course uses a GitHub repository that contains companion files (notebooks, datasets, and examples). You will download the repository into your course book folder.\nRun the commands for your course only. Run them one at a time, pressing Enter after each command.\n\nIf you are in CMSC-1217 Introduction to Data Analytics\nFirst, move into your book folder:\ncd C:\\cmsc-1217\\book\nThen, clone the companion repository:\ngit clone -b 3rd-edition https://github.com/wesm/pydata-book.git\nThis creates the folder:\nC:\\cmsc-1217\\book\\pydata-book\\\n\n\nIf you are in CMSC-2208 Introduction to Machine Learning\nFirst, move into your book folder:\ncd C:\\cmsc-2208\\book\nThen, clone the companion repository:\ngit clone https://github.com/amueller/introduction_to_ml_with_python.git\nThis creates the folder:\nC:\\cmsc-2208\\book\\introduction_to_ml_with_python\\\nNote: If you see a message that the folder already exists, that usually means you already cloned the repository. That is fine—do not clone it again.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#start-jupyter-notebook-in-the-correct-folder-choose-yourcourse",
    "href": "shared/guides/setup/miniconda.html#start-jupyter-notebook-in-the-correct-folder-choose-yourcourse",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "11) Start Jupyter Notebook in the correct folder (choose yourcourse)",
    "text": "11) Start Jupyter Notebook in the correct folder (choose yourcourse)\nYou will start the Jupyter Notebook (Classic) server from your course-work folder. This ensures that when the browser opens, you immediately see the files you are expected to edit and submit.\nRun the commands for your course only. Run them one at a time, pressing Enter after each command.\n\nIf you are in CMSC-1217 Introduction to Data Analytics\nFirst, confirm you are in the correct course environment. Conda environments are activated per terminal window. That means:\n\nIf you are using the same Anaconda/Miniconda Prompt window as earlier steps, your environment may already be active.\nIf you opened a new terminal window (or restarted your computer), you must activate the environment again.\n\nRun:\nconda activate pydata-book\nCheck: your prompt should begin with (pydata-book).\nNext, move into your course work folder:\ncd C:\\cmsc-1217\\course-work\nThen start Jupyter Notebook (Classic):\njupyter notebook\n\n\nIf you are in CMSC-2208 Introduction to Machine Learning\nFirst, confirm you are in the correct course environment. Conda environments are activated per terminal window. That means:\n\nIf you are using the same Anaconda/Miniconda Prompt window as earlier steps, your environment may already be active.\nIf you opened a new terminal window (or restarted your computer), you must activate the environment again.\n\nRun:\nconda activate cmsc-2208\nCheck: your prompt should begin with (pydata-book).\nNext, move into your course work folder:\ncd C:\\cmsc-2208\\course-work\nThen start Jupyter Notebook (Classic):\njupyter notebook\nA browser tab will open showing the contents of your course-work folder.\nImportant: Leave the terminal window open while you use Jupyter Notebook. That terminal is running the notebook server. If you close it, Jupyter will stop.\n\n\nAccessing the book folder (when needed)\nThe course keeps course-work (your work) and book (reference materials) separate on purpose. When you need to view a book notebook:\nStart a second Jupyter Notebook server in the book folder.\n\nOpen a second Anaconda/Miniconda Prompt window\nActivate the same environment\ncd into the book folder\nRun jupyter notebook\n\nFor CMSC-1217 (run each command seperately):\nconda activate pydata-book\ncd C:\\cmsc-1217\\book\njupyter notebook\nFor CMSC-2208 (run each command seperately):\nconda activate cmsc-2208\ncd C:\\cmsc-2208\\book\njupyter notebook\nThis opens a second browser tab rooted in the book folder.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#create-your-first-notebook-and-confirm-the-environment",
    "href": "shared/guides/setup/miniconda.html#create-your-first-notebook-and-confirm-the-environment",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "12) Create your first notebook and confirm the environment",
    "text": "12) Create your first notebook and confirm the environment\n\nIn your browser, go to the file list page (the page that shows folders/files).\n\nIf you are currently inside a notebook, click File → Open… to return to the file list.\n\nClick New (top-right), then select Python 3 (ipykernel).\nIn the first cell, run:\n\nimport sys\nsys.executable\n\nConfirm the path includes your course environment:\n\n\nCMSC-1217: \\\\envs\\\\pydata-book\\\\\nCMSC-2208: \\\\envs\\\\cmsc-2208\\\\\n\nWhy this matters: The kernel name shown in menus can vary, but sys.executable reliably confirms the notebook is using the correct environment.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#stop-jupyterlab-when-finished",
    "href": "shared/guides/setup/miniconda.html#stop-jupyterlab-when-finished",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "13) Stop JupyterLab when finished",
    "text": "13) Stop JupyterLab when finished\nGo back to the terminal that started JupyterLab and press:\n\nCtrl + C, then type y if prompted.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#optional-cmsc-1217-ipython-in-the-terminal",
    "href": "shared/guides/setup/miniconda.html#optional-cmsc-1217-ipython-in-the-terminal",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "Optional (CMSC-1217): IPython in the terminal",
    "text": "Optional (CMSC-1217): IPython in the terminal\n\nCMSC-1217 only: You will use IPython for interactive exploration.\n\nIn Anaconda/Miniconda Prompt:\nconda activate pydata-book\nIPython\nYou should see an In [1]: prompt.\nTo exit:\nexit()",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/miniconda.html#common-issues-and-solutions",
    "href": "shared/guides/setup/miniconda.html#common-issues-and-solutions",
    "title": "Shared Setup (Windows) — Miniconda, Course Environments, IPython, and JupyterLab",
    "section": "Common issues and solutions",
    "text": "Common issues and solutions\n\nIssue: Imports fail inside a notebook\nMost common cause: the wrong environment/kernel is active.\nSolution\n\nIn the notebook, run sys.executable to confirm the environment path.\nIf it is incorrect, switch the notebook kernel to the correct one:\n\nKernel → Change Kernel (select the environment for your course)\n\n\n\n\nIssue: JupyterLab opens in the wrong folder\nSolution: cd into the correct course-work folder before starting JupyterLab.\n\n\nIssue: You forgot to activate the environment\nSolution: close JupyterLab (Ctrl+C), then re-run the launch commands in Section 11.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Miniconda Setup"
    ]
  },
  {
    "objectID": "shared/guides/setup/environment-setup-guide-vscode.html",
    "href": "shared/guides/setup/environment-setup-guide-vscode.html",
    "title": "Week 1 — VSCode Setup and First Notebook (Windows)",
    "section": "",
    "text": "What you’re doing\nYou are going to open a Jupyter notebook in VS Code and run it successfully using the course Python environment.\n\n\n1) Open the course folder in VS Code\n\nOpen VS Code\nGo to File → Open Folder…\nOpen your course work folder (example: C:\\cmsc-1217\\)\n\n\n\n2) Install the extensions (if you don’t already have them)\nIn VS Code, install:\n\nPython (Microsoft)\nJupyter (Microsoft)\n\n\n\n3) Create the course Python environment and select it in VS Code (one-time)\n\n3A) Create the course environment folder\n\nCreate a folder on your C: drive named:\n\nC:\\cmsc-1217-env\\\n\n\n\n\n3B) Create the virtual environment (.venv)\n\nOpen Command Prompt (Windows search → type “Command Prompt”)\nRun:\n\ncd C:\\cmsc-1217-env\npython -m venv .venv\n\n\n3C) Activate the environment\nIn the same Command Prompt window, run:\nC:\\cmsc-1217-env\\.venv\\Scripts\\activate.bat\nYou should see (.venv) appear at the start of the line.\n\n\n3D) Install the notebook packages (first-time setup)\nStill in the activated environment, run:\npython -m pip install --upgrade pip\npython -m pip install numpy pandas matplotlib jupyter\n\n\n3E) Select the environment in VS Code (Interpreter)\n\nIn VS Code, press Ctrl + Shift + P\nType Python: Select Interpreter\nChoose the interpreter that looks like:\n\n\nC:\\cmsc-1217-env\\.venv\\Scripts\\python.exe\n\nIf you don’t see it:\n\nchoose Enter interpreter path…\npaste: C:\\cmsc-1217-env\\.venv\\Scripts\\python.exe\n\nThis is the environment we will use all semester.\n\n\n\n4) Open the notebook\n\nIn the Explorer panel, click the notebook file (.ipynb) your instructor provided.\n\n\n\n5) Select the notebook kernel (first time only)\nThe first time you run a notebook, VS Code may ask you to pick a kernel.\nChoose the kernel that matches the course environment path:\n\nC:\\cmsc-1217-env\\.venv\\Scripts\\python.exe\n\nThis can feel repetitive the first time. That’s normal. Once selected, it usually stays set for that folder.\n\n\n6) Select the kernel, then run a quick test\n\nIn the top-right of the notebook, click Select Kernel.\nChoose the kernel that points to the course environment:\nC:\\cmsc-1217-env\\.venv\\Scripts\\python.exe\n(It may also show up as “Python 3.x (.venv)”—the key is that it points to cmsc-1217-env\\.venv.)\nClick + Code.\nIn the code block that appears, paste:\n\nprint(\"Notebook is running\")\n\nRun it by clicking the Run (▶) icon on the code block, or press Shift + Enter.\nYou should see Notebook is running appear underneath.\n\n\n\n7) Quick confirmation (your instructor may ask for this)\nRun this in a cell:\nimport sys\nsys.executable\nIt should show something like:\nC:\\cmsc-1217-env\\.venv\\Scripts\\python.exe\n\n\nIf something goes wrong\n\nIf you see ModuleNotFoundError (missing package), you are almost always using the wrong interpreter/kernel.\n\nRe-do Step 3 and Step 5 and pick the one that points to cmsc-1217-env\\.venv."
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "",
    "text": "These are optional “extra help” videos for Week 2.\nWatch them when the reading feels abstract or when you want a second explanation of the same concept.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#how-to-use-these-videos",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#how-to-use-these-videos",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "",
    "text": "These are optional “extra help” videos for Week 2.\nWatch them when the reading feels abstract or when you want a second explanation of the same concept.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#classification-vs-regression",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#classification-vs-regression",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "Classification vs Regression",
    "text": "Classification vs Regression\nRegression vs. Classification — What’s the difference?\nby AWS Educate (2020s, ~5–10 min) — Beginner explanation of what changes when your target is a number vs a label.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#generalization-why-we-split-into-training-vs-test",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#generalization-why-we-split-into-training-vs-test",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "Generalization: why we split into training vs test",
    "text": "Generalization: why we split into training vs test\nTrain/Test Split (and why training accuracy can mislead you)\nby StatQuest (Josh Starmer) (2018–2020s, ~10–15 min) — Why we don’t evaluate on the same rows we trained on.\n\nCross Validation (bonus idea for small datasets)\nby StatQuest (Josh Starmer) (2018–2020s, ~10–15 min) — Why results can be unstable when you don’t have many rows.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#underfitting-and-overfitting",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#underfitting-and-overfitting",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "Underfitting and Overfitting",
    "text": "Underfitting and Overfitting\nOverfitting (video)\nby deeplizard (~4 min) — Explains what overfitting is and what it tends to look like when you compare training vs test performance.\n\nUnderfitting (video)\nby deeplizard (~3–4 min) — Explains underfitting (model too simple to learn the real pattern) and why it performs poorly.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#k-nearest-neighbors-knn",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#k-nearest-neighbors-knn",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "k-Nearest Neighbors (kNN)",
    "text": "k-Nearest Neighbors (kNN)\nK-Nearest Neighbors (kNN) clearly explained\nby StatQuest (Josh Starmer) (2018–2020s, ~10–15 min) — Neighbors “vote” and how changing k changes behavior.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#accuracy-what-does-correct-mean",
    "href": "shared/guides/cmsc-2208/week-02/cmsc-2208-chap-02-learning-resources.html#accuracy-what-does-correct-mean",
    "title": "Week 2: Chapter 2 Learning Videos (Supervised Learning + kNN)",
    "section": "Accuracy: what does “correct” mean?",
    "text": "Accuracy: what does “correct” mean?\nThe Confusion Matrix (what accuracy is measuring)\nby StatQuest (Josh Starmer) (2018–2020s, ~10–15 min) — What “correct vs incorrect” means in classification.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 2",
      "Chapter 2 Week 2 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html",
    "href": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html",
    "title": "Week 2: Chapter 3 Additional Reference Videos (Files, Parsing, Comprehensions, Lambda, Generators)",
    "section": "",
    "text": "Try / Except | Python (Tutorial)\nby Socratica — A focused explanation of what try and except actually do, and why this pattern is useful when you want to keep processing after one failure."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#exceptions-and-safe-processing-tryexcept",
    "href": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#exceptions-and-safe-processing-tryexcept",
    "title": "Week 2: Chapter 3 Additional Reference Videos (Files, Parsing, Comprehensions, Lambda, Generators)",
    "section": "",
    "text": "Try / Except | Python (Tutorial)\nby Socratica — A focused explanation of what try and except actually do, and why this pattern is useful when you want to keep processing after one failure."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#list-comprehensions-compact-build-a-new-list-loops",
    "href": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#list-comprehensions-compact-build-a-new-list-loops",
    "title": "Week 2: Chapter 3 Additional Reference Videos (Files, Parsing, Comprehensions, Lambda, Generators)",
    "section": "List comprehensions (compact “build a new list” loops)",
    "text": "List comprehensions (compact “build a new list” loops)\nLearn Python LIST COMPREHENSIONS in 10 minutes!\nby Bro Code — Quick overview of list comprehension syntax, comparing them to regular for loops with many clear examples.\n\nList Comprehensions | Python (Tutorial)\nby CS Dojo — List comprehension basics in Python - let’s go!\n\nPython Tutorial: List Comprehensions\nby Corey Schafer — A deeper walkthrough showing common patterns and how comprehensions relate to the equivalent loop form."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#lambda-a-small-function-used-inline",
    "href": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#lambda-a-small-function-used-inline",
    "title": "Week 2: Chapter 3 Additional Reference Videos (Files, Parsing, Comprehensions, Lambda, Generators)",
    "section": "Lambda (a small function used inline)",
    "text": "Lambda (a small function used inline)\nLearn Python LAMBDA in 6 minutes!\nby Bro Code — Introduces lambda functions as short, anonymous functions and shows common use cases like sorting and mapping.\n\nPython Tutorial: Lambda Functions\nby Corey Schafer — Explains what a lambda is, when it’s useful, and how it connects to patterns like sorted(..., key=...) and max(..., key=...)."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#generators-and-iteration-yield-streaming-one-row-at-a-time",
    "href": "shared/guides/cmsc-1217/week-02/cmsc-1217-chap-03-learning-resources.html#generators-and-iteration-yield-streaming-one-row-at-a-time",
    "title": "Week 2: Chapter 3 Additional Reference Videos (Files, Parsing, Comprehensions, Lambda, Generators)",
    "section": "Generators and iteration (yield, streaming one row at a time)",
    "text": "Generators and iteration (yield, streaming one row at a time)\nPython Generators Explained\nby Programming with Mosh — Clear, beginner-friendly explanation of generators, how they work, and why they’re useful for memory-efficient data processing.\n\nPython Tutorial: Generators - How to use them and the benefits you receive\nby Corey Schafer — A practical explanation of yield, how generator functions behave differently from normal functions, and what changes when you iterate over a generator."
  },
  {
    "objectID": "course/syllabus.html",
    "href": "course/syllabus.html",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "",
    "text": "Instructor(s): Joseph Silman Office: Online\nE-mail: jsilman@sctcc.edu (PRIMARY COMMUNICATION METHOD) Phone: (320)308-6595\nOffice Hours: See Attached Course Schedule in D2L or on Course Page.\nCredits: 3 (Lecture 2; Lab 1)",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#course-description",
    "href": "course/syllabus.html#course-description",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Course Description",
    "text": "Course Description\nThis course introduces students to the fundamentals of machine learning using Python, equipping them with practical, hands-on skills that build on foundational data analytics and advanced programming proficiencies from CMSC1217 and CMSC1236. Students will explore key concepts including supervised and unsupervised learning, data preparation, model training, evaluation, and project structuring. Emphasizing a programming-focused approach, the course enables learners to create simple machine learning tools, ultimately preparing them for advanced technical roles and enhancing their employability in an AI-driven tech industry. By the end of the course, students will be able to design, implement, and evaluate basic machine learning models for real-world problem solving.\nThis online class requires extensive use of your webcam",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#course-outcomes",
    "href": "course/syllabus.html#course-outcomes",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Course Outcomes",
    "text": "Course Outcomes\n\nUpon completion of this course the student will:\n\nDescribe the fundamental programming-based concepts of machine learning, including supervised and unsupervised learning, and their practical programming applications using Python.\nExplain how programming tools automate and enhance machine learning processes for real- world datasets.\nUse Python to implement basic supervised learning models, such as linear regression or classification, on simple datasets.\nCompare different programming-based evaluation metrics to assess model performance effectively for machine learning tasks.\nAssess the effectiveness of data preparation techniques using Python for improving machine learning model outcomes.\nDevelop a machine learning project using Python, integrating data preparation, a basic model, programming-based evaluation, and project structuring.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#course-content",
    "href": "course/syllabus.html#course-content",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Course Content",
    "text": "Course Content\n\nIntroduction to Machine Learning Concepts\nData Preparation for Machine Learning Using Python\nBasic Supervised Learning Using Python\nBasic Unsupervised Learning Using Python\nProgramming-Focused Capstone Machine Learning Project",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#required-materials",
    "href": "course/syllabus.html#required-materials",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Required Materials",
    "text": "Required Materials\nIntroduction to Machine Learning with Python 1st Edition : Andreas C. Muller & Sarah Guido\nComputer running current OS that supports Microsoft Office\nWebcam Required\n*** STUDENTS ARE REQUIRED TO HAVE THEIR MATERIALS AT THE START OF THE COURSE***",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#methods-of-instruction",
    "href": "course/syllabus.html#methods-of-instruction",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Methods of Instruction",
    "text": "Methods of Instruction\nStudent Presentation Problem Solving Internet\nCreative Projects Assigned Reading\nProject Critiques Labs Lecture\nFilm/Slides/Videos Demonstrations",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#gradingevaluation",
    "href": "course/syllabus.html#gradingevaluation",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Grading/Evaluation",
    "text": "Grading/Evaluation\nTests, textbook problems, individual projects, video assignments, and performance will be graded. Grades will be updated in D2L and you are responsible for verifying that your grades are correct. Academic Progress reports will only be sent out to students earning a grade of a “D” or lower. Course Grades will be per the following criteria:\nA = 100 – 90%\nB = 89 – 80%\nC = 79 – 70%\nD = 69 – 60 %\nF = Below 60 %\nGrades are weighted as follows:\n\nLabs and Video Activities - 50%\nFinal Projects – 20%\nD2L Quizzes/Tests – 30%\n\nSome assignments will be based off a pass/fail criterion. Grading Rubrics will be assigned to most course activities. Some assignments are automatically graded. Activities may be assigned but not be graded, these are for self-review to prepare you for graded activities. ‘Proof’ files will be provided to you on occasion to help you identify any issues with your assignment. It is expected that you review these when you receive your rubric feedback. Extra credit assignments or additional assignments are not offered in this course.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#course-policiespractices",
    "href": "course/syllabus.html#course-policiespractices",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Course Policies/Practices",
    "text": "Course Policies/Practices\n\nAcademic Integrity\nAcademic integrity is highly valued at St. Cloud Technical & Community College and throughout higher education. Maintaining academic integrity is the responsibility of every member of the college community: faculty, staff, administrators and students. Academic integrity requires students to refrain from engaging in or tolerating acts including, but not limited to, submitting false academic records, cheating, plagiarizing, altering, forging, or misusing a college academic record; acquiring or using test materials without faculty permission; acting alone or in cooperation with another to falsify records or to obtain dishonest grades, honors, or awards.\nAny violation of the St. Cloud Technical & Community College’s Academic Integrity Policy S3.28 is considered a disciplinary offense and will be subject to the policies of this instructor, entrance into the Academic Integrity Database, and possible disciplinary action as outlined in the Academic Integrity Procedure S3.28.1. Students accused of academic dishonesty may appeal the decision. Students may review the Academic Integrity process and access the Academic Integrity Appeal Form at https://www.sctcc.edu/academic-integrity.\nClass policy for Academic Integrity Violations will result in a reduction of your grade.\nhttp://sctcc.edu/sites/default/files/policies/S3.24%20Complaint%20Grievance.pdf\n\n\nClassroom Behavior\nAttendance is a necessary part of this course. Refer to the attendance policy outlined later in the syllabus\nOnline discussions should be civilized and respectful to everyone and relevant to the topic we are discussing.\nDisruptive behavior on any communication platform will not be tolerated.\nYou are expected to do your own work unless otherwise directed. Cheating, plagiarism, and any other form of academic dishonesty will not be tolerated. Please refer to the Code of Student Conduct and Academic Integrity policy for details.\nAny indication of inebriation or being under the influence, and/or displaying of alcohol or use of paraphernalia that could be associated with drug usage during assignments/course activities will result in removal from the session and/or course. No smoking during any online sessions. My policy is to remove you from the course and leave it to you to file a grievance to return.\nMeaningful and constructive dialogue is encouraged, but this also requires a degree of mutual respect, willingness to listen, and tolerance of opposing points of view. Respect for individual differences and alternative viewpoints will always be expected in this course. One’s words and use of language should be temperate and within acceptable bounds of civility and decency.\nDisruptive behaviors, including excessive talking about off topic items, arriving late to the start of meetings or returning late from breaks, sleeping, reading, or watching media, or game playing is not permitted and will result in a grade penalty.\nExtreme disruptive behavior, fighting (verbally), using repetitive profanity, personal or physical threats or insults, and angry outbursts, will result in your removal from the course in accordance with policies and procedures outlined in the SCTCC’s Code of Student Conduct.\nIf a student’s disruptive behavior causes them to miss an assignment, they will not receive credit for that assignment. If a student has been sanctioned due to disruptive behavior, they waive the right to a warning on a second occurrence.\n\n\nData Privacy and Student Course Progress\nStudents are encouraged to take ownership over their academic progress and communicate with faculty directly for any questions or concerns regarding their coursework. I am reachable at my listed contact information and office hours on this syllabus and happy to discuss your progress with you.\nPer the College’s Student Data Practices policy, I am unable to share information about a student’s course progress to anyone other than the student, including third parties such as parents/guardians, unless a current Information Release is on file with the Records and Registration Office. Faculty are often unable to confirm if an Information Release is on file, so it is always preferred that the student speak with the faculty one-on-one.\nBecause of the items outlined above, should a third party contact me directly with a question or concern about your progress, I will defer my response to you and not the third party. Students shall also not invite third parties to attend class meetings (online), as this causes a disruption to the learning environment.\nIf a student would like their course progress information released to a third party, they may do so by contacting the Dean’s office to make a request for specific information to be shared with the third party.\n\n\nComputer Requirements\nA Computer meeting the program requirements and able to run multiple instances of Windows or Linux OS concurrently (using virtualization) is required for this class.\nComputer requirements are outlined here:\nhttps://www.sctcc.edu/degrees-programs/computer-programming\n\n\nD2L\nA fundamental understanding of D2L is required for this course as it is the primary method to communicate course schedules, grade information, and class news. It is the student’s responsibility to learn to navigate and find information on D2L. SCTCC does offer information sessions on this tool.\n\n\nAttendance/Participation:\nAttendance is expected and monitored. You are responsible for monitoring your attendance and your absence count.\nAttendance will be determined by up to two contributing factors, logging into your D2L Brightspace course, and submission of weekly assignments. Failure to log into class and/or submit assignments for a week will result in an absence. Failure to log into class during a week where an assignment is not due and is not a scheduled holiday/spring break week will result in an absence. I will check logins and assignment submission for the previous week every Monday.\nYou must log into your online class and complete an activity by 11:30 PM on the first Wednesday of the first week of class. Failure to log in will result in being dropped from the course.\nMissing two consecutive weeks of class will result in being dropped from the course.\nMissing two total weeks of class (non-consecutive and any combination of unexcused/excused) will result in a reduction of one letter grade (10% of total points). Missing three total weeks of class will result in an additional reduction of a letter grade. Any absence over three will result in a further reduction of a letter grade for each occurrence. Classes I teach are considered independent of each other with respect to attendance. You may meet attendance requirements in one of my classes but fail attendance requirements in another class.\n\n\nTesting:\nD2L tests are electronic quizzes. These may be scheduled ahead of time or given without warning. If a D2L test is password protected, sharing passwords with another student will constitute an integrity violation.\nTests are not restricted to a specific day for this course but must be completed by the assignment due date as scheduled. You may have multiple attempts at an Online D2L quiz. Time for each question is normally 45 seconds per question. If multiple attempts are allowed, you must score greater than 60% on your first attempt to be allowed a second attempt and you must score greater than an 80% on your second attempt to be allowed a third attempt (if there is a third attempt). If you exceed the time limit on the quiz, the grading engine will mark the test as a 0 and will not allow any further work on your quiz. Always be aware of the time remaining on your quiz. You may not leave a quiz once you start it.\nNote on auto generated D2L Tests: You may not be able to backtrack through the questions on a D2L quiz. The Next Page and Submit Button are close to each other on the test page. It is possible to Submit a quiz inadvertently if you are not paying attention so ensure that you are reading the popup messages.\nTests and quizzes cannot be made up.\n\n\nSchedule\nSubject to change based on instructor and class needs.\n\n\nAssignments/Projects\nAll assignments must be turned in by the due date. Due dates will be announced during class and posted on D2L . Late assignments will not be accepted. A late assignment is any assignment that is not in the dropbox by the due/end date. Sometimes assignments will consist of two parts, a video submission, and a file submission. Missing either portion of the assignment will make the assignment ineligible for grading. Video assignments that cannot be accessed by the instructor at the time of grading will also not be accepted. All projects and assignments should be archived and kept. Any group work assigned is also ineligible for makeup so make sure that you are a responsible, considerate groupmate.\nTypes of assignments you may encounter in this course:\nWeekly practice assignments – are based on the week’s topics. You will normally receive 5 – 7 days to complete these types of assignment.\nProjects – are longer assignments that are bigger in scope than a practice assignment and usually have a timeline with concrete checkpoints. These assignments may be done in a group setting or individually depending on the project.\nVideo Assignments – are assignments in which you will use your webcam and record your desktop to complete a specific activity. Some video assignments are group-based activities. Ensure that you review the assignment and the assignment rubric for specific instructions.\n\n\nDress\nStudents are expected to dress in a manner that is considerate of their classmates and the instructor for any online video assignments.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#standards-for-materials-submitted",
    "href": "course/syllabus.html#standards-for-materials-submitted",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Standards for Materials Submitted",
    "text": "Standards for Materials Submitted\nSpecific standards for submitting assignments will be outlined by the instructor in the assignment instructions. Any assignment that does not follow these standards will not be accepted or subject to a significant grade penalty, this includes not submitting files in the correct file format and/or misnaming the file. If you have a question on a file submission, please ensure that you contact the instructor before the due date.\n\nStudent Email\nStudents will be expected to use email as the primary method of communication with the instructor. Students must use their college email account for all email communication with the instructor. Assignments will not be accepted via email. All students must abide by the following requirements:\n\nIf the subject line is missing, your email will not be accepted. The subject line MUST have your course name and course section/class time.\nStudents will check their email at least once per day.\nStudents will read all emails and attachments sent by the instructor.\nAny attachments submitted via email must pertain to the course.\n\nEmail communication is encouraged, however only administrative items will be answered on the night homework is due. An example of an administrative item is informing me of the birth of a child. Technical questions regarding homework will not be addressed. The purpose behind this policy is to limit procrastination.\nExpect email to be answered by the end of the next business day. Email sent on Friday will be answered the following Monday.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#student-responsibilitiescontributions",
    "href": "course/syllabus.html#student-responsibilitiescontributions",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Student Responsibilities/Contributions",
    "text": "Student Responsibilities/Contributions\n\nAttendance and participation is crucial to succeed in this class.\nEvery student will be required to produce projects based on professional standard for the industry.\nEvery student is expected to turn in all work as assigned.\nEvery student is expected to work cooperatively with classmates and the instructor.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#statement-of-accomodations",
    "href": "course/syllabus.html#statement-of-accomodations",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Statement of Accomodations",
    "text": "Statement of Accomodations\nSt. Cloud Technical & Community College is committed to supporting students with disabilities in obtaining, understanding, and advocating for equitable and inclusive access in all aspects of their education and campus life. It is the role of Accessibility Services to provide and/or arrange reasonable accommodations to qualified students who have a disability (or have acquired a disability) during any point of their tenure at SCTCC. Accommodations are established through collaboration between students, Accessibility Services, faculty, and staff to empower students to pursue their academic goals free from barriers while upholding the integrity of the academic experience.\nDisabilities take on several forms including but not limited to mental health, cognitive, learning, behavioral, chronic health/systemic, and physical.\nIf you have a disability (or think you may have a disability) contact Accessibility Services at 320-308-5064 or acc@sctcc.edu to establish an accommodation plan.\nIt is the responsibility of the student requesting accommodations to provide their instructor with their accommodation plan via email. It is encouraged that students with approved accommodations connect with their instructor as soon as they are able in order to proactively discuss how reasonable accommodation will be implemented in class and/or to address any concerns regarding emergency procedures. Students may submit their plan to faculty at any time during the semester, but accommodations cannot be retroactively applied.\nMore information and guidelines are available at www.sctcc.edu/accessibility.\nThis syllabus is available in alternate formats upon request by contacting Accessibility Services at 320-308-5757, 1-800-222-1009, or acc@sctcc.edu. TTY users may call MN Relay Service at 711 to contact the college. Discrimination against individuals on the grounds of disability is prohibited.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#statement-of-diversity",
    "href": "course/syllabus.html#statement-of-diversity",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Statement of Diversity",
    "text": "Statement of Diversity\nThe entire class will benefit from the wealth of diversity brought by each individual, so students are asked to extend every courtesy and respect that they, in turn, would expect from the class.\nThis college is committed to creating a positive, supportive environment that welcomes diversity of opinions and ideas for students. There will be no tolerance of race discrimination/harassment, sexual discrimination/harassment, or discrimination/harassment based on age, disability, color, creed, national origin, religion, sexual orientation, marital status, status with regard to public assistance, or membership in a local commission.",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "course/syllabus.html#course-calendar",
    "href": "course/syllabus.html#course-calendar",
    "title": "CMSC 2208 – Introduction to Machine Learning",
    "section": "Course Calendar",
    "text": "Course Calendar\nSee News and Content page on D2L. The Instructor may make adjustments or changes. Notification will be given in class prior to change.\n\n\n\n\n\n\n\n\n\nWeek\nTopics (with chapter/section anchors)\nDeliverable\nReading (Müller & Guido)\n\n\n\n\n1 (Jan 12–Jan 18)\nCourse onboarding + “What is ML?” + environment setup + notebook workflow\nWeek 1 Verification Assignment (setup + screenshots + video) + D2L Quiz 1\nCh. 1\n\n\n2 (Jan 19–Jan 25)\nFirst supervised model + scikit-learn workflow: kNN\nPractice Notebook 1 (completion): “Classify player archetypes (kNN)” + D2L Quiz 2\nCh. 2.1.1 (kNN)\n\n\n3 (Jan 26–Feb 01)\nLinear models: classification + regression\nPractice Notebook 2 (completion): “Predict player performance (linear/logistic)” + D2L Quiz 3\nCh. 2.2 (Linear Models)\n\n\n4 (Feb 02–Feb 08)\nTrees + ensembles: random forests / gradient boosting\nSkill Check 1 (autograded): train + report results on game stats + D2L Quiz 4\nCh. 2.3 (Decision Trees) + 2.3.2 (Ensembles)\n\n\n5 (Feb 09–Feb 15)\nClassifier outputs and confidence: decision function vs predicted probabilities\nReflection 1 (video): “Using uncertainty to compare models” + D2L Quiz 5\nCh. 2.4: Uncertainty Estimates from Classifiers\n\n\n6 (Feb 16–Feb 22)\nPreprocessing & scaling workflow: transformations and applying them correctly\nPractice Notebook 3 (completion): “Scale game stats correctly (train vs test)” + D2L Quiz 6\nCh. 3.3.1–3.3.4\n\n\n7 (Feb 23–Mar 01)\nDimensionality reduction + visualization: PCA and t-SNE (“player map”)\nPractice Notebook 4 (completion): “Build a player map (PCA / t-SNE)” + D2L Quiz 7\nCh. 3.4.1 (PCA) + 3.4.3 (t-SNE) (NMF optional)\n\n\n8 (Mar 02–Mar 08)\nClustering: k-means, agglomerative, DBSCAN, and how to compare clusters\nPractice Notebook 5 (completion): “Cluster players by playstyle” + D2L Quiz 8\nCh. 3.5.1–3.5.5\n\n\n9 (Mar 09–Mar 15)\nSpring Break (no due dates)\n—\n—\n\n\n10 (Mar 16–Mar 22)\nRepresenting data + feature engineering (one-hot, binning, interactions)\nPractice Notebook 6 (completion): “Engineer features for matchmaking” + D2L Quiz 9\nCh. 4\n\n\n11 (Mar 23–Mar 29)\nModel evaluation: train/test vs cross-validation + metrics beyond accuracy\nPractice Notebook 7 (completion): “Evaluate models beyond accuracy” + D2L Quiz 10\nCh. 5 (evaluation + metrics)\n\n\n12 (Mar 30–Apr 05)\nModel selection + tuning: grid search + CV workflow\nSkill Check 2 (autograded): metrics + CV + grid search summary + D2L Quiz 11\nCh. 5 (model selection / grid search)\n\n\n13 (Apr 06–Apr 12)\nPipelines: preprocessing + model as one reproducible workflow\nPractice Notebook 8 (completion): “Build a full pipeline for prediction” + D2L Quiz 12\nCh. 6\n\n\n14 (Apr 13–Apr 19)\nPipelines + tuning together (grid search inside pipeline; avoid leakage)\nSkill Check 3 (autograded): pipeline + grid search + report best parameters + D2L Quiz 13\nCh. 6\n\n\n15 (Apr 20–Apr 26)\nText data intro (chat/reviews) or capstone prep emphasis\nReflection 2 / Project Proposal (graded): dataset + goal + metric + plan + D2L Quiz 14\nCh. 7 (+ Ch. 8 possible)\n\n\n16 (Apr 27–May 03)\nFinal project work time (milestone: clean data + baseline + evaluation plan)\n— (work week)\nAs needed (review Ch. 4–6)\n\n\n17 (May 04–May 10)\nFinal project work time (milestone: tuned model + results + narrative + video)\n— (work week)\nAs needed",
    "crumbs": [
      "Home",
      "Course information",
      "Syllabus"
    ]
  },
  {
    "objectID": "assignments/week-05/week-5-demo.html",
    "href": "assignments/week-05/week-5-demo.html",
    "title": "Week 2: Supervised Learning Algorithms",
    "section": "",
    "text": "Last week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).\nThis week, we’re adding two completely different algorithms to our toolkit. While they work in very different ways from kNN, we’ll use the same train/test framework to evaluate and tune them.\n\n\nRidge Regression makes predictions using a formula. It learns weights for each feature and combines them:\npredicted_score = w₁ × attendance + w₂ × homework + w₃ × quiz + w₄ × exam + b\nUnlike kNN, which stores all the training data and looks up neighbors, Ridge learns these weights once and then uses the same formula for every prediction. This makes it fast and the predictions easy to interpret.\nWhat makes Ridge different from kNN: - kNN stores training data and searches through it; Ridge learns a formula - kNN has no “training” phase (just stores data); Ridge solves for optimal weights - kNN predictions depend on nearby points; Ridge uses the same weights for everyone\nThe complexity control: Ridge has a parameter called alpha that controls regularization. Higher alpha forces the weights to be smaller, creating a simpler model. We’ll explore how different alpha values affect performance.\n\n\n\nDecision Trees work by learning a series of questions to ask about each student. For example:\n\nIs their exam average greater than 65?\n\nIf yes: Is their quiz average greater than 70?\nIf no: Is their homework rate greater than 80?\n\n\nThe tree keeps asking questions until it reaches a prediction. Unlike both kNN and Ridge, you can literally draw out the decision-making process and follow along.\nWhat makes Decision Trees different from kNN: - kNN uses distance/similarity; Trees use yes/no rules - kNN considers multiple neighbors; Trees follow one path to a leaf - kNN treats all features equally; Trees automatically select which features matter most\nThe complexity control: Trees have a parameter called max_depth that limits how many questions they can ask in a row. Deeper trees can capture more complex patterns but risk overfitting.\n\n\n\nBy the end of this demo, you’ll be able to: - Train Ridge Regression models and interpret their weights - Train Decision Trees and visualize their decision rules - Use the train/test pattern to tune alpha (for Ridge) and max_depth (for Trees) - Understand which features are most important for predictions (especially useful with Trees)\nLet’s start by building our dataset.\n\n\n\n\n\nBefore we can train any models, we need data. We’re going to create a synthetic dataset of 100 students, each with four measured features: attendance percentage, homework completion rate, quiz average, and exam average. From these features, we’ll calculate a final score for each student. We’ll also add some random variation to make the data more realistic. In the real world, predictions are never perfect because there are always factors we can’t measure (like how much sleep a student got, or whether they were having a bad day).\nThis dataset will serve two purposes: we’ll use the final_score as a numeric target for Ridge Regression, and we’ll convert it to a pass/fail label for Decision Trees.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Create reproducible random data\nrng = np.random.default_rng(42)\nn = 100\n\ndf = pd.DataFrame({\n    \"attendance\": rng.integers(60, 101, n),\n    \"homework_rate\": rng.integers(50, 101, n),\n    \"quiz_avg\": rng.integers(40, 101, n),\n    \"exam_avg\": rng.integers(40, 101, n),\n})\n\n# Create final score with some realistic noise\nbase_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"])\nnoise = rng.normal(0, 3, n)  # Add random noise\ndf[\"final_score\"] = (base_score + noise).round(0)\n\n# Classification target: pass/fail\ndf[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\")\n\nprint(f\"Dataset size: {len(df)} students\")\ndf.head()\nUnderstanding the code:\nImporting libraries: - numpy (abbreviated as np) provides tools for generating random numbers and doing numerical operations - pandas (abbreviated as pd) lets us work with tables of data called DataFrames - matplotlib.pyplot (abbreviated as plt) is for creating visualizations - train_test_split from scikit-learn will help us split data into training and test sets later\nCreating reproducible random data: - rng = np.random.default_rng(42) creates a random number generator with a fixed seed (42). Using a seed means we get the same “random” numbers every time we run this code, making results reproducible. - n = 100 sets our dataset size to 100 students\nBuilding the DataFrame: - pd.DataFrame({...}) creates a table with four columns - rng.integers(60, 101, n) generates 100 random whole numbers between 60 and 100 (inclusive) - Each column represents one feature: attendance percentage, homework completion rate, quiz average, and exam average - After this step, we have a table where each row is one student and each column is one measurement\nCreating a realistic final score: - base_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"]) calculates a weighted average where exams count for 50%, quizzes for 30%, and homework for 20%. Notice we didn’t use attendance in the formula. - noise = rng.normal(0, 3, n) generates random variation from a bell curve centered at 0 with a spread of 3 points. Think of this as the unpredictable factors we can’t measure: maybe a student had a headache during the exam, or got lucky on multiple choice questions, or studied extra hard one week. These small random variations make our data more like the real world. - df[\"final_score\"] = (base_score + noise).round(0) adds the random variation to the base score and rounds to whole numbers - This creates a semi-realistic relationship: final scores are mostly determined by the formula, but not perfectly\nCreating a classification target: - df[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\") creates a new column that labels each student as “pass” if their final score is 70 or higher, “fail” otherwise - This gives us a categorical target for classification tasks\nDisplaying the results: - print(f\"Dataset size: {len(df)} students\") shows how many students we created - df.head() displays the first 5 rows of our dataset so we can see what it looks like\nKey insight: By adding random variation, we’ve created a dataset where patterns exist but aren’t perfect. This is important because it means there’s actually a difference between simple and complex models. A perfectly predictable dataset wouldn’t show us overfitting or underfitting.\n\n\n\nDecision Trees are fundamentally different from both kNN and Ridge Regression. Instead of calculating distances or learning weights, a Decision Tree builds a flowchart of yes/no questions. Each question splits the data based on one feature, and the tree keeps asking questions until it can make a prediction.\nHere’s a simple example of what a decision tree might learn:\nIs exam_avg &gt; 65?\n├─ YES → Is quiz_avg &gt; 70?\n│         ├─ YES → Predict: PASS\n│         └─ NO → Is homework_rate &gt; 80?\n│                  ├─ YES → Predict: PASS\n│                  └─ NO → Predict: FAIL\n└─ NO → Predict: FAIL\nWhat makes Decision Trees different from Ridge and kNN: - Ridge uses a formula with weights; Trees use a sequence of if/then rules - kNN looks at similar examples; Trees follow a predetermined path based on feature thresholds - Ridge and kNN use all features for every prediction; Trees might only use a few features for any given prediction\nThe complexity control (max_depth): Decision Trees have a parameter called max_depth that controls how many questions they can ask in a row. When we create a Decision Tree, we specify this parameter:\ntree = DecisionTreeClassifier(max_depth=3)   # Can ask up to 3 questions\ntree = DecisionTreeClassifier(max_depth=10)  # Can ask up to 10 questions\n\nSmall max_depth (like 1 or 2): The tree can only ask a few questions, creating a simple model that might miss important patterns (underfitting)\nLarge max_depth (like 10 or unlimited): The tree can ask many questions, creating a complex model that might memorize training data quirks (overfitting)\n\nMax_depth works like a depth limit: it’s the longest path from the top of the tree to any prediction at the bottom.\nAccuracy vs R²: Unlike Ridge, Decision Trees for classification use accuracy as their score instead of R². Accuracy is simpler: it’s just the percentage of predictions that were correct. If we predict correctly for 23 out of 25 test students, our test accuracy is 23/25 = 0.92 or 92%.\n\n\nWe’ll use the same train/test split approach as before, but this time we’re predicting pass/fail (classification) instead of final_score (regression). This means we use the categorical target column instead of the numeric one.\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Use pass/fail as the target\ny_class = df[\"pass_fail\"]\n\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X, y_class, test_size=0.25, random_state=0\n)\nUnderstanding the code:\nImporting the model: - from sklearn.tree import DecisionTreeClassifier imports the Decision Tree classifier from scikit-learn\nSelecting the classification target: - y_class = df[\"pass_fail\"] extracts the pass/fail column as our target - We’re using the same features (X) as before, but now predicting a category instead of a number\nSplitting the data: - train_test_split(X, y_class, test_size=0.25, random_state=0) splits our data just like we did with Ridge - We get training and test sets for both features and labels - The _class suffix on the variable names helps us remember these are for classification, not regression\n\n\n\nJust like we tried different alpha values with Ridge, we’ll try different max_depth values with Decision Trees. We’ll use a loop to train multiple trees and compare their performance.\ndepths = [1, 2, 3, 5, 10, None]  # None = unlimited depth\ntree_results = []\n\nfor depth in depths:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=0)\n    tree.fit(X_train_class, y_train_class)\n    \n    train_acc = tree.score(X_train_class, y_train_class)\n    test_acc = tree.score(X_test_class, y_test_class)\n    \n    depth_str = \"unlimited\" if depth is None else str(depth)\n    \n    print(f\"max_depth={depth_str:9s} → train: {train_acc:.3f}, test: {test_acc:.3f}\")\nUnderstanding the code:\nSetting up the experiment: - depths = [1, 2, 3, 5, 10, None] creates a list of max_depth values to try - None is a special value that means no limit (the tree can grow as deep as needed to perfectly classify the training data) - tree_results = [] creates an empty list for storing results\nThe loop: - for depth in depths: loops through each max_depth value\nTraining each tree: - tree = DecisionTreeClassifier(max_depth=depth, random_state=0) creates a new tree with the current max_depth - random_state=0 ensures reproducible results (trees can make different choices when features are equally good) - tree.fit(X_train_class, y_train_class) trains the tree. During this step, the tree finds the best questions to ask at each level.\nEvaluating each tree: - tree.score(X_train_class, y_train_class) calculates accuracy on training data - tree.score(X_test_class, y_test_class) calculates accuracy on test data - Remember: accuracy is just the fraction of correct predictions\nFormatting the output: - depth_str = \"unlimited\" if depth is None else str(depth) converts the depth to a readable string - The print statement displays results for each max_depth value\nWhat to look for in the output: - If train accuracy is 1.0 (100%) but test accuracy is lower, we’re overfitting - If both accuracies are low, we’re underfitting - We want both to be reasonably high and close together\n\n\n\nOne of the best features of Decision Trees is that we can actually visualize them and see exactly what questions the model learned. Let’s train a tree with max_depth=3 so it’s small enough to read comfortably.\nfrom sklearn.tree import plot_tree\n\n# Train a tree we can actually read\nsimple_tree = DecisionTreeClassifier(max_depth=3, random_state=0)\nsimple_tree.fit(X_train_class, y_train_class)\n\nplt.figure(figsize=(16, 8))\nplot_tree(simple_tree, \n          feature_names=[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"],\n          class_names=[\"fail\", \"pass\"],\n          filled=True, \n          rounded=True,\n          fontsize=11)\nplt.title(\"Decision Tree (max_depth=3)\")\nplt.show()\nUnderstanding the code:\nImporting the visualization tool: - from sklearn.tree import plot_tree imports a function specifically for drawing decision trees\nTraining a visualization-friendly tree: - simple_tree = DecisionTreeClassifier(max_depth=3, random_state=0) creates a shallow tree - simple_tree.fit(X_train_class, y_train_class) trains it on our data - We use max_depth=3 because deeper trees become too large to read\nCreating the visualization: - plt.figure(figsize=(16, 8)) creates a wide canvas (16 inches by 8 inches) because trees spread horizontally - plot_tree(...) draws the tree structure\nVisualization parameters: - feature_names=[...] tells the plot what to call each feature - class_names=[\"fail\", \"pass\"] labels the two classes - filled=True colors the boxes based on the prediction (makes it easier to read) - rounded=True makes the boxes have rounded corners (purely aesthetic) - fontsize=11 makes the text readable\nHow to read the resulting tree:\nEach box in the tree shows: 1. The question being asked (like “exam_avg &lt;= 62.5”) 2. The number of samples that reach this point 3. The class distribution (how many fail vs pass) 4. The predicted class for samples that reach this box\nFollowing a path through the tree: - Start at the top box (the root) - At each box, check if the condition is true - If YES (≤), go to the left child - If NO (&gt;), go to the right child - Continue until you reach a leaf (a box with no children below it) - The leaf tells you the prediction\nUnderstanding the colors: - Orange boxes predict “fail” - Blue boxes predict “pass” - Darker colors mean the prediction is more confident (more samples agree) - Light colors mean the prediction is less certain (samples are mixed)\nExample: Tracing a prediction\nLet’s say we have a student with: - attendance = 85 - homework_rate = 75 - quiz_avg = 68 - exam_avg = 58\nStarting at the root: 1. Is exam_avg &lt;= 62.5? YES (58 &lt;= 62.5), so go LEFT 2. Is quiz_avg &lt;= 55.5? NO (68 &gt; 55.5), so go RIGHT 3. Is homework_rate &lt;= 73.5? NO (75 &gt; 73.5), so go RIGHT 4. We’ve reached a leaf that predicts “pass”\nThis example shows how the tree makes a prediction by asking a specific sequence of questions.\n\n\n\nDecision Trees automatically tell us which features were most useful for making predictions. This is called feature importance, and it’s one of the most valuable outputs from a tree model.\ntree = DecisionTreeClassifier(max_depth=5, random_state=0)\ntree.fit(X_train_class, y_train_class)\n\n# Create a DataFrame of feature importances\nimportance_df = pd.DataFrame({\n    \"feature\": [\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"],\n    \"importance\": tree.feature_importances_\n}).sort_values(\"importance\", ascending=False)\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.barh(importance_df[\"feature\"], importance_df[\"importance\"])\nplt.xlabel(\"Importance (higher = more useful)\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance from Decision Tree\")\nplt.tight_layout()\nplt.show()\n\nprint(importance_df.to_string(index=False))\nUnderstanding the code:\nTraining a tree for importance analysis: - tree = DecisionTreeClassifier(max_depth=5, random_state=0) creates a tree with moderate depth - We use depth 5 so the tree has enough structure to make meaningful distinctions - tree.fit(X_train_class, y_train_class) trains the tree\nExtracting feature importances: - tree.feature_importances_ is a built-in attribute that contains importance scores for each feature - This returns an array with one importance value per feature, in the same order as our feature columns\nOrganizing the results: - pd.DataFrame({...}) creates a table pairing each feature name with its importance - .sort_values(\"importance\", ascending=False) sorts from most important to least important - This makes it easy to see which features matter most at a glance\nCreating the bar chart: - plt.barh(...) creates a horizontal bar chart (easier to read feature names) - Longer bars mean higher importance - The bars represent how much each feature contributed to reducing prediction errors\nDisplaying both views: - plt.show() displays the chart - print(importance_df.to_string(index=False)) prints the exact numbers\nWhat feature importance means:\nFeature importance values are numbers between 0 and 1 that always sum to 1.0 (or 100% total). You can think of them as showing how the tree allocated its “decision-making budget” across features:\n\nHigher values (like 0.4 or 0.5) mean the tree used this feature frequently for important splits that significantly improved predictions\nLower values (like 0.05 or 0.1) mean the feature was used occasionally or only for minor refinements\nZero means the tree never used this feature at all\n\nFor example, if exam_avg has an importance of 0.6, quiz_avg has 0.3, homework_rate has 0.1, and attendance has 0.0, we can interpret this as: - exam_avg was the most critical feature, responsible for 60% of the tree’s decision-making power - quiz_avg was helpful, contributing 30% - homework_rate played a minor role at 10% - attendance was completely ignored by the tree\nWhy might a feature have zero importance? This happens when other features provide the same information. In our example, if attendance was always perfectly correlated with homework_rate, the tree would only need to use one of them.\nKey insight: Feature importance helps us understand what the model learned. If we see that attendance has zero importance, we might reconsider whether we need to collect that data at all. Conversely, if exam_avg dominates, it tells us that exam performance is the strongest predictor of pass/fail outcomes in our dataset."
  },
  {
    "objectID": "assignments/week-05/week-5-demo.html#introduction-new-algorithms-same-framework",
    "href": "assignments/week-05/week-5-demo.html#introduction-new-algorithms-same-framework",
    "title": "Week 2: Supervised Learning Algorithms",
    "section": "",
    "text": "Last week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).\nThis week, we’re adding two completely different algorithms to our toolkit. While they work in very different ways from kNN, we’ll use the same train/test framework to evaluate and tune them.\n\n\nRidge Regression makes predictions using a formula. It learns weights for each feature and combines them:\npredicted_score = w₁ × attendance + w₂ × homework + w₃ × quiz + w₄ × exam + b\nUnlike kNN, which stores all the training data and looks up neighbors, Ridge learns these weights once and then uses the same formula for every prediction. This makes it fast and the predictions easy to interpret.\nWhat makes Ridge different from kNN: - kNN stores training data and searches through it; Ridge learns a formula - kNN has no “training” phase (just stores data); Ridge solves for optimal weights - kNN predictions depend on nearby points; Ridge uses the same weights for everyone\nThe complexity control: Ridge has a parameter called alpha that controls regularization. Higher alpha forces the weights to be smaller, creating a simpler model. We’ll explore how different alpha values affect performance.\n\n\n\nDecision Trees work by learning a series of questions to ask about each student. For example:\n\nIs their exam average greater than 65?\n\nIf yes: Is their quiz average greater than 70?\nIf no: Is their homework rate greater than 80?\n\n\nThe tree keeps asking questions until it reaches a prediction. Unlike both kNN and Ridge, you can literally draw out the decision-making process and follow along.\nWhat makes Decision Trees different from kNN: - kNN uses distance/similarity; Trees use yes/no rules - kNN considers multiple neighbors; Trees follow one path to a leaf - kNN treats all features equally; Trees automatically select which features matter most\nThe complexity control: Trees have a parameter called max_depth that limits how many questions they can ask in a row. Deeper trees can capture more complex patterns but risk overfitting.\n\n\n\nBy the end of this demo, you’ll be able to: - Train Ridge Regression models and interpret their weights - Train Decision Trees and visualize their decision rules - Use the train/test pattern to tune alpha (for Ridge) and max_depth (for Trees) - Understand which features are most important for predictions (especially useful with Trees)\nLet’s start by building our dataset."
  },
  {
    "objectID": "assignments/week-05/week-5-demo.html#setup-building-a-student-performance-dataset",
    "href": "assignments/week-05/week-5-demo.html#setup-building-a-student-performance-dataset",
    "title": "Week 2: Supervised Learning Algorithms",
    "section": "",
    "text": "Before we can train any models, we need data. We’re going to create a synthetic dataset of 100 students, each with four measured features: attendance percentage, homework completion rate, quiz average, and exam average. From these features, we’ll calculate a final score for each student. We’ll also add some random variation to make the data more realistic. In the real world, predictions are never perfect because there are always factors we can’t measure (like how much sleep a student got, or whether they were having a bad day).\nThis dataset will serve two purposes: we’ll use the final_score as a numeric target for Ridge Regression, and we’ll convert it to a pass/fail label for Decision Trees.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Create reproducible random data\nrng = np.random.default_rng(42)\nn = 100\n\ndf = pd.DataFrame({\n    \"attendance\": rng.integers(60, 101, n),\n    \"homework_rate\": rng.integers(50, 101, n),\n    \"quiz_avg\": rng.integers(40, 101, n),\n    \"exam_avg\": rng.integers(40, 101, n),\n})\n\n# Create final score with some realistic noise\nbase_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"])\nnoise = rng.normal(0, 3, n)  # Add random noise\ndf[\"final_score\"] = (base_score + noise).round(0)\n\n# Classification target: pass/fail\ndf[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\")\n\nprint(f\"Dataset size: {len(df)} students\")\ndf.head()\nUnderstanding the code:\nImporting libraries: - numpy (abbreviated as np) provides tools for generating random numbers and doing numerical operations - pandas (abbreviated as pd) lets us work with tables of data called DataFrames - matplotlib.pyplot (abbreviated as plt) is for creating visualizations - train_test_split from scikit-learn will help us split data into training and test sets later\nCreating reproducible random data: - rng = np.random.default_rng(42) creates a random number generator with a fixed seed (42). Using a seed means we get the same “random” numbers every time we run this code, making results reproducible. - n = 100 sets our dataset size to 100 students\nBuilding the DataFrame: - pd.DataFrame({...}) creates a table with four columns - rng.integers(60, 101, n) generates 100 random whole numbers between 60 and 100 (inclusive) - Each column represents one feature: attendance percentage, homework completion rate, quiz average, and exam average - After this step, we have a table where each row is one student and each column is one measurement\nCreating a realistic final score: - base_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"]) calculates a weighted average where exams count for 50%, quizzes for 30%, and homework for 20%. Notice we didn’t use attendance in the formula. - noise = rng.normal(0, 3, n) generates random variation from a bell curve centered at 0 with a spread of 3 points. Think of this as the unpredictable factors we can’t measure: maybe a student had a headache during the exam, or got lucky on multiple choice questions, or studied extra hard one week. These small random variations make our data more like the real world. - df[\"final_score\"] = (base_score + noise).round(0) adds the random variation to the base score and rounds to whole numbers - This creates a semi-realistic relationship: final scores are mostly determined by the formula, but not perfectly\nCreating a classification target: - df[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\") creates a new column that labels each student as “pass” if their final score is 70 or higher, “fail” otherwise - This gives us a categorical target for classification tasks\nDisplaying the results: - print(f\"Dataset size: {len(df)} students\") shows how many students we created - df.head() displays the first 5 rows of our dataset so we can see what it looks like\nKey insight: By adding random variation, we’ve created a dataset where patterns exist but aren’t perfect. This is important because it means there’s actually a difference between simple and complex models. A perfectly predictable dataset wouldn’t show us overfitting or underfitting."
  },
  {
    "objectID": "assignments/week-05/week-5-demo.html#part-2-decision-trees",
    "href": "assignments/week-05/week-5-demo.html#part-2-decision-trees",
    "title": "Week 2: Supervised Learning Algorithms",
    "section": "",
    "text": "Decision Trees are fundamentally different from both kNN and Ridge Regression. Instead of calculating distances or learning weights, a Decision Tree builds a flowchart of yes/no questions. Each question splits the data based on one feature, and the tree keeps asking questions until it can make a prediction.\nHere’s a simple example of what a decision tree might learn:\nIs exam_avg &gt; 65?\n├─ YES → Is quiz_avg &gt; 70?\n│         ├─ YES → Predict: PASS\n│         └─ NO → Is homework_rate &gt; 80?\n│                  ├─ YES → Predict: PASS\n│                  └─ NO → Predict: FAIL\n└─ NO → Predict: FAIL\nWhat makes Decision Trees different from Ridge and kNN: - Ridge uses a formula with weights; Trees use a sequence of if/then rules - kNN looks at similar examples; Trees follow a predetermined path based on feature thresholds - Ridge and kNN use all features for every prediction; Trees might only use a few features for any given prediction\nThe complexity control (max_depth): Decision Trees have a parameter called max_depth that controls how many questions they can ask in a row. When we create a Decision Tree, we specify this parameter:\ntree = DecisionTreeClassifier(max_depth=3)   # Can ask up to 3 questions\ntree = DecisionTreeClassifier(max_depth=10)  # Can ask up to 10 questions\n\nSmall max_depth (like 1 or 2): The tree can only ask a few questions, creating a simple model that might miss important patterns (underfitting)\nLarge max_depth (like 10 or unlimited): The tree can ask many questions, creating a complex model that might memorize training data quirks (overfitting)\n\nMax_depth works like a depth limit: it’s the longest path from the top of the tree to any prediction at the bottom.\nAccuracy vs R²: Unlike Ridge, Decision Trees for classification use accuracy as their score instead of R². Accuracy is simpler: it’s just the percentage of predictions that were correct. If we predict correctly for 23 out of 25 test students, our test accuracy is 23/25 = 0.92 or 92%.\n\n\nWe’ll use the same train/test split approach as before, but this time we’re predicting pass/fail (classification) instead of final_score (regression). This means we use the categorical target column instead of the numeric one.\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Use pass/fail as the target\ny_class = df[\"pass_fail\"]\n\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X, y_class, test_size=0.25, random_state=0\n)\nUnderstanding the code:\nImporting the model: - from sklearn.tree import DecisionTreeClassifier imports the Decision Tree classifier from scikit-learn\nSelecting the classification target: - y_class = df[\"pass_fail\"] extracts the pass/fail column as our target - We’re using the same features (X) as before, but now predicting a category instead of a number\nSplitting the data: - train_test_split(X, y_class, test_size=0.25, random_state=0) splits our data just like we did with Ridge - We get training and test sets for both features and labels - The _class suffix on the variable names helps us remember these are for classification, not regression\n\n\n\nJust like we tried different alpha values with Ridge, we’ll try different max_depth values with Decision Trees. We’ll use a loop to train multiple trees and compare their performance.\ndepths = [1, 2, 3, 5, 10, None]  # None = unlimited depth\ntree_results = []\n\nfor depth in depths:\n    tree = DecisionTreeClassifier(max_depth=depth, random_state=0)\n    tree.fit(X_train_class, y_train_class)\n    \n    train_acc = tree.score(X_train_class, y_train_class)\n    test_acc = tree.score(X_test_class, y_test_class)\n    \n    depth_str = \"unlimited\" if depth is None else str(depth)\n    \n    print(f\"max_depth={depth_str:9s} → train: {train_acc:.3f}, test: {test_acc:.3f}\")\nUnderstanding the code:\nSetting up the experiment: - depths = [1, 2, 3, 5, 10, None] creates a list of max_depth values to try - None is a special value that means no limit (the tree can grow as deep as needed to perfectly classify the training data) - tree_results = [] creates an empty list for storing results\nThe loop: - for depth in depths: loops through each max_depth value\nTraining each tree: - tree = DecisionTreeClassifier(max_depth=depth, random_state=0) creates a new tree with the current max_depth - random_state=0 ensures reproducible results (trees can make different choices when features are equally good) - tree.fit(X_train_class, y_train_class) trains the tree. During this step, the tree finds the best questions to ask at each level.\nEvaluating each tree: - tree.score(X_train_class, y_train_class) calculates accuracy on training data - tree.score(X_test_class, y_test_class) calculates accuracy on test data - Remember: accuracy is just the fraction of correct predictions\nFormatting the output: - depth_str = \"unlimited\" if depth is None else str(depth) converts the depth to a readable string - The print statement displays results for each max_depth value\nWhat to look for in the output: - If train accuracy is 1.0 (100%) but test accuracy is lower, we’re overfitting - If both accuracies are low, we’re underfitting - We want both to be reasonably high and close together\n\n\n\nOne of the best features of Decision Trees is that we can actually visualize them and see exactly what questions the model learned. Let’s train a tree with max_depth=3 so it’s small enough to read comfortably.\nfrom sklearn.tree import plot_tree\n\n# Train a tree we can actually read\nsimple_tree = DecisionTreeClassifier(max_depth=3, random_state=0)\nsimple_tree.fit(X_train_class, y_train_class)\n\nplt.figure(figsize=(16, 8))\nplot_tree(simple_tree, \n          feature_names=[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"],\n          class_names=[\"fail\", \"pass\"],\n          filled=True, \n          rounded=True,\n          fontsize=11)\nplt.title(\"Decision Tree (max_depth=3)\")\nplt.show()\nUnderstanding the code:\nImporting the visualization tool: - from sklearn.tree import plot_tree imports a function specifically for drawing decision trees\nTraining a visualization-friendly tree: - simple_tree = DecisionTreeClassifier(max_depth=3, random_state=0) creates a shallow tree - simple_tree.fit(X_train_class, y_train_class) trains it on our data - We use max_depth=3 because deeper trees become too large to read\nCreating the visualization: - plt.figure(figsize=(16, 8)) creates a wide canvas (16 inches by 8 inches) because trees spread horizontally - plot_tree(...) draws the tree structure\nVisualization parameters: - feature_names=[...] tells the plot what to call each feature - class_names=[\"fail\", \"pass\"] labels the two classes - filled=True colors the boxes based on the prediction (makes it easier to read) - rounded=True makes the boxes have rounded corners (purely aesthetic) - fontsize=11 makes the text readable\nHow to read the resulting tree:\nEach box in the tree shows: 1. The question being asked (like “exam_avg &lt;= 62.5”) 2. The number of samples that reach this point 3. The class distribution (how many fail vs pass) 4. The predicted class for samples that reach this box\nFollowing a path through the tree: - Start at the top box (the root) - At each box, check if the condition is true - If YES (≤), go to the left child - If NO (&gt;), go to the right child - Continue until you reach a leaf (a box with no children below it) - The leaf tells you the prediction\nUnderstanding the colors: - Orange boxes predict “fail” - Blue boxes predict “pass” - Darker colors mean the prediction is more confident (more samples agree) - Light colors mean the prediction is less certain (samples are mixed)\nExample: Tracing a prediction\nLet’s say we have a student with: - attendance = 85 - homework_rate = 75 - quiz_avg = 68 - exam_avg = 58\nStarting at the root: 1. Is exam_avg &lt;= 62.5? YES (58 &lt;= 62.5), so go LEFT 2. Is quiz_avg &lt;= 55.5? NO (68 &gt; 55.5), so go RIGHT 3. Is homework_rate &lt;= 73.5? NO (75 &gt; 73.5), so go RIGHT 4. We’ve reached a leaf that predicts “pass”\nThis example shows how the tree makes a prediction by asking a specific sequence of questions.\n\n\n\nDecision Trees automatically tell us which features were most useful for making predictions. This is called feature importance, and it’s one of the most valuable outputs from a tree model.\ntree = DecisionTreeClassifier(max_depth=5, random_state=0)\ntree.fit(X_train_class, y_train_class)\n\n# Create a DataFrame of feature importances\nimportance_df = pd.DataFrame({\n    \"feature\": [\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"],\n    \"importance\": tree.feature_importances_\n}).sort_values(\"importance\", ascending=False)\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.barh(importance_df[\"feature\"], importance_df[\"importance\"])\nplt.xlabel(\"Importance (higher = more useful)\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance from Decision Tree\")\nplt.tight_layout()\nplt.show()\n\nprint(importance_df.to_string(index=False))\nUnderstanding the code:\nTraining a tree for importance analysis: - tree = DecisionTreeClassifier(max_depth=5, random_state=0) creates a tree with moderate depth - We use depth 5 so the tree has enough structure to make meaningful distinctions - tree.fit(X_train_class, y_train_class) trains the tree\nExtracting feature importances: - tree.feature_importances_ is a built-in attribute that contains importance scores for each feature - This returns an array with one importance value per feature, in the same order as our feature columns\nOrganizing the results: - pd.DataFrame({...}) creates a table pairing each feature name with its importance - .sort_values(\"importance\", ascending=False) sorts from most important to least important - This makes it easy to see which features matter most at a glance\nCreating the bar chart: - plt.barh(...) creates a horizontal bar chart (easier to read feature names) - Longer bars mean higher importance - The bars represent how much each feature contributed to reducing prediction errors\nDisplaying both views: - plt.show() displays the chart - print(importance_df.to_string(index=False)) prints the exact numbers\nWhat feature importance means:\nFeature importance values are numbers between 0 and 1 that always sum to 1.0 (or 100% total). You can think of them as showing how the tree allocated its “decision-making budget” across features:\n\nHigher values (like 0.4 or 0.5) mean the tree used this feature frequently for important splits that significantly improved predictions\nLower values (like 0.05 or 0.1) mean the feature was used occasionally or only for minor refinements\nZero means the tree never used this feature at all\n\nFor example, if exam_avg has an importance of 0.6, quiz_avg has 0.3, homework_rate has 0.1, and attendance has 0.0, we can interpret this as: - exam_avg was the most critical feature, responsible for 60% of the tree’s decision-making power - quiz_avg was helpful, contributing 30% - homework_rate played a minor role at 10% - attendance was completely ignored by the tree\nWhy might a feature have zero importance? This happens when other features provide the same information. In our example, if attendance was always perfectly correlated with homework_rate, the tree would only need to use one of them.\nKey insight: Feature importance helps us understand what the model learned. If we see that attendance has zero importance, we might reconsider whether we need to collect that data at all. Conversely, if exam_avg dominates, it tells us that exam performance is the strongest predictor of pass/fail outcomes in our dataset."
  },
  {
    "objectID": "assignments/week-03/week-3-demo.html",
    "href": "assignments/week-03/week-3-demo.html",
    "title": "Week 3 Demo: Ridge and Logistic Regression",
    "section": "",
    "text": "Last week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).\nThis week, we’re learning about linear models, a completely different family of algorithms.\n\n\nLinear models make predictions using a formula that combines your input features. The key characteristic that makes them “linear” is that they multiply each feature by a weight and add them together. For example, if you’re predicting a student’s final score based on their homework, quiz, and exam grades, a linear model learns a formula like:\npredicted_score = (w1 × homework) + (w2 × quiz) + (w3 × exam) + constant\nDuring training, the model tries many different combinations of weights to see which ones make the most accurate predictions on the training data. For example, it might try:\n\nAttempt 1: w1=0.1, w2=0.1, w3=0.1 → predictions are terrible\nAttempt 2: w1=0.5, w2=0.3, w3=0.8 → predictions are better\nAttempt 3: w1=0.2, w2=0.3, w3=0.5 → predictions are even better\n\nThe model keeps adjusting the weights until it finds the combination that minimizes the difference between its predictions and the actual values in the training data. This process happens automatically when you call .fit() - you don’t have to manually try different weights yourself.\nIn our student data example in Part 1, if exam scores actually matter most for the final grade, the model should learn that w3 (for exam) should be larger than w1 (for homework). The training process figures this out by looking at the patterns in the data.\nWhat makes linear models fundamentally different from kNN: - kNN stores all training data and searches through it; linear models learn a formula - kNN has no “training” phase (just stores data); linear models solve for optimal weights during training - kNN predictions depend on nearby examples; linear models use the same formula for everyone - kNN can’t tell you which features matter; linear models show importance through weights\nThis week, we’ll explore two specific linear models: Ridge Regression (for predicting numbers) and Logistic Regression (for predicting categories).\n\n\n\nRidge Regression is a linear model for regression tasks (predicting numeric values like final scores). It learns weights for each feature, then uses those weights in a formula to make predictions.\nThe complexity control: Ridge has a parameter called alpha that controls regularization. Regularization keeps the weights from getting too large, which helps prevent overfitting. Higher alpha forces the weights to be smaller, creating a simpler model.\n\n\n\nLogistic Regression is a linear model for classification tasks (predicting categories like pass/fail). Despite its name containing “regression,” it predicts categories, not numbers.\nIt works similarly to Ridge by learning weights and combining features, but instead of outputting the weighted sum directly, it converts the sum into a probability between 0 and 1. If the probability is above 0.5, it predicts one class; otherwise, it predicts the other class.\nThe complexity control: Logistic Regression has a parameter called C that controls regularization, but it works opposite to Ridge’s alpha. Higher C means less regularization (more complex model), while higher alpha meant more regularization (simpler model).\n\n\n\nBy the end of this demo, you’ll be able to: - Understand what linear models are and how they differ from kNN - Train Ridge Regression models for predicting numeric values - Train Logistic Regression models for predicting categories - Interpret the learned weights to understand feature importance - Use the same train/test evaluation framework across different algorithms"
  },
  {
    "objectID": "assignments/week-03/week-3-demo.html#introduction",
    "href": "assignments/week-03/week-3-demo.html#introduction",
    "title": "Week 3 Demo: Ridge and Logistic Regression",
    "section": "",
    "text": "Last week we explored k-Nearest Neighbors (kNN), which makes predictions by finding similar examples in the training data. We learned how to use training and test scores to find the right value of k - not too small (overfitting) and not too large (underfitting).\nThis week, we’re learning about linear models, a completely different family of algorithms.\n\n\nLinear models make predictions using a formula that combines your input features. The key characteristic that makes them “linear” is that they multiply each feature by a weight and add them together. For example, if you’re predicting a student’s final score based on their homework, quiz, and exam grades, a linear model learns a formula like:\npredicted_score = (w1 × homework) + (w2 × quiz) + (w3 × exam) + constant\nDuring training, the model tries many different combinations of weights to see which ones make the most accurate predictions on the training data. For example, it might try:\n\nAttempt 1: w1=0.1, w2=0.1, w3=0.1 → predictions are terrible\nAttempt 2: w1=0.5, w2=0.3, w3=0.8 → predictions are better\nAttempt 3: w1=0.2, w2=0.3, w3=0.5 → predictions are even better\n\nThe model keeps adjusting the weights until it finds the combination that minimizes the difference between its predictions and the actual values in the training data. This process happens automatically when you call .fit() - you don’t have to manually try different weights yourself.\nIn our student data example in Part 1, if exam scores actually matter most for the final grade, the model should learn that w3 (for exam) should be larger than w1 (for homework). The training process figures this out by looking at the patterns in the data.\nWhat makes linear models fundamentally different from kNN: - kNN stores all training data and searches through it; linear models learn a formula - kNN has no “training” phase (just stores data); linear models solve for optimal weights during training - kNN predictions depend on nearby examples; linear models use the same formula for everyone - kNN can’t tell you which features matter; linear models show importance through weights\nThis week, we’ll explore two specific linear models: Ridge Regression (for predicting numbers) and Logistic Regression (for predicting categories).\n\n\n\nRidge Regression is a linear model for regression tasks (predicting numeric values like final scores). It learns weights for each feature, then uses those weights in a formula to make predictions.\nThe complexity control: Ridge has a parameter called alpha that controls regularization. Regularization keeps the weights from getting too large, which helps prevent overfitting. Higher alpha forces the weights to be smaller, creating a simpler model.\n\n\n\nLogistic Regression is a linear model for classification tasks (predicting categories like pass/fail). Despite its name containing “regression,” it predicts categories, not numbers.\nIt works similarly to Ridge by learning weights and combining features, but instead of outputting the weighted sum directly, it converts the sum into a probability between 0 and 1. If the probability is above 0.5, it predicts one class; otherwise, it predicts the other class.\nThe complexity control: Logistic Regression has a parameter called C that controls regularization, but it works opposite to Ridge’s alpha. Higher C means less regularization (more complex model), while higher alpha meant more regularization (simpler model).\n\n\n\nBy the end of this demo, you’ll be able to: - Understand what linear models are and how they differ from kNN - Train Ridge Regression models for predicting numeric values - Train Logistic Regression models for predicting categories - Interpret the learned weights to understand feature importance - Use the same train/test evaluation framework across different algorithms"
  },
  {
    "objectID": "assignments/week-03/week-3-demo.html#part-1-ridge-regression---basic-workflow",
    "href": "assignments/week-03/week-3-demo.html#part-1-ridge-regression---basic-workflow",
    "title": "Week 3 Demo: Ridge and Logistic Regression",
    "section": "Part 1: Ridge Regression - Basic Workflow",
    "text": "Part 1: Ridge Regression - Basic Workflow\n\nUnderstanding Ridge Regression\nAs mentioned in the introduction, Ridge Regression makes predictions by combining features with learned weights. Let’s explore this in detail\nThe prediction formula looks like this:\npredicted_score = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b\nEach feature gets multiplied by its weight (w₁, w₂, etc.), and then we add a constant (b, called the intercept).\nWeights tell us how much each feature contributes to the prediction. For example, if the model learns that w₃ (the weight for quiz_avg) is 0.5 and w₁ (the weight for attendance) is 0.1, it means quiz average has five times more influence on the final score than attendance does. A weight of 0 would mean that feature doesn’t matter at all. Negative weights mean that as the feature increases, the prediction decreases.\nThink of weights like importance scores: larger absolute values (whether positive or negative) mean that feature has a bigger impact on predictions. If exam_avg has a weight of 0.8 and attendance has a weight of 0.1, the model is saying “exam average matters a lot more than attendance for predicting final scores.”\n\nWhat is regularization?\nRidge uses something called regularization, which means it prefers smaller weights. Why does this matter? When weights get very large (like [3.2, -2.8, 5.1, -4.3]), the model becomes very sensitive to small changes in the input data. A model with smaller weights (like [0.3, 0.2, 0.5, 0.4]) is more stable and less likely to be thrown off by random variation in the data. Think of it like the difference between a recipe that says “add exactly 247 grains of salt” versus “add a pinch of salt.” The first is overly precise and fragile, the second is more robust.\nRidge tries to keep weights small and reasonable, which helps prevent overfitting. It does this by penalizing large weights during training, forcing the model to find a solution that both fits the data well AND keeps weights modest.\n\n\nThe complexity control (alpha)\nThe parameter alpha controls how large the weights in the prediction formula can be. The formula structure is always the same (a weighted sum of features), but alpha determines whether those weights can be large or must stay small.\nImagine you’re creating a formula to predict how long it takes to commute to campus. Your formula always uses the same factors (distance, time of day, weather, route), but alpha controls how much weight each factor gets:\n\nWith small alpha (like 0.1): Ridge can assign large weights. Your formula might use weights like “add 15.7 minutes per mile of distance, subtract 23.2 minutes if you leave before 7am, add 18.9 minutes if it’s raining.” These large, specific weights make your predictions very sensitive to each factor. This might fit your personal commute data perfectly, but one unusual day can throw the predictions way off.\nWith large alpha (like 100): Ridge must keep weights small. Your formula might use weights like “add 2 minutes per mile of distance, add 5 minutes if it’s rush hour, add 3 minutes if it’s raining.” These smaller weights make your predictions less sensitive to any single factor. The predictions might be less precise for your specific situation, but they are more stable and work better in new situations.\n\nWhen we create a Ridge model, we pass alpha as an argument:\nridge = Ridge(alpha=0.1)   # Small alpha: allows large weights\nridge = Ridge(alpha=100)   # Large alpha: forces small weights\nThe .fit() method then finds the best weights it can within the constraint set by alpha. For this demo, we’ll use alpha=1.0, which is scikit-learn’s default and represents moderate regularization.\n\n\nWhat is R²?\nWhen we evaluate a regression model in scikit-learn, we use a method called .score() that returns a number called R² (R-squared). This is the metric we use to measure how well the model performs. You’ll see it calculated like this in our code:\ntrain_r2 = ridge.score(X_train, y_train)\ntest_r2 = ridge.score(X_test, y_test)\nR² ranges from 0 to 1 and tells us how much of the variation in the data our model explains: - 1.0 means perfect predictions (every prediction exactly matches the true value) - 0.5 means okay predictions (the model captures some patterns but misses others) - 0.0 means the model is no better than just guessing the average every time\nFor example, if we get train_r2 = 0.85, it means our model explains 85% of the variation in the training data. Higher R² values are better, and we want both training and test R² to be reasonably high and close together.\n\n\n\nStep 1: Create a student dataset\nLet’s start with a student performance dataset. We’ll create data for 300 students, each with four measured features: attendance percentage, homework completion rate, quiz average, and exam average. We’ll calculate a final score based on a weighted combination of these features, plus some random noise to represent the unpredictable factors we can’t measure (like how much sleep a student got, or whether they were having a bad day).\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Create reproducible random data\nrng = np.random.default_rng(42)\nn = 300\n\ndf = pd.DataFrame({\n    \"attendance\": rng.integers(60, 101, n),\n    \"homework_rate\": rng.integers(50, 101, n),\n    \"quiz_avg\": rng.integers(40, 101, n),\n    \"exam_avg\": rng.integers(40, 101, n),\n})\n\n# Create final score with some realistic noise\nbase_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"])\nnoise = rng.normal(0, 5, n)\ndf[\"final_score\"] = (base_score + noise).round(0)\n\n# Add classification target for Logistic Regression (coming later)\ndf[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\")\n\nprint(f\"Dataset size: {len(df)} students\")\nprint(f\"Number of features: {len(df.columns) - 2}\")  # -2 for final_score and pass_fail\ndf.head()\n\nDataset size: 300 students\nNumber of features: 4\n\n\n\n\n\n\n\n\n\nattendance\nhomework_rate\nquiz_avg\nexam_avg\nfinal_score\npass_fail\n\n\n\n\n0\n63\n68\n77\n40\n63.0\nfail\n\n\n1\n91\n92\n87\n42\n64.0\nfail\n\n\n2\n86\n50\n85\n53\n64.0\nfail\n\n\n3\n77\n95\n48\n93\n68.0\nfail\n\n\n4\n77\n77\n90\n67\n70.0\npass\n\n\n\n\n\n\n\nUnderstanding the code:\nImporting libraries: - numpy (abbreviated as np) provides tools for generating random numbers and doing numerical operations - pandas (abbreviated as pd) lets us work with tables of data called DataFrames - matplotlib.pyplot (abbreviated as plt) is for creating visualizations - train_test_split from scikit-learn will help us split data into training and test sets\nCreating reproducible random data: - rng = np.random.default_rng(42) creates a random number generator with a fixed seed (42). Using a seed means we get the same “random” numbers every time we run this code, making results reproducible. - n = 300 sets our dataset size to 300 students\nBuilding the DataFrame: - pd.DataFrame({...}) creates a table with four columns - rng.integers(60, 101, n) generates 300 random whole numbers between 60 and 100 (inclusive) - Each column represents one feature: attendance percentage, homework completion rate, quiz average, and exam average - After this step, we have a table where each row is one student and each column is one measurement\nCreating a realistic final score: - base_score = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"]) calculates a weighted average where exams count for 50%, quizzes for 30%, and homework for 20%. Notice we didn’t use attendance in the formula. - noise = rng.normal(0, 5, n) generates random variation from a bell curve centered at 0 with a spread of 5 points. Think of this as the unpredictable factors we can’t measure: maybe a student had a headache during the exam, or got lucky on multiple choice questions, or studied extra hard one week. These small random variations make our data more like the real world. - df[\"final_score\"] = (base_score + noise).round(0) adds the random variation to the base score and rounds to whole numbers\nDisplaying the results: - print(f\"Dataset size: {len(df)} students\") shows how many students we created - df.head() displays the first 5 rows of our dataset so we can see what it looks like.\nCreating a classification target: - df[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\") creates a binary label where students with final_score ≥ 70 are marked as “pass”, otherwise “fail” - We create this now because we’ll use it later for Logistic Regression (classification) - For now, we’ll focus on predicting the numeric final_score with Ridge Regression\n\n\nStep 2: Prepare the data\nBefore we train any model, we need to separate our features from our target and split into training and test sets. This is the same process we used last week with kNN: we separate X (the inputs we use to make predictions) from y (what we’re trying to predict), then split both into training and test portions.\n\nfrom sklearn.linear_model import Ridge\n\n# Features (X) and target (y)\nX = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]]\ny = df[\"final_score\"]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=0\n)\n\nprint(f\"Training set: {len(X_train)} students\")\nprint(f\"Test set: {len(X_test)} students\")\n\nTraining set: 225 students\nTest set: 75 students\n\n\nUnderstanding the code:\nImporting the model: - from sklearn.linear_model import Ridge imports the Ridge Regression model from scikit-learn\nSeparating features and target: - X = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]] creates a table with just our input features (the four columns we’ll use to make predictions). By convention, we use capital X for features. - y = df[\"final_score\"] extracts just the target column (what we want to predict). By convention, we use lowercase y for the target.\nSplitting the data: - train_test_split(X, y, test_size=0.25, random_state=0) splits our data into four pieces - test_size=0.25 means 25% of the data goes to testing, 75% to training - random_state=0 makes the split reproducible (same split every time) - X_train, X_test, y_train, y_test captures the four results: training features, test features, training targets, and test targets - With 300 students total, this gives us about 225 students for training and 75 for testing\n\n\nStep 3: Train a Ridge model\nNow let’s train a single Ridge model to see how the basic workflow works. We’ll use alpha=1.0, which is scikit-learn’s default value. This represents moderate regularization: not too weak (which would allow overfitting) and not too strong (which would oversimplify the model).\n\n# Create and train a Ridge model\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\n# Evaluate on both training and test sets\ntrain_r2 = ridge.score(X_train, y_train)\ntest_r2 = ridge.score(X_test, y_test)\n\nprint(f\"Training R²: {train_r2:.3f}\")\nprint(f\"Test R²: {test_r2:.3f}\")\nprint(f\"\\nLearned weights: {ridge.coef_}\")\nprint(f\"Intercept: {ridge.intercept_:.2f}\")\n\nTraining R²: 0.791\nTest R²: 0.776\n\nLearned weights: [0.03404973 0.19636421 0.2986387  0.5035881 ]\nIntercept: -2.90\n\n\nUnderstanding the code:\nCreating and training the model: - ridge = Ridge(alpha=1.0) creates a Ridge model with alpha=1.0 (moderate regularization) - ridge.fit(X_train, y_train) trains the model on the training data. This is where the model learns the optimal weights.\nEvaluating the model: - ridge.score(X_train, y_train) calculates the R² score on the training data (how well does it fit the data it learned from?) - ridge.score(X_test, y_test) calculates the R² score on the test data (how well does it work on new students it has never seen?)\nExamining what was learned: - ridge.coef_ shows the weights learned for each feature - ridge.intercept_ shows the constant (b) added to predictions\nUnderstanding the output:\nR² scores: - Training R²: 0.791 means the model explains 79.1% of the variation in the training data - Test R²: 0.776 means the model explains 77.6% of the variation in the test data - Both scores are reasonably high (closer to 1.0 is better) - The scores are close together (difference of only 0.015), which indicates the model generalizes well\nLearned weights: - The four weights correspond to our four features: attendance, homework_rate, quiz_avg, and exam_avg - [0.034, 0.196, 0.299, 0.504] shows how much each feature contributes to the prediction - Notice these roughly match our true formula (0.2 homework, 0.3 quiz, 0.5 exam) - Attendance has a small weight (0.034) even though we did not use it in the formula. This happens because Ridge finds small correlations in the data.\nIntercept: - -2.90 is the constant added to every prediction - This adjusts the baseline of the predictions to match the scale of the final scores\nWhat this tells us: The model is working well. It has learned weights that are close to the true relationship, and it performs similarly on both training and test data (no overfitting)."
  },
  {
    "objectID": "assignments/week-03/week-3-demo.html#part-2-logistic-regression-for-classification",
    "href": "assignments/week-03/week-3-demo.html#part-2-logistic-regression-for-classification",
    "title": "Week 3 Demo: Ridge and Logistic Regression",
    "section": "Part 2: Logistic Regression for Classification",
    "text": "Part 2: Logistic Regression for Classification\n\nUnderstanding Logistic Regression\nAs we discussed in the introduction, Logistic Regression is a linear model for classification tasks. Despite its name, it’s actually a classification algorithm, not a regression algorithm. It’s called “regression” for historical reasons, but it predicts categories (like pass/fail) rather than continuous numbers. Like Ridge Regression, Logistic Regression learns weights for each feature and combines them in a formula. However, it uses these weights differently to make categorical predictions.\nHow does it work? Logistic Regression combines features with weights just like Ridge:\nscore = w₁ × attendance + w₂ × homework_rate + w₃ × quiz_avg + w₄ × exam_avg + b\nBut instead of using this score directly as a prediction, it converts the score into a probability between 0 and 1. If the probability is above 0.5, it predicts “pass”; otherwise, it predicts “fail”.\nWhat makes Logistic Regression different from Ridge: - Ridge predicts numbers (regression); Logistic predicts categories (classification) - Ridge uses R² as its performance metric; Logistic uses accuracy (percentage of correct predictions) - Ridge returns the weighted sum directly; Logistic converts it to a probability first\n\nThe complexity control (C)\nLogistic Regression has a parameter called C that controls regularization, similar to Ridge’s alpha. However, C works in the opposite direction. The formula structure is always the same (a weighted sum of features), but C determines whether those weights can be large or must stay small.\nWhat are we predicting and why do weights matter? We’re predicting pass/fail, where a student passes if their final score is 70 or above. Remember from our data creation that final scores are based on: 0.2 × homework + 0.3 × quiz + 0.5 × exam. This means exam scores matter most for passing, quizzes matter moderately, and homework matters least. Attendance wasn’t even in the formula, so it shouldn’t matter at all.\nA well-trained Logistic Regression model should learn weights that reflect this reality: - exam_avg should get the largest weight (most important for passing) - quiz_avg should get a moderate weight - homework_rate should get a small weight - attendance should get a weight close to zero (it doesn’t actually affect passing)\nHow does C affect these weights?\nWith small C (like 0.1): Strong regularization forces all weights to be small and similar in size. The model might learn weights like [0.01, 0.02, 0.03, 0.02], where all features have roughly equal, small influence. This means the model can’t distinguish that exam scores matter more than attendance, even though they clearly do. The model is too simple to capture the true pattern.\nWith large C (like 100): Weak regularization allows weights to vary widely. The model might learn weights like [0.1, 0.8, 2.3, 5.1], correctly recognizing that exam (5.1) matters much more than attendance (0.1). But if the weights get too large, the model becomes overly confident and sensitive to small changes, which can cause problems with new data.\nWith moderate C (like 1.0): The model balances these extremes, learning weights that reflect the true importance of features without becoming overly sensitive.\nWhen we create a Logistic Regression model, we pass C as an argument:\nlogreg = LogisticRegression(C=0.1)    # Small C: strong regularization, forces small weights\nlogreg = LogisticRegression(C=100)    # Large C: weak regularization, allows large weights\nThe .fit() method finds the best weights within the constraint set by C. Notice that C works backwards from Ridge’s alpha: higher C means less regularization (more complex), while higher alpha meant more regularization (simpler). For this demo, we’ll use C=1.0, which is scikit-learn’s default and represents moderate regularization.\n\n\n\nStep 1: Prepare classification data\nWe’ll use the same dataset and features as Part 1, but this time we’re predicting pass/fail categories instead of numeric scores. This is the same train/test split process we’ve used before.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# Use pass/fail as the target\ny_class = df[\"pass_fail\"]\n\n# Features stay the same\nX_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n    X, y_class, test_size=0.25, random_state=0\n)\n\nprint(f\"Training set: {len(X_train_class)} students\")\nprint(f\"Test set: {len(X_test_class)} students\")\n\nTraining set: 225 students\nTest set: 75 students\n\n\nUnderstanding the code:\nImporting the model: - from sklearn.linear_model import LogisticRegression imports the Logistic Regression classifier from scikit-learn - Notice it’s in the linear_model module, just like Ridge, because it’s also a linear model\nSelecting the classification target: - y_class = df[\"pass_fail\"] extracts the pass/fail column as our target - This is categorical data (two classes: “pass” and “fail”) rather than numeric data\nSplitting the data: - We use the same train_test_split function with the same parameters - The only difference from Part 1 is we’re using y_class (categories) instead of y (numbers) - We add _class to our variable names to keep track of which data is for classification\n\n\nStep 2: Train a Logistic Regression model\nNow let’s train a single Logistic Regression model to see how the basic workflow works. We’ll use C=1.0, which is scikit-learn’s default value and represents moderate regularization.\n\n# Create and train a Logistic Regression model\nlogreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0)\nlogreg.fit(X_train_class, y_train_class)\n\n# Evaluate on both training and test sets\ntrain_acc = logreg.score(X_train_class, y_train_class)\ntest_acc = logreg.score(X_test_class, y_test_class)\n\nprint(f\"Training accuracy: {train_acc:.3f}\")\nprint(f\"Test accuracy: {test_acc:.3f}\")\nprint(f\"\\nLearned weights: {logreg.coef_[0]}\")\nprint(f\"Intercept: {logreg.intercept_[0]:.2f}\")\n\nTraining accuracy: 0.867\nTest accuracy: 0.907\n\nLearned weights: [-0.00720698  0.05896261  0.10066204  0.19035106]\nIntercept: -24.03\n\n\nUnderstanding the code:\nCreating and training the model: - logreg = LogisticRegression(C=1.0, max_iter=1000, random_state=0) creates a Logistic Regression model - C=1.0 uses the default regularization strength - max_iter=1000 sets the maximum number of iterations for the optimization algorithm. The model will stop earlier if it finds the best weights, but this ensures it has enough time for difficult problems. - random_state=0 ensures reproducible results - logreg.fit(X_train_class, y_train_class) trains the model on the training data. This is where the model learns the optimal weights.\nEvaluating the model: - logreg.score(X_train_class, y_train_class) calculates accuracy on the training data (what fraction of training predictions are correct?) - logreg.score(X_test_class, y_test_class) calculates accuracy on the test data (what fraction of test predictions are correct?) - Accuracy is simpler than R². It’s just the percentage of correct predictions.\nExamining what was learned: - logreg.coef_[0] shows the weights learned for each feature (we use [0] because Logistic stores weights in a 2D array for multi-class problems) - logreg.intercept_[0] shows the constant (b) added to the score\nUnderstanding the output:\nAccuracy scores: - Training accuracy: 0.867 means the model correctly predicted pass/fail for 86.7% of the training students - Test accuracy: 0.907 means the model correctly predicted pass/fail for 90.7% of the test students - Both scores are high (closer to 1.0 is better) - The test accuracy is actually slightly higher than training accuracy, which is unusual but can happen with smaller datasets. This suggests the model is not overfitting.\nLearned weights: - The four weights are: [-0.007, 0.059, 0.101, 0.190] - These correspond to: attendance, homework_rate, quiz_avg, and exam_avg - Notice that exam_avg has the largest weight (0.190), which makes sense because exam scores are worth 50% of the final score in our data - quiz_avg has the second largest weight (0.101), matching its 30% contribution - homework_rate has a smaller weight (0.059), matching its 20% contribution - attendance has a weight near zero (-0.007), which makes sense because we did not use attendance when creating final scores\nThe model correctly learned which features actually matter for predicting pass/fail.\nIntercept: - -24.03 is the constant added to the weighted sum before converting to a probability - This large negative value shifts the decision boundary. It means the weighted sum needs to be fairly positive (above 24) before the model predicts “pass” with high confidence.\nWhat this tells us: The model has learned a sensible decision boundary. The weights reflect the true importance of each feature in determining pass/fail, with exam scores mattering most and attendance mattering least."
  },
  {
    "objectID": "assignments/week-03/week-3-demo.html#summary-three-supervised-learning-algorithms",
    "href": "assignments/week-03/week-3-demo.html#summary-three-supervised-learning-algorithms",
    "title": "Week 3 Demo: Ridge and Logistic Regression",
    "section": "Summary: Three Supervised Learning Algorithms",
    "text": "Summary: Three Supervised Learning Algorithms\nWe’ve now seen three different supervised learning algorithms across two weeks: kNN, Ridge Regression, and Logistic Regression.\n\nWhat We Learned This Week\nIn this demo, we focused on the basic workflow for Ridge and Logistic Regression: - How to train the models with default parameters - How to interpret the output (R² scores, accuracy, learned weights) - That Ridge learns weights showing feature importance for regression tasks - That Logistic learns weights showing feature importance for classification tasks - That both use the same train/test evaluation framework as kNN\n\n\nAlgorithm Comparison\nEach algorithm has a “complexity parameter” that controls how simple or complex the model is:\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nTask\nComplexity Parameter\nDefault Value\nSimpler Model\nMore Complex Model\n\n\n\n\nkNN (Week 2)\nClassification or Regression\nn_neighbors\n5\nLarge k\nSmall k\n\n\nRidge Regression (Week 3)\nRegression\nalpha\n1.0\nLarge alpha\nSmall alpha\n\n\nLogistic Regression (Week 3)\nClassification\nC\n1.0\nSmall C\nLarge C\n\n\n\nNote: Ridge’s alpha and Logistic’s C work in opposite directions. Higher alpha means more regularization (simpler), but higher C means less regularization (more complex). This takes practice to remember.\n\n\nKey Takeaways\n\nAll three algorithms use the same workflow: split data into train/test, train the model, evaluate performance\nRidge and Logistic learn weights that show which features are important; kNN does not\nRidge predicts numbers (regression), Logistic predicts categories (classification)\nDefault parameters (alpha=1.0, C=1.0) often work well for initial exploration\nYou can examine learned weights to understand what the model thinks is important\n\n\n\nFurther Reading in the Textbook\nThis demo showed you the basic workflow for Ridge and Logistic Regression. The textbook (pages 45-67) covers important topics we didn’t demonstrate here:\n\nParameter tuning: How to systematically try different values of alpha and C to find the best model\nOverfitting and underfitting: Examples showing what happens when regularization is too weak or too strong\nLearning curves: Visualizations showing how model performance changes with different parameter values\nLasso Regression: Another regularized linear model that can automatically select important features\nWhen to use each model: Detailed guidance on choosing between Linear Regression, Ridge, and Lasso\n\n\n\nLooking Ahead\nNext week (Week 4), we’ll explore Decision Trees and ensemble methods like Random Forests. These algorithms work completely differently from kNN and linear models, but we’ll use the same train/test framework to evaluate them."
  },
  {
    "objectID": "assignments/week-02/week-2-demo.html",
    "href": "assignments/week-02/week-2-demo.html",
    "title": "Week 2 Demo: Classification vs Regression Targets",
    "section": "",
    "text": "Download Jupyter Notebook\nThis page builds a tiny student-performance dataset to demonstrate the core ideas Chapter 2 introduces."
  },
  {
    "objectID": "assignments/week-02/week-2-demo.html#build-a-tiny-dataset",
    "href": "assignments/week-02/week-2-demo.html#build-a-tiny-dataset",
    "title": "Week 2 Demo: Classification vs Regression Targets",
    "section": "Build a Tiny Dataset",
    "text": "Build a Tiny Dataset\nThis cell creates a tiny, fake dataset so we can focus on the idea of inputs (features) and outputs (targets) without needing a real file yet. By the end of the cell, we’ll have a table (df) where each row represents one student.\n\nimport numpy as np\nimport pandas as pd\n\nrng = np.random.default_rng(0)\nn = 12\n\ndf = pd.DataFrame({\n    \"attendance\": rng.integers(60, 101, n),\n    \"homework_rate\": rng.integers(50, 101, n),\n    \"quiz_avg\": rng.integers(40, 101, n),\n    \"exam_avg\": rng.integers(40, 101, n),\n})\n\n# numeric target (regression)\ndf[\"final_score\"] = (0.2*df[\"homework_rate\"] + 0.3*df[\"quiz_avg\"] + 0.5*df[\"exam_avg\"]).round(0)\n\n# categorical target (classification)\ndf[\"pass_fail\"] = np.where(df[\"final_score\"] &gt;= 70, \"pass\", \"fail\")\n\ndf\n\n\n\n\n\n\n\n\nattendance\nhomework_rate\nquiz_avg\nexam_avg\nfinal_score\npass_fail\n\n\n\n\n0\n94\n75\n64\n44\n56.0\nfail\n\n\n1\n86\n80\n92\n58\n73.0\npass\n\n\n2\n80\n99\n73\n69\n76.0\npass\n\n\n3\n71\n87\n42\n65\n62.0\nfail\n\n\n4\n72\n82\n86\n64\n74.0\npass\n\n\n5\n61\n77\n84\n41\n61.0\nfail\n\n\n6\n63\n78\n91\n40\n63.0\nfail\n\n\n7\n60\n97\n50\n47\n58.0\nfail\n\n\n8\n67\n64\n45\n40\n46.0\nfail\n\n\n9\n93\n91\n92\n80\n86.0\npass\n\n\n10\n86\n84\n41\n72\n65.0\nfail\n\n\n11\n97\n50\n73\n79\n71.0\npass\n\n\n\n\n\n\n\nLoading the tools we need\nThe first two lines load two libraries:\n\nNumPy (np): used here for generating random numbers and doing simple numeric operations.\npandas (pd): used to create and display a table called a DataFrame (think: spreadsheet in Python).\n\nMaking the randomness repeatable\nNext, the cell creates a random number generator:\n\nrng is a “random number generator” object we’ll use to create fake attendance and scores.\nThe 0 is a seed, which makes the results repeatable. That means if you run the notebook again, you’ll get the same random dataset each time (useful for teaching and debugging).\n\nChoosing how many students to generate\n\nn = 12 means we’re generating 12 rows, so our dataset will represent 12 students.\nIf we changed n to 100, we’d generate 100 students instead.\n\nBuilding the DataFrame (our dataset table)\nThe DataFrame is created with four columns that represent student features (inputs):\n\nattendance\nhomework_rate\nquiz_avg\nexam_avg\n\nEach column is filled with random whole numbers in a chosen range. Those ranges are just meant to look realistic (for example, attendance between about 60 and 100).\nSo at this point:\n\neach row = one student\neach column = one measured input about that student\n\nCreating a numeric target for regression\nNext the cell creates a new column: and * final_score is calculated as a weighted combination of homework, quizzes, and exams. * This is meant to imitate how a course grade might be computed (exams count more than quizzes, etc.). * This column is a number, so it can be used as a regression target (predict a numeric value).\nCreating a label target for classification\nThen the cell creates another new column:\n\npass_fail turns the numeric final_score into a category label: \"pass\" or \"fail\".\nThis column is a classification target, because the output must be one of a fixed set of options.\n\nDisplaying the result\nThe last line tells Jupyter/Quarto to display df, so you can see the dataset you just created as a table.\nIn supervised learning, we organize our data in a table:\n\nEach row is one example (here, one student).\nThe columns split into:\n\nX (features): the input information we use to make a prediction\n(attendance, homework_rate, quiz_avg, exam_avg)\ny (target/label): the output we want to predict\n\n\nIn our demo, we keeping X the same, and we show two choices for y:\n\npass_fail is a classification target (the model chooses a label from {pass, fail})\nfinal_score is a regression target (the model predicts a numeric value)"
  },
  {
    "objectID": "assignments/week-02/week-2-demo.html#review-the-difference-in-targets",
    "href": "assignments/week-02/week-2-demo.html#review-the-difference-in-targets",
    "title": "Week 2 Demo: Classification vs Regression Targets",
    "section": "Review the Difference in Targets",
    "text": "Review the Difference in Targets\nThis section zooms in on the target column for classification: pass_fail. The goal is to make the idea of a classification target visible: it isn’t a wide range of numbers — it’s a small set of labels.\n\nClassification target (labels)\n\nCounting how many of each label we have\nThis first line counts how many students fall into each class:\n\ndf[\"pass_fail\"] selects the pass_fail column from the DataFrame.\n.value_counts() counts how many times each label appears.\nThe output should look like a small summary table showing something like:\n\nhow many students are \"pass\"\nhow many students are \"fail\"\n\n\nThis is useful because classification targets often come in classes, and it helps to know whether your dataset has a reasonable spread of those classes (for example, not all pass and none fail).\n\ndf[\"pass_fail\"].value_counts()\n\npass_fail\nfail    7\npass    5\nName: count, dtype: int64\n\n\n\n\nMaking the label counts visual with a bar chart\nThe next cell turns those same counts into a picture.\n\nimport matplotlib.pyplot as plt loads the plotting library we’ll use to draw the chart.\ndf[\"pass_fail\"].value_counts().plot(kind=\"bar\") does two steps at once:\n\nit recomputes the counts of \"pass\" vs \"fail\"\nit plots them as a bar chart\n\nplt.title(...) adds a title so the chart clearly communicates what it represents.\nplt.xlabel(\"label\") labels the horizontal axis: the category names (pass and fail).\nplt.ylabel(\"count\") labels the vertical axis: how many students are in each category.\nplt.show() displays the plot.\n\nThe key takeaway from this plot: a classification target is made up of labels from a fixed set of options, and we can summarize it by counting how many examples fall into each label.\n\nimport matplotlib.pyplot as plt\n\ndf[\"pass_fail\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Classification target: pass/fail\")\nplt.xlabel(\"label\")\nplt.ylabel(\"count\")\nplt.show()\n\n\n\n\n\n\n\n\nNow that we’ve looked at the classification target as label counts, we’ll look at the regression target as numeric values.\n\n\n\nRegression target (numeric)\nThis next cell looks at the regression target column: final_score. The goal is to show that regression targets are numeric values, so we summarize them using numeric statistics and distributions.\n\nA quick numeric summary (describe)\nThe first line:\n\nselects the final_score column\nproduces a standard summary of numeric data\n\nThe output includes common summary statistics like:\n\nhow many values there are (count)\nthe average (mean)\nspread (std)\nminimum and maximum (min, max)\nand percentile cutoffs (like 25%, 50%, 75%)\n\n\ndf[\"final_score\"].describe()\n\ncount    12.000000\nmean     65.916667\nstd      10.672465\nmin      46.000000\n25%      60.250000\n50%      64.000000\n75%      73.250000\nmax      86.000000\nName: final_score, dtype: float64\n\n\n\n\nA histogram to show the distribution of scores\nThe next cell visualizes the same column as a histogram.\n\nThe histogram groups numeric values into bins (ranges) and counts how many students fall into each range.\nbins=8 controls how many ranges the score axis is divided into. More bins means narrower ranges; fewer bins means wider ranges.\n\nThen the plot labels make it readable:\n\nthe title explains what you’re looking at\nthe x-axis label (“score”) tells you the values are numeric\nthe y-axis label (“count”) tells you we’re counting how many students fall into each score range\n\n\ndf[\"final_score\"].plot(kind=\"hist\", bins=8)\nplt.title(\"Regression target: final score\")\nplt.xlabel(\"score\")\nplt.ylabel(\"count\")\nplt.show()"
  },
  {
    "objectID": "assignments/week-02/week-2-demo.html#generalization-train-vs-test-split",
    "href": "assignments/week-02/week-2-demo.html#generalization-train-vs-test-split",
    "title": "Week 2 Demo: Classification vs Regression Targets",
    "section": "Generalization: Train vs Test Split",
    "text": "Generalization: Train vs Test Split\nThis section sets up the train/test split so we can check whether a model would work on new students, not just the students in our dataset. We do that by holding back some rows as a test set and using the rest as a training set. By the end of this section, you’ll have X_train, X_test, y_train, and y_test—two groups of students that let us measure generalization later.\n\nStep 1: Choose X (features) and y (target)\n\nfrom sklearn.model_selection import train_test_split\n\n# X = input features (what we use to predict)\nX = df[[\"attendance\", \"homework_rate\", \"quiz_avg\", \"exam_avg\"]]\n\n# y = target (what we want to predict)\ny = df[\"pass_fail\"]\n\nImporting the split tool\n\nfrom sklearn.model_selection import train_test_split imports a helper function from scikit-learn.\nWe’ll use it to split our rows into a training set and a test set in a consistent way.\n\nDefining X (features)\n\nX = df[[...]] selects the feature columns from the DataFrame.\nWe are choosing the same four inputs we’ve been using throughout the demo:\n\nattendance\nhomework_rate\nquiz_avg\nexam_avg\n\n\nThis creates a new table X that contains only the input information.\nDefining y (target)\n\ny = df[\"pass_fail\"] selects the target column we want to predict.\nHere we are choosing classification as our task, so y is the label column (pass or fail).\n\nAt this point, we have separated the dataset into:\n\nX: what we use to predict\ny: what we want to predict\n\n\n\nStep 2: Split the rows into training and test sets\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=0\n)\n\nprint(\"Total rows:\", len(df))\nprint(\"Training rows:\", len(X_train))\nprint(\"Test rows:\", len(X_test))\n\nTotal rows: 12\nTraining rows: 9\nTest rows: 3\n\n\nSplitting X and y together\n\ntrain_test_split(X, y, ...) splits the dataset into two groups of rows.\nWe split X and y at the same time so that each feature row stays matched with its correct label.\n\nThis gives us four outputs:\n\nX_train: the feature rows in the training set\nX_test: the feature rows in the test set\ny_train: the labels that go with the training rows\ny_test: the labels that go with the test rows\n\nChoosing how large the test set is\n\ntest_size=0.25 means 25% of the rows go into the test set.\nWith 12 total students, this will usually mean about 3 students in the test set and 9 in training.\n\nMaking the split repeatable\n\nrandom_state=0 makes the split repeatable.\nWithout it, the split could change each time you run the notebook, which makes it harder to compare results.\n\nPrinting the sizes\nThe print(...) lines show how many rows ended up in each group so you can confirm the split happened."
  },
  {
    "objectID": "assignments/week-02/week-2-demo.html#knn",
    "href": "assignments/week-02/week-2-demo.html#knn",
    "title": "Week 2 Demo: Classification vs Regression Targets",
    "section": "kNN",
    "text": "kNN\nNow that we have a training set and a test set, we can train a model on the training students and see how well it generalizes to the test students.\nWe’ll use k-nearest neighbors (kNN) for classification. kNN predicts the label for a new student by finding the k closest students in the training set and using a majority vote. Changing k changes how flexible the model is.\n\nStep 1: Train one kNN model and check train vs test performance\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\nprint(\"Training accuracy:\", knn.score(X_train, y_train))\nprint(\"Test accuracy:\", knn.score(X_test, y_test))\n\nTraining accuracy: 1.0\nTest accuracy: 1.0\n\n\n\nKNeighborsClassifier(...) creates a kNN classifier.\nn_neighbors=3 sets k = 3, meaning the model looks at the 3 nearest training students when it predicts a label.\nknn.fit(X_train, y_train) trains the model using the training set.\n\nFor kNN, “training” mostly means storing the training data so the model can look up neighbors later.\n\nknn.score(...) reports accuracy: the fraction of predictions that match the true labels.\n\nThe training accuracy is how well it predicts the training students.\nThe test accuracy is how well it predicts the held-back test students.\n\n\nThis is our first concrete look at generalization: test accuracy is the estimate of how well this model works on new students.\n\n\nStep 2: Watch what happens when we change k\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmax_k = len(X_train)  # k can't be bigger than the number of training rows\nks = range(1, max_k + 1)\n\ntrain_scores = []\ntest_scores = []\n\nfor k in ks:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    train_scores.append(knn.score(X_train, y_train))\n    test_scores.append(knn.score(X_test, y_test))\n\n\nresults = pd.DataFrame({\n    \"k\": list(ks),\n    \"train_acc\": train_scores,\n    \"test_acc\": test_scores\n})\nresults.round(3)\n\n\n\n\n\n\n\n\nk\ntrain_acc\ntest_acc\n\n\n\n\n0\n1\n1.000\n1.000\n\n\n1\n2\n1.000\n0.667\n\n\n2\n3\n1.000\n1.000\n\n\n3\n4\n1.000\n0.667\n\n\n4\n5\n1.000\n1.000\n\n\n5\n6\n0.667\n0.333\n\n\n6\n7\n0.667\n0.333\n\n\n7\n8\n0.667\n0.333\n\n\n8\n9\n0.667\n0.333\n\n\n\n\n\n\n\nImporting the model\n\nfrom sklearn.neighbors import KNeighborsClassifier imports the kNN classification model from scikit-learn.\nWe import it inside this cell so the cell works even if it’s run on its own.\n\nChoosing the k values we’re allowed to test\n\nmax_k = len(X_train) gets the number of rows in the training set.\nFor kNN, k cannot be larger than the number of training examples, because the model can’t look for “10 nearest neighbors” if you only have 9 training students.\nks = range(1, max_k + 1) creates the list of k values we’ll test. If max_k is 9, then this produces k values from 1 through 9.\n\nCreating containers to store our results\n\ntrain_scores = [] creates an empty list that will store training accuracy for each k.\ntest_scores = [] creates an empty list that will store test accuracy for each k.\n\nThese lists start empty, and we’ll fill them as we loop.\nLooping over k values\n\nfor k in ks: starts a loop. That means we will run the indented code once for each k value.\n\nInside the loop:\n\nknn = KNeighborsClassifier(n_neighbors=k) creates a new kNN model using the current k value.\nknn.fit(X_train, y_train) trains the model on the training set. For kNN, this mainly means the model stores the training data so it can compare new points to it later.\ntrain_scores.append(knn.score(X_train, y_train)) calculates accuracy on the training set and adds it to the train_scores list.\ntest_scores.append(knn.score(X_test, y_test)) calculates accuracy on the test set and adds it to the test_scores list.\n\nSo after the loop finishes:\n\ntrain_scores[i] is the training accuracy for k = ks[i]\ntest_scores[i] is the test accuracy for k = ks[i]\n\nDisplaying the results\n\nWe build a small table called results with three columns: k, train_acc, and test_acc.\npd.DataFrame({...}) creates the table by lining up each k value with its training accuracy and test accuracy.\nresults.round(3) rounds the accuracy values to three decimals so the table is easier to read.\nThe final line displays the table so we can quickly compare training vs test accuracy for each k before plotting.\n\nAt this stage, the important thing is not the exact numbers—it’s that we now have a way to compare training performance vs test performance as k changes.\n\n\nStep 3: Plot train vs test accuracy as k changes\n\nplt.plot(list(ks), train_scores, label=\"training accuracy\")\nplt.plot(list(ks), test_scores, label=\"test accuracy\")\nplt.xlabel(\"k (number of neighbors)\")\nplt.ylabel(\"accuracy\")\nplt.title(\"kNN: training vs test accuracy as k changes\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nPlotting the training accuracy line\n\nplt.plot(list(ks), train_scores, label=\"training accuracy\") draws a line graph of training accuracy.\nlist(ks) provides the x-values (the k values we tested).\ntrain_scores provides the y-values (the training accuracy for each k).\nlabel=\"training accuracy\" gives this line a name so it can appear in the legend.\n\nPlotting the test accuracy line\n\nplt.plot(list(ks), test_scores, label=\"test accuracy\") draws a second line on the same plot for test accuracy.\nIt uses the same x-values (k), but a different set of y-values (test_scores).\nThis lets us compare training and test performance at the same k values.\n\nLabeling the axes\n\nplt.xlabel(\"k (number of neighbors)\") labels the horizontal axis so it’s clear what the x-values represent.\nplt.ylabel(\"accuracy\") labels the vertical axis so it’s clear we’re measuring accuracy (from 0 to 1).\n\nAdding a title\n\nplt.title(\"kNN: training vs test accuracy as k changes\") adds a title that summarizes what the plot is showing.\n\nAdding a legend\n\nplt.legend() displays a legend box that matches each line to its label (“training accuracy” vs “test accuracy”).\nWithout this, you’d see two lines but wouldn’t know which is which.\n\nDisplaying the plot\n\nplt.show() tells Python to render the chart.\n\nThis plot is valuable because it shows how changing k changes model behavior:\n\nWhen k is small, the model can fit the training data very closely, so training accuracy is often high.\nAs k increases, the model becomes less flexible, so training accuracy usually drops.\nThe test accuracy is the estimate of generalization, and we watch how it changes as k changes.\n\nBecause our dataset is tiny, the test line may jump around, but the key idea still holds: k controls how complex the model is, and training vs test accuracy helps us see overfitting vs underfitting."
  },
  {
    "objectID": "assignments/week-01/index.html",
    "href": "assignments/week-01/index.html",
    "title": "Week 1 Assignment (CMSC-2208)",
    "section": "",
    "text": "Submission location: All items are submitted in D2L (Week 1 dropboxes). Reading: Chapter 1.\n\nPart 1: Reading\nRead all of Chapter 1 of Introduction to Machine Learning with Python (Müller & Guido).\nPay special attention to:\n\nEssential libraries and tools (what the course uses and why)\nKey terms used in the chapter: dataset, features (X), target/labels (y), training set, test set\nThe chapter’s first workflow idea: split data → train a model → evaluate results\n\n\n\nPart 2: Jupyter Notebook Verification\n\nA) Launch Jupyter Notebook in the correct folder\nFrom Anaconda Prompt / Miniconda Prompt:\n\nActivate the cmsc-2208 environment\n\nconda activate cmsc-2208\nIf you are using a new terminal window, you must activate the environment again.\n\nNavigate to the course-work folder\n\ncd C:\\cmsc-2208\\course-work\n\nStart jupyter notebook\n\njupyter notebook\nA browser tab will open showing the contents of C:\\cmsc-2208\\course-work\\.\n\n\nB) Create a week01 folder and a verification notebook\n\nIn the Jupyter file list, create a folder named: week01\nOpen the week01 folder\nCreate a new notebook\nSave the notebook as: lastname-nb-verification.ipynb\n\n\n\nC) Run these cells (in order)\nimport sys\nsys.executable\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport scipy\nimport sklearn\n# Confirm that a built-in dataset can be loaded\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX = iris[\"data\"]\ny = iris[\"target\"]\nX.shape, y.shape\n# Confirm that train/test splitting works (a core workflow step)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nX_train.shape, X_test.shape\n\n\n\nPart 3: D2L quiz\nComplete the D2L Chapter 1 quiz.\n\n\nPart 4: Video reflection (graded)\n\nVideo submission instructions\nRecord a short video of yourself explaining your understanding of the Week 1 workflow. Focus on explaining concepts clearly and using correct terminology rather than reading text from the screen.\nIn your video, address:\n\nWhat feels different from running .py scripts in an editor compared to running code in a notebook\nWhat the kernel is (in the context of jupyter notebooks)\nWhy “restart the kernel” and “run all” are useful habits\nOne concept from Chapter 1 you can explain accurately\n\nRequirements\n\nClear video and audio quality\nIntro (required): Start your video by saying: “Hello, my name is [Your Name]. This is the [Assignment Name] for CMSC 2208.”\nName your video file: lastname_Week01Reflection\nUpload your video to Kaltura\nDo not embed the video in the Dropbox submission\nPaste the share link into the D2L assignment text submission box\nMake sure the link text includes lastname_Week01Reflection in the link name (this will appear if you copy the link)\n\n\n\n\nD2L submission checklist\nSubmit the following items to the Week 1 D2L dropboxes.\n\nVideo submission (link only)\n\nVideo filename: lastname_Week01Reflection\nSubmit: Paste the share link into the D2L text submission box (do not embed the video in the Dropbox)\nSubmit your lastname-nb-verification.ipynb file"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "assignments/week-02/index.html",
    "href": "assignments/week-02/index.html",
    "title": "Week 2 Assignments",
    "section": "",
    "text": "Submission location: No Dropbox submission this week. All work is completed in D2L."
  },
  {
    "objectID": "assignments/week-02/index.html#week-2-assignments",
    "href": "assignments/week-02/index.html#week-2-assignments",
    "title": "Week 2 Assignments",
    "section": "",
    "text": "Submission location: No Dropbox submission this week. All work is completed in D2L."
  },
  {
    "objectID": "assignments/week-02/index.html#part-1-reading",
    "href": "assignments/week-02/index.html#part-1-reading",
    "title": "Week 2 Assignments",
    "section": "Part 1: Reading",
    "text": "Part 1: Reading\nRead the assigned section of Chapter 2 on k-nearest neighbors (kNN).\nAs you read, focus on these ideas (these are also the ideas the quiz will test):\n\nSupervised learning structure: how datasets are organized as X (features) and y (target/label)\nClassification vs regression: what changes when y is a label vs y is a number\nGeneralization: why we care about performance on new, unseen data\nTraining set vs test set: what each one is used for and why we split the data\nOverfitting vs underfitting: two common ways models fail to generalize\nkNN mechanics (conceptually): neighbors, distance, and majority vote\nThe role of k: how changing k changes model behavior and can affect training vs test performance"
  },
  {
    "objectID": "assignments/week-02/index.html#part-2-week-2-demo",
    "href": "assignments/week-02/index.html#part-2-week-2-demo",
    "title": "Week 2 Assignments",
    "section": "Part 2: Week 2 Demo",
    "text": "Part 2: Week 2 Demo\nUse the demo to reinforce the Chapter 2 vocabulary:\n\nseeing X and y in a concrete dataset\nseeing a train/test split\nseeing training accuracy vs test accuracy\nseeing how changing k affects results"
  },
  {
    "objectID": "assignments/week-02/index.html#part-3-d2l-quiz",
    "href": "assignments/week-02/index.html#part-3-d2l-quiz",
    "title": "Week 2 Assignments",
    "section": "Part 3: D2L Quiz",
    "text": "Part 3: D2L Quiz\nComplete the Week 2 D2L quiz.\nThe quiz focuses on:\n\nidentifying X vs y in a small scenario\nclassification vs regression (based on what y represents)\nwhat “generalization” means in the book’s terms\nwhat training vs test accuracy mean (and what each one evaluates)\nrecognizing overfitting vs underfitting patterns (training high/test low vs both low)\nkNN basics: what neighbors are and what “vote” means\nwhat happens at extreme k values (very small vs very large)"
  },
  {
    "objectID": "assignments/week-02/index.html#completion-checklist",
    "href": "assignments/week-02/index.html#completion-checklist",
    "title": "Week 2 Assignments",
    "section": "Completion checklist",
    "text": "Completion checklist\n\nChapter 2 reading (kNN section)\nWeek 2 D2L quiz"
  },
  {
    "objectID": "assignments/week-03/index.html",
    "href": "assignments/week-03/index.html",
    "title": "Week 3 Assignments",
    "section": "",
    "text": "Here is the link to the Week 3 demo.\n Download Jupyter Notebook",
    "crumbs": [
      "Home",
      "Weekly Assignments",
      "Week 3"
    ]
  },
  {
    "objectID": "assignments/week-03/index.html#part-1-reading",
    "href": "assignments/week-03/index.html#part-1-reading",
    "title": "Week 3 Assignments",
    "section": "Part 1: Reading",
    "text": "Part 1: Reading\nRead Chapter 2, pages 45-70 (Linear Models section, including Naive Bayes).\nThis section introduces linear models for regression and classification, including Ridge Regression, Lasso, LinearRegression, LogisticRegression, LinearSVC, and Naive Bayes classifiers. The demo provides a hands-on walkthrough of Ridge and Logistic Regression with detailed explanations of how regularization works and how to interpret learned weights.",
    "crumbs": [
      "Home",
      "Weekly Assignments",
      "Week 3"
    ]
  },
  {
    "objectID": "assignments/week-03/index.html#part-2-work-through-the-week-3-demo",
    "href": "assignments/week-03/index.html#part-2-work-through-the-week-3-demo",
    "title": "Week 3 Assignments",
    "section": "Part 2: Work through the Week 3 Demo",
    "text": "Part 2: Work through the Week 3 Demo\nThe Week 3 demo covers:\n\nUnderstanding what linear models are and how they differ from kNN\nTraining Ridge Regression models for predicting numeric values\nUnderstanding regularization and the alpha parameter\nInterpreting R² scores and learned weights\nTraining Logistic Regression models for classification\nUnderstanding the C parameter and how it differs from alpha\nEvaluating classification accuracy and interpreting learned weights\nComparing Ridge and Logistic Regression with kNN",
    "crumbs": [
      "Home",
      "Weekly Assignments",
      "Week 3"
    ]
  },
  {
    "objectID": "assignments/week-03/index.html#part-3-d2l-quiz",
    "href": "assignments/week-03/index.html#part-3-d2l-quiz",
    "title": "Week 3 Assignments",
    "section": "Part 3: D2L Quiz",
    "text": "Part 3: D2L Quiz\nComplete the Week 3 D2L quiz (Linear Models concepts).\nThe quiz covers:\n\nCore concepts: linear prediction formulas, regularization (L1 vs L2), overfitting vs underfitting\nRidge Regression: alpha parameter, R² scores, when coefficients shrink toward zero\nLasso: L1 regularization, sparse models, automatic feature selection\nLogistic Regression: C parameter, decision boundaries, one-vs-rest multiclass approach\nNaive Bayes: variant selection (Gaussian, Bernoulli, Multinomial), smoothing parameter\nModel interpretation: coefficient meanings, learning curves, method chaining in scikit-learn",
    "crumbs": [
      "Home",
      "Weekly Assignments",
      "Week 3"
    ]
  },
  {
    "objectID": "assignments/week-03/index.html#d2l-submission-checklist",
    "href": "assignments/week-03/index.html#d2l-submission-checklist",
    "title": "Week 3 Assignments",
    "section": "D2L submission checklist",
    "text": "D2L submission checklist\nComplete the following in D2L:\n\nWeek 3 D2L quiz (no file uploads required this week)\n\nNote: This week focuses on reading Chapter 2 (pages 45-70), understanding the demo, and demonstrating comprehension through the quiz. There are no screenshot or coding submissions this week. The emphasis is on understanding how linear models work, how they differ from kNN, and how regularization controls model complexity.",
    "crumbs": [
      "Home",
      "Weekly Assignments",
      "Week 3"
    ]
  },
  {
    "objectID": "assignments/week-04/index.html",
    "href": "assignments/week-04/index.html",
    "title": "Week 4 Assignment (CMSC-2208)",
    "section": "",
    "text": "Submission location: All items are submitted in D2L (Week 4 dropbox).\n\nPart 1: Case Study 1 — Predicting Student Grades with Ridge Regression\n\nBackground\nA university wants to predict students’ final course grades to identify students who may need additional support. They collected data from 200 students in an introductory course. For each student, they recorded four features:\n\nattendance: percentage of classes attended (0-100)\nhomework_rate: percentage of homework assignments completed (0-100)\nquiz_avg: average quiz score (0-100)\nexam_avg: average exam score (0-100)\n\nThe target variable is final_grade, a numeric score from 0 to 100.\nA data scientist split the data into 150 training students and 50 test students, then trained Ridge Regression models to predict final grades.\n\n\nModel Results\nModel 1: Ridge(alpha=1.0)\n\n\n\nMetric\nValue\n\n\n\n\nTraining R²\n0.82\n\n\nTest R²\n0.79\n\n\n\nLearned Coefficients:\n\n\n\nFeature\nWeight\n\n\n\n\nattendance\n0.08\n\n\nhomework_rate\n0.15\n\n\nquiz_avg\n0.31\n\n\nexam_avg\n0.52\n\n\nintercept\n-4.2\n\n\n\nModel 2: Ridge(alpha=0.01)\n\n\n\nMetric\nValue\n\n\n\n\nTraining R²\n0.92\n\n\nTest R²\n0.58\n\n\n\nModel 3: Ridge(alpha=100)\n\n\n\nMetric\nValue\n\n\n\n\nTraining R²\n0.45\n\n\nTest R²\n0.43\n\n\n\n\n\nCase Study 1 Questions\nAs you prepare for your video reflection, consider the following:\n\nThis is a regression task. How can you tell from the problem description?\nLooking at Model 1’s coefficients, which feature has the strongest influence on predicted grades? Which has the weakest? What does this suggest about what the model learned?\nWhat does R² measure? How do you interpret Model 1’s Training R² of 0.82 and Test R² of 0.79?\nCompare Models 1, 2, and 3. Which model is overfitting? Which is underfitting? How can you tell from the training and test scores?\nExplain how alpha controls model complexity in Ridge Regression. What happens to the coefficients when alpha is very large? What happens when alpha is very small?\n\n\n\n\nPart 2: Case Study 2 — Predicting Customer Purchases with Logistic Regression\n\nBackground\nAn online retailer wants to predict which customers are likely to make a purchase so they can target marketing efforts. They collected data from 1,000 past customers. For each customer, they recorded four features:\n\nage: customer’s age in years\nincome: annual income (in thousands of dollars)\nwebsite_visits: number of times the customer visited the website in the past month\nprevious_purchases: number of past purchases from the company\n\nThe target variable is purchase, a binary label: “buy” or “no buy.”\nA data scientist split the data into 750 training customers and 250 test customers, then trained Logistic Regression models to predict purchases.\n\n\nModel Results\nThree models were trained with different C values:\n\n\n\nModel\nC\nTraining Accuracy\nTest Accuracy\n\n\n\n\nModel A\n0.01\n0.68\n0.67\n\n\nModel B\n1.0\n0.78\n0.76\n\n\nModel C\n100\n0.91\n0.72\n\n\n\nCoefficients from Model B (C=1.0):\n\n\n\nFeature\nCoefficient\n\n\n\n\nage\n-0.02\n\n\nincome\n0.45\n\n\nwebsite_visits\n0.38\n\n\nprevious_purchases\n0.61\n\n\nintercept\n-3.1\n\n\n\n\n\nCase Study 2 Questions\nAs you prepare for your video reflection, consider the following:\n\nThis is a classification task. How can you tell from the problem description?\nWhat does accuracy measure for a classification model? How is it different from R²?\nCompare Models A, B, and C. Which model is overfitting? Which is underfitting? How can you tell?\nExplain how the C parameter affects model complexity in Logistic Regression. How does C work differently from alpha in Ridge Regression?\nLooking at Model B’s coefficients, which feature is most strongly associated with making a purchase? The coefficient for age is negative (-0.02). What does a negative coefficient mean for the prediction?\nIf you had to choose one model to deploy for predicting new customer purchases, which would you choose? Why?\n\n\n\n\nPart 3: Scenario C — Email Spam Detection (Reference Only)\n\nBackground\nA company wants to automatically filter spam emails. They have 50,000 emails labeled as “spam” or “not spam.” For each email, they extracted 200 features including word frequencies, sender information, and formatting characteristics.\nNo model results are provided for this scenario. Use it as a reference for the video reflection.\n\n\n\nPart 5: Video Reflection\nThis video reflection is the primary deliverable for this assignment. You will demonstrate your understanding of linear models by discussing the case studies above. Focus on explaining concepts clearly and using correct terminology rather than reading from a script.\n\nVideo submission instructions\nRecord a video in which you address the four sections below. Organize your video to cover each section in order.\n\n\nSection A: Linear Model Foundations\n\nExplain what a linear model does. Describe the general prediction formula and what the weights and intercept represent.\nExplain the difference between classification and regression. Use Case Study 1 and Case Study 2 as examples.\n\n\n\nSection B: Case Study 1 Discussion (Ridge Regression)\n\nWhich of the three Ridge models would you recommend? Justify your choice using the training and test R² scores.\nLooking at Model 1’s coefficients, explain what they tell you about which features matter for predicting grades.\nExplain how the alpha parameter affects model complexity. Connect this to the overfitting and underfitting patterns you observed.\n\n\n\nSection C: Case Study 2 Discussion (Logistic Regression)\n\nWhich of the three Logistic Regression models would you recommend? Justify your choice using the training and test accuracy scores.\nLooking at Model B’s coefficients, explain what they tell you—including what the negative coefficient for age means.\nExplain how the C parameter affects model complexity. How does C work differently from alpha?\n\n\n\nSection D: Comparing Linear Models to kNN\n\nExplain why linear models are considered more interpretable than kNN. What can you learn from examining coefficients that you cannot learn from a kNN model?\nConsider Scenario C (email spam detection with 200 features). Would kNN or a linear model be a better starting point for this problem? Why?\n\n\n\n\nD2L Submission Checklist\nSubmit the following items to the Week 4 D2L dropbox.\n\nVideo submission (link only)\n\nVideo filename: lastname_Week04LinearModels\nUpload your video to Kaltura\nDo not embed the video in the Dropbox submission\nPaste the share link into the D2L text submission box\nMake sure the link text includes lastname_Week04LinearModels in the link name",
    "crumbs": [
      "Home",
      "Weekly Assignments",
      "Week 4 (Current Week)"
    ]
  },
  {
    "objectID": "course/schedule.html",
    "href": "course/schedule.html",
    "title": "CMSC 2208 Course Schedule",
    "section": "",
    "text": "Weekly module opens: Monday 8:00 AM\nAssignments due: Sunday 11:30 PM\nWork cadence:\n\nMon–Thu: work through reading + practice notebook\nSun: graded items due (when assigned)\n\nSpring break: Mar 9–13, 2026\nFinal project block: Apr 27–May 10, 2026\nFinal project due: May 11, 2026",
    "crumbs": [
      "Home",
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#course-rhythm",
    "href": "course/schedule.html#course-rhythm",
    "title": "CMSC 2208 Course Schedule",
    "section": "",
    "text": "Weekly module opens: Monday 8:00 AM\nAssignments due: Sunday 11:30 PM\nWork cadence:\n\nMon–Thu: work through reading + practice notebook\nSun: graded items due (when assigned)\n\nSpring break: Mar 9–13, 2026\nFinal project block: Apr 27–May 10, 2026\nFinal project due: May 11, 2026",
    "crumbs": [
      "Home",
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "course/schedule.html#schedule",
    "href": "course/schedule.html#schedule",
    "title": "CMSC 2208 Course Schedule",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nWeek\nTopics (with chapter/section anchors)\nDeliverable\nReading (Müller & Guido)\n\n\n\n\n1 (Jan 12–Jan 18)\nCourse onboarding + “What is ML?” + environment setup + notebook workflow\nWeek 1 Verification Assignment (setup + screenshots + video) + D2L Quiz 1\nCh. 1\n\n\n2 (Jan 19–Jan 25)\nFirst supervised model + scikit-learn workflow: kNN\nD2L Quiz 2\nCh. 2.1.1 (kNN)\n\n\n3 (Jan 26–Feb 01)\nLinear models: classification + regression\nD2L Quiz 3\nCh. 2.2 (Linear Models)\n\n\n4 (Feb 02–Feb 08)\nTrees + ensembles: random forests / gradient boosting\nSkill Check 1 (autograded): train + report results on game stats + D2L Quiz 4\nCh. 2.3 (Decision Trees) + 2.3.2 (Ensembles)\n\n\n5 (Feb 09–Feb 15)\nClassifier outputs and confidence: decision function vs predicted probabilities\nReflection 1 (video): “Using uncertainty to compare models” + D2L Quiz 5\nCh. 2.4: Uncertainty Estimates from Classifiers\n\n\n6 (Feb 16–Feb 22)\nPreprocessing & scaling workflow: transformations and applying them correctly\nPractice Notebook 3 (completion): “Scale game stats correctly (train vs test)” + D2L Quiz 6\nCh. 3.3.1–3.3.4\n\n\n7 (Feb 23–Mar 01)\nDimensionality reduction + visualization: PCA and t-SNE (“player map”)\nPractice Notebook 4 (completion): “Build a player map (PCA / t-SNE)” + D2L Quiz 7\nCh. 3.4.1 (PCA) + 3.4.3 (t-SNE) (NMF optional)\n\n\n8 (Mar 02–Mar 08)\nClustering: k-means, agglomerative, DBSCAN, and how to compare clusters\nPractice Notebook 5 (completion): “Cluster players by playstyle” + D2L Quiz 8\nCh. 3.5.1–3.5.5\n\n\n9 (Mar 09–Mar 15)\nSpring Break (no due dates)\n—\n—\n\n\n10 (Mar 16–Mar 22)\nRepresenting data + feature engineering (one-hot, binning, interactions)\nPractice Notebook 6 (completion): “Engineer features for matchmaking” + D2L Quiz 9\nCh. 4\n\n\n11 (Mar 23–Mar 29)\nModel evaluation: train/test vs cross-validation + metrics beyond accuracy\nPractice Notebook 7 (completion): “Evaluate models beyond accuracy” + D2L Quiz 10\nCh. 5 (evaluation + metrics)\n\n\n12 (Mar 30–Apr 05)\nModel selection + tuning: grid search + CV workflow\nSkill Check 2 (autograded): metrics + CV + grid search summary + D2L Quiz 11\nCh. 5 (model selection / grid search)\n\n\n13 (Apr 06–Apr 12)\nPipelines: preprocessing + model as one reproducible workflow\nPractice Notebook 8 (completion): “Build a full pipeline for prediction” + D2L Quiz 12\nCh. 6\n\n\n14 (Apr 13–Apr 19)\nPipelines + tuning together (grid search inside pipeline; avoid leakage)\nSkill Check 3 (autograded): pipeline + grid search + report best parameters + D2L Quiz 13\nCh. 6\n\n\n15 (Apr 20–Apr 26)\nText data intro (chat/reviews) or capstone prep emphasis\nReflection 2 / Project Proposal (graded): dataset + goal + metric + plan + D2L Quiz 14\nCh. 7 (+ Ch. 8 possible)\n\n\n16 (Apr 27–May 03)\nFinal project work time (milestone: clean data + baseline + evaluation plan)\n— (work week)\nAs needed (review Ch. 4–6)\n\n\n17 (May 04–May 10)\nFinal project work time (milestone: tuned model + results + narrative + video)\n— (work week)\nAs needed\n\n\n\n\nFinal project (Apr 27–May 11)\nDue May 11, 2026: notebook report (pipeline + evaluation + interpretation) + short video walkthrough.",
    "crumbs": [
      "Home",
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Use this site as your course handbook. It stays up to date all semester.\n\nUse the left sidebar to find weekly materials and guides.\nD2L is the starting point for announcements, grades, and submissions.\nCourse information (syllabus, schedule, policies, office hours) is maintained here.\n\n\nCourse workflow\n\nCheck D2L for announcements, deadlines, and any updates.\nRead the Weekly Overview on this site (Week 1, Week 2, …) and follow the links provided.\nComplete the work (assignments) and submit in D2L.\n\n\n\nSupport\n\nOffice hours: see the Office Hours page.\nZoom: Office Zoom Link\n\nHow to get help:\n\nUse Discord/email for course questions.\nAdd troubleshooting policy.\n\n\n\n\nInstructor contact\n\nInstructor: Joseph Silman\n\nEmail: jsilman@sctcc.edu\nPhone Number: (320) 308 - 6595\n\n\n\nQuick links\n\nSyllabus\nSchedule"
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "",
    "text": "Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\nby Keith Galli via freeCodeCamp — A comprehensive 1-hour tutorial covering NumPy from basics to advanced topics. Covers array creation, indexing techniques, mathematical operations, statistics, and practical data manipulation. Perfect for seeing all the concepts in action."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#complete-numpy-overview-comprehensive",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#complete-numpy-overview-comprehensive",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "",
    "text": "Complete Python NumPy Tutorial (Creating Arrays, Indexing, Math, Statistics, Reshaping)\nby Keith Galli via freeCodeCamp — A comprehensive 1-hour tutorial covering NumPy from basics to advanced topics. Covers array creation, indexing techniques, mathematical operations, statistics, and practical data manipulation. Perfect for seeing all the concepts in action."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#numpy-arrays-and-array-creation",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#numpy-arrays-and-array-creation",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "NumPy Arrays and Array Creation",
    "text": "NumPy Arrays and Array Creation\nNumPy Arrays | Python Tutorial\nby Corey Schafer — Clear introduction to NumPy arrays, why they’re faster than lists, and how to create and work with 1D and 2D arrays. Explains the fundamental differences between NumPy arrays and Python lists.\n\nCreating NumPy Arrays\nby Socratica — Focused explanation of different methods for creating NumPy arrays, including np.array(), np.zeros(), np.ones(), np.arange(), and np.linspace(). Shows when to use each method."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#array-indexing-and-slicing",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#array-indexing-and-slicing",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "Array Indexing and Slicing",
    "text": "Array Indexing and Slicing\nNumPy Array Indexing and Slicing\nby Corey Schafer — Detailed walkthrough of how to access and modify array elements, rows, and columns. Emphasizes the critical difference between views and copies in NumPy slicing."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#boolean-indexing-filtering-arrays-with-conditions",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#boolean-indexing-filtering-arrays-with-conditions",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "Boolean Indexing (Filtering Arrays with Conditions)",
    "text": "Boolean Indexing (Filtering Arrays with Conditions)\nNumPy Boolean Arrays and Masking | Python Tutorial\nby Corey Schafer — Explains how to use boolean indexing to filter data based on conditions. Covers creating boolean masks, using comparison operators, and combining multiple conditions with & and |.\n\nBoolean Indexing in NumPy\nby Enthought — Focused tutorial on boolean indexing patterns, showing how to replace loops with vectorized conditional selection. Demonstrates practical filtering use cases."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#vectorization-and-array-operations",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#vectorization-and-array-operations",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "Vectorization and Array Operations",
    "text": "Vectorization and Array Operations\nNumPy Vectorization Explained\nby Python Programmer — Clear explanation of what vectorization means and why NumPy operations are faster than loops. Shows side-by-side comparisons of loop-based vs. vectorized approaches."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#statistical-operations-and-the-axis-parameter",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#statistical-operations-and-the-axis-parameter",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "Statistical Operations and the Axis Parameter",
    "text": "Statistical Operations and the Axis Parameter\nNumPy Array Methods and Axis Argument\nby Tech With Tim — Explains NumPy’s statistical methods (mean(), sum(), std(), etc.) and thoroughly clarifies how the axis parameter works in multidimensional arrays. Essential for understanding aggregations."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#data-types-dtypes-and-memory",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#data-types-dtypes-and-memory",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "Data Types (dtypes) and Memory",
    "text": "Data Types (dtypes) and Memory\nUnderstanding NumPy Data Types\nby Python Engineer — Explains what dtypes are, why they matter for performance and memory usage, and how to work with different data types in NumPy arrays."
  },
  {
    "objectID": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#random-number-generation",
    "href": "shared/guides/cmsc-1217/week-03/cmsc-1217-chap-04-learning-resources.html#random-number-generation",
    "title": "Week 3: Chapter 4 Additional Reference Videos (NumPy Arrays, Indexing, Boolean Masking, Vectorization)",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nRandom Numbers in NumPy\nby Keith Galli — Covers NumPy’s random number generation functions, including uniform and normal distributions, random integers, and the importance of setting seeds for reproducibility."
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "",
    "text": "These are optional “extra help” videos for Week 3.\nWatch them when the reading feels abstract or when you want a second explanation of the same concept.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#how-to-use-these-videos",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#how-to-use-these-videos",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "",
    "text": "These are optional “extra help” videos for Week 3.\nWatch them when the reading feels abstract or when you want a second explanation of the same concept.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#what-are-linear-models",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#what-are-linear-models",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "What Are Linear Models?",
    "text": "What Are Linear Models?\nLinear Regression, Clearly Explained!!!\nby StatQuest (Josh Starmer) (2017, ~27 min) — The foundation of all linear models. Shows how models find a “line of best fit” and introduces key concepts like least squares, R², and prediction formulas.\nNote: This video is longer than usual but provides essential foundation for understanding Ridge and Logistic Regression.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#understanding-r²-r-squared",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#understanding-r²-r-squared",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "Understanding R² (R-squared)",
    "text": "Understanding R² (R-squared)\nR-squared, Clearly Explained!!!\nby StatQuest (Josh Starmer) (2017, ~11 min) — What R² measures and why values closer to 1.0 mean better predictions. Explains “variation explained” and how to interpret Ridge regression performance.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#logistic-regression-for-classification",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#logistic-regression-for-classification",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "Logistic Regression for Classification",
    "text": "Logistic Regression for Classification\nLogistic Regression\nby StatQuest (Josh Starmer) (2018, ~9 min) — How linear models work for classification by converting weighted sums to probabilities. Explains the S-curve, prediction thresholds, and the difference between logistic and linear regression.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#ridge-regression-and-regularization",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#ridge-regression-and-regularization",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "Ridge Regression and Regularization",
    "text": "Ridge Regression and Regularization\nRegularization Part 1: Ridge (L2) Regression\nby StatQuest (Josh Starmer) (2018, ~20 min) — Why we add penalties to keep weights small and prevent overfitting. Explains the alpha parameter, how Ridge differs from regular linear regression, and when to use regularization.\n\nRegularization Part 2.5: Ridge vs Lasso Regression, Visualized!!!\nby StatQuest (Josh Starmer) (2019, ~9 min) — Visual explanation of why Ridge shrinks coefficients toward zero but never exactly to zero, while Lasso can set coefficients to exactly zero. Helps clarify Ridge behavior from the textbook.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#summary",
    "href": "shared/guides/cmsc-2208/week-03/cmsc-2208-chap-02-learining-resources-week-3.html#summary",
    "title": "Week 3: Chapter 2 Learning Videos (Linear Models: Ridge & Logistic Regression)",
    "section": "Summary",
    "text": "Summary\nAll videos this week are from StatQuest with Josh Starmer. His videos align well with the scikit-learn focused approach in your textbook and demo.\nIf you want to explore beyond these core topics, check out Josh Starmer’s website at statquest.org/video-index.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 3",
      "Chapter 2 Week 3 Reference Videos"
    ]
  },
  {
    "objectID": "shared/guides/setup/jupyter-learning-links.html",
    "href": "shared/guides/setup/jupyter-learning-links.html",
    "title": "Jupyter Notebooks & JupyterLab Learning Resources",
    "section": "",
    "text": "Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\nby Corey Schafer (2016, ~30 min) — Comprehensive beginner overview of installation, navigation, cells, kernels, and Markdown.\n\nJupyter Notebooks Tutorial | How to use them & tips and tricks!\nby AssemblyAI (2022, ~18 min) — Practical guide to common settings, basic operations, cell management, indicators, keyboard shortcuts, and useful tips for data science workflows.\n\nJupyter Notebook Complete Beginner Guide - From Jupyter to JupyterLab, Google Colab and Kaggle!\nby Rob Mulla (2022, ~25 min) — Covers classic Notebooks, transition to JupyterLab, and cloud-based alternatives.\n\nGetting Started with Jupyter Notebooks in VS Code\nby Visual Studio Code (2024, ~6 min) — Official guide to creating, running, and debugging Jupyter Notebooks directly in VS Code, including environment setup, code execution, Markdown support, variable exploration, and data visualization.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Jupyter Learning Links"
    ]
  },
  {
    "objectID": "shared/guides/setup/jupyter-learning-links.html#introduction-to-jupyter-notebooks",
    "href": "shared/guides/setup/jupyter-learning-links.html#introduction-to-jupyter-notebooks",
    "title": "Jupyter Notebooks & JupyterLab Learning Resources",
    "section": "",
    "text": "Jupyter Notebook Tutorial: Introduction, Setup, and Walkthrough\nby Corey Schafer (2016, ~30 min) — Comprehensive beginner overview of installation, navigation, cells, kernels, and Markdown.\n\nJupyter Notebooks Tutorial | How to use them & tips and tricks!\nby AssemblyAI (2022, ~18 min) — Practical guide to common settings, basic operations, cell management, indicators, keyboard shortcuts, and useful tips for data science workflows.\n\nJupyter Notebook Complete Beginner Guide - From Jupyter to JupyterLab, Google Colab and Kaggle!\nby Rob Mulla (2022, ~25 min) — Covers classic Notebooks, transition to JupyterLab, and cloud-based alternatives.\n\nGetting Started with Jupyter Notebooks in VS Code\nby Visual Studio Code (2024, ~6 min) — Official guide to creating, running, and debugging Jupyter Notebooks directly in VS Code, including environment setup, code execution, Markdown support, variable exploration, and data visualization.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Jupyter Learning Links"
    ]
  },
  {
    "objectID": "shared/guides/setup/jupyter-learning-links.html#recent-developments-in-the-jupyter-ecosystem",
    "href": "shared/guides/setup/jupyter-learning-links.html#recent-developments-in-the-jupyter-ecosystem",
    "title": "Jupyter Notebooks & JupyterLab Learning Resources",
    "section": "Recent Developments in the Jupyter Ecosystem",
    "text": "Recent Developments in the Jupyter Ecosystem\nJupyter Frontends & JupyterLite updates | Jupyter Open Studio Day 2025\n(2025, ~30 min) — Overview of current features and updates from the official Jupyter community.\n\nAdditional resources and interactive examples are available on the official Jupyter website.",
    "crumbs": [
      "Home",
      "Guides",
      "Week 1",
      "Jupyter Learning Links"
    ]
  },
  {
    "objectID": "shared/policies/ai-policy.html",
    "href": "shared/policies/ai-policy.html",
    "title": "Using AI as a Learning Tool",
    "section": "",
    "text": "Purpose of this policy\nTools that use artificial intelligence are now widely available, and many of you already use them when learning technical material. This course allows the use of AI tools as a learning aid, similar to a tutor or reference source. At the same time, this course requires that submitted work reflect your own understanding and decision-making.\nThis page explains how AI tools may be used productively in this course, and where their use becomes inappropriate.\n\n\nAI as a learning tool\nUsing AI as a learning tool means using it to clarify ideas, terminology, or examples that you do not yet understand.\n\nIdeas An idea is a concept or principle discussed in the course materials, such as how a process works, why a particular approach is used, or what a tool is designed to do. Clarifying an idea means using AI to help you understand the purpose or reasoning behind something you have already encountered in class, rather than asking for a final answer.\nTerminology Terminology refers to the specific words and phrases used in technical subjects. These terms often have precise meanings that differ from everyday usage. Using AI appropriately includes asking for explanations of unfamiliar terms so you can understand how they are used in context, especially when reading textbooks, documentation, or assignment instructions.\nExamples Examples are demonstrations that show how an idea or concept is applied in practice. Using AI to clarify examples means asking for help understanding why an example works, what each part is doing, or how it relates to the underlying concept. It does not mean copying an example and submitting it as your own work.\n\nIn all cases, using AI as a learning tool should help you build understanding, not replace the effort of reading, experimenting, or reasoning through a problem yourself.\n\n\nAI as a crutch\nAI tools can support learning, but they can also be used in ways that reduce learning. This policy uses the term “crutch” to describe situations where the tool is doing the thinking and decision-making that the student is expected to practice.\nUsing AI as a crutch means relying on it to produce work or reasoning that you do not understand well enough to explain and defend. The issue is not that the tool was used; the issue is that the tool replaced the learning process.\nInappropriate uses include:\n\nAsking AI to complete an assignment or lab task for you Example: “Write the solution for this assignment.”\nSubmitting AI-generated code, answers, or explanations that you cannot explain in your own words Example: Copying an explanation of why a solution works without being able to describe the steps yourself.\nTreating AI output as authoritative without verification Example: Accepting an answer that conflicts with course materials, documentation, or results you can reproduce.\nUsing AI to bypass the learning objective Example: Asking for the final answer when the point of the activity is to practice reasoning, debugging, designing, or interpreting results.\nUsing AI to write or script reflections that are meant to describe your own understanding Example: Asking AI to generate a script or talking points for a reflection video instead of explaining the ideas in your own words.\n\nIn these cases, AI is being used to avoid the intended learning work rather than to support it.\n\n\nWhy this matters\nAI can produce output that appears correct even when it contains errors or is based on incorrect assumptions. Because of that, it is important to treat AI output as a starting point for learning, not as an authority.\nThis course emphasizes:\n\nverifying results,\nunderstanding workflows,\nand explaining decisions clearly.\n\nIf you rely on AI answers without testing them or understanding them, you are likely to miss errors and build misunderstandings. Those gaps tend to surface later in the course when tasks require independent problem solving, accurate interpretation, and clear explanations in graded work.\n\n\nExpectations for submitted work\nAll submitted work must reflect your own understanding.\nThis means:\n\nYou should be able to explain any code you submit.\nYou should be able to describe why you made particular choices (model, metric, preprocessing).\nIn reflections, your explanations should sound like you, not like a generic answer generator.\n\nYou are not required to disclose routine AI use for clarification or studying. However, if your work suggests that you do not understand what you submitted, that will be treated the same way as any other lack of understanding, regardless of the source.\nIf you use an AI tool, ask yourself:\n\nCould I explain this answer to another student without looking it up?\nDid I test or verify what the tool suggested?\nDid the tool help me understand, or did it just give me something to submit?\n\nIf the tool helped you understand, you are using it appropriately. If it replaced your thinking, you are not.",
    "crumbs": [
      "Home",
      "Course information",
      "AI Policy"
    ]
  },
  {
    "objectID": "shared/policies/homework-coursework.html",
    "href": "shared/policies/homework-coursework.html",
    "title": "Homework / Coursework policies",
    "section": "",
    "text": "Due dates and late work\n\nAll assignments must be submitted by the due date/time in D2L.\nLate work is not accepted. (“Late” = not in the dropbox by the due/end date.)\nSome assignments have multiple required components (video + file); missing any required component makes the assignment ineligible for grading.\nVideo work must be accessible at grading time or it is not accepted.\nStudents should archive/keep their submitted work.\nGroup work (if assigned) is not eligible for makeup.\n\n\n\nSubmission standards\n\nAssignment-specific submission standards will be provided in instructions.\nWork may be rejected or heavily penalized for not following standards, including:\n\nincorrect file format\nincorrect file naming\nnot following submission instructions\n\nStudents should contact the instructor before the due date with submission questions.\nAssignments will not be accepted via email.\n\n\n\nTests / quizzes (D2L)\n\nQuizzes/tests may be scheduled or unannounced; must be completed by the due date.\nPassword-protected test passwords may not be shared (integrity violation).\nMultiple attempts may be allowed, with thresholds:\n\nmust score &gt;60% on attempt 1 to unlock attempt 2\nmust score &gt;80% on attempt 2 to unlock attempt 3 (if offered)\n\nTime limits apply; exceeding the time limit may result in a 0 and block further work.\nStudents may not leave a quiz once started.\nAuto-generated quizzes may prevent backtracking; students are responsible for careful navigation/submission.\nNo make-up tests/quizzes.\n\n\n\nGrading practices (coursework-related)\n\nGrades are posted in D2L; students are responsible for verifying accuracy.\nSome activities may be assigned but not graded (self-review practice).\nRubrics are used for most activities; some items are auto-graded.\n“Proof” files may be provided; students are expected to review feedback/proof files.\nNo extra credit / no additional assignments offered.\nSome assignments may be pass/fail.\n\n\n\nVideo related coursework expectations\n\nSome assignments require webcam-based video submission (and possibly desktop recording).\nStudent must be visable in any video assignment or the assignment will not be graded.\nStudent must introduce themselves by name, assignment, and course at the start of any video submission - “Hello my name is [name], this is the [assignment] for [class]”\nStudents must dress appropriately/considerately for online video assignments.",
    "crumbs": [
      "Home",
      "Course information",
      "Assignment Policies"
    ]
  },
  {
    "objectID": "weekly-overview/week-01/index.html",
    "href": "weekly-overview/week-01/index.html",
    "title": "Week 1 Guide",
    "section": "",
    "text": "This week is about building the starting point for the semester. Chapter 1 introduces the language and workflow used throughout the book. Your focus is to understand what machine learning means in this course, what problems it is used for, and how the textbook organizes the steps from data to a trained model.\nChapter 1 of Introduction to Machine Learning with Python sets the foundation for the rest of the book by introducing the core vocabulary and framing used throughout. It explains what machine learning is used for, outlines the kinds of tasks the book focuses on, and emphasizes the importance of understanding your problem and your data before choosing a model. The chapter also introduces the main tools used in the course (especially scikit-learn and the supporting Python ecosystem) and shows a complete first example so you can see what an end-to-end machine learning task looks like in practice."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#week-1-focus",
    "href": "weekly-overview/week-01/index.html#week-1-focus",
    "title": "Week 1 Guide",
    "section": "",
    "text": "This week is about building the starting point for the semester. Chapter 1 introduces the language and workflow used throughout the book. Your focus is to understand what machine learning means in this course, what problems it is used for, and how the textbook organizes the steps from data to a trained model.\nChapter 1 of Introduction to Machine Learning with Python sets the foundation for the rest of the book by introducing the core vocabulary and framing used throughout. It explains what machine learning is used for, outlines the kinds of tasks the book focuses on, and emphasizes the importance of understanding your problem and your data before choosing a model. The chapter also introduces the main tools used in the course (especially scikit-learn and the supporting Python ecosystem) and shows a complete first example so you can see what an end-to-end machine learning task looks like in practice."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#how-this-course-approaches-machine-learning",
    "href": "weekly-overview/week-01/index.html#how-this-course-approaches-machine-learning",
    "title": "Week 1 Guide",
    "section": "How this course approaches machine learning",
    "text": "How this course approaches machine learning\nThis course approaches machine learning as a programming workflow carried out in Python. Chapter 1 reflects that approach by introducing machine learning through concrete tasks and examples, using the Python tool ecosystem and scikit-learn’s model interface. In Chapter 1, the authors focus on:\n\nframing a real-world question as a machine learning task,\nrepresenting the problem with a dataset (features and targets),\ntraining a model using scikit-learn,\nand checking performance using held-out test data.\n\nIn this course, you will primarily be asked to implement and evaluate models using existing tools, interpret what the results mean, and explain your choices using correct terminology. While mathematical ideas may appear as needed to support interpretation, the course emphasis is on building practical skill with the workflow and developing good habits for working with models and data."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#what-machine-learning-means-in-this-book",
    "href": "weekly-overview/week-01/index.html#what-machine-learning-means-in-this-book",
    "title": "Week 1 Guide",
    "section": "What machine learning means in this book",
    "text": "What machine learning means in this book\nChapter 1 introduces machine learning as extracting knowledge from data, and it motivates machine learning by contrasting it with systems built from hand-coded decision rules. The chapter uses examples such as spam filtering and face detection to show why rule-based approaches can break down, and how learning from data can be a practical alternative.\nTwo ideas are introduced early:\n\nMachine learning methods automate decision-making by generalizing from known examples, rather than relying on a fixed set of hand-built rules.\nThe quality of what a model can learn depends on what information is actually present in the dataset, especially in the features used to describe each example.\n\nFor that reason, Chapter 1 spends time defining how data is described in machine learning. It introduces the “table” framing (samples as rows and features as columns), names the key terms sample and feature, and emphasizes that no algorithm can make predictions using information that is not represented in the features."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#supervised-vs-unsupervised-learning",
    "href": "weekly-overview/week-01/index.html#supervised-vs-unsupervised-learning",
    "title": "Week 1 Guide",
    "section": "Supervised vs unsupervised learning",
    "text": "Supervised vs unsupervised learning\nChapter 1 distinguishes two broad categories of machine learning problems:\n\nsupervised learning, where you have input data along with a known outcome you want to predict\nunsupervised learning, where you have input data but no known outcomes provided to the algorithm\n\nThe chapter gives concrete examples of both kinds of tasks, and it emphasizes that supervised learning requires building a dataset that includes the desired outcome.\nChapter 1 then walks through a supervised learning example: predicting iris species from flower measurements. This example is used to introduce the vocabulary you will see repeatedly:\n\nsamples (rows/data points) and features (columns/properties)\ntargets/labels (the known outcomes in supervised learning)\ntraining data versus test data (evaluating on data not used to fit the model)\n\nYou do not need to memorize every detail yet. The goal is to recognize what makes a problem supervised, and to understand why the book spends time on data representation and evaluation before expanding into more algorithms in later chapters.\nNote: the formal “classification vs regression” distinction is introduced in Chapter 2, even though Chapter 1’s iris walkthrough is a classification example."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#datasets-features-and-targets",
    "href": "weekly-overview/week-01/index.html#datasets-features-and-targets",
    "title": "Week 1 Guide",
    "section": "Datasets, features, and targets",
    "text": "Datasets, features, and targets\nChapter 1 introduces the vocabulary the book uses to describe data in machine learning.\n\nA dataset is a collection of examples (also called samples or data points).\nEach example is described by features, which are measurable properties used as inputs to a model.\nIn supervised learning, each example also has a target value (often called a label). In classification problems, the possible target values are sometimes called classes.\n\nChapter 1 also describes datasets in a “table” form: samples are organized as rows and features as columns.\nThe book uses the notation X for the input data (the features) and y for the target values. You will see X and y throughout the text and in scikit-learn examples.\nThis week, focus on recognizing what X and y represent conceptually rather than worrying about exact shapes or implementation details."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#training-data-and-test-data",
    "href": "weekly-overview/week-01/index.html#training-data-and-test-data",
    "title": "Week 1 Guide",
    "section": "Training data and test data",
    "text": "Training data and test data\nChapter 1 introduces the distinction between training data and test data as part of its first complete machine learning example.\nThe chapter explains that:\n\na model is fit using one portion of the available data (the training set),\nand performance is evaluated using a separate portion of the data that was not used during training (the test set).\n\nThis separation allows you to check how well a model performs on new, unseen data, rather than just how well it fits the data it was trained on. Chapter 1 emphasizes that evaluating a model on the same data used for training can give an overly optimistic view of its performance.\nYou should understand why this separation is necessary, even though the mechanics of splitting data and evaluating models will be explored in more detail in later chapters."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#a-first-end-to-end-example",
    "href": "weekly-overview/week-01/index.html#a-first-end-to-end-example",
    "title": "Week 1 Guide",
    "section": "A first end-to-end example",
    "text": "A first end-to-end example\nChapter 1 includes a complete, small-scale machine learning example using the iris flower dataset. The purpose of this example is not to teach you every detail, but to show what an end-to-end machine learning task looks like in practice and to introduce the workflow the book will return to throughout the course.\nAs you read this example, focus on the sequence of steps:\n\nload a dataset,\nseparate features and targets,\ninspect the data (for example, with simple visualizations),\nsplit the data into training and test sets,\ntrain a model on the training data,\nevaluate the model on the test data.\n\nDo not worry if some parts feel unfamiliar at first. The goal in Week 1 is to recognize what each step is doing and why it appears in the workflow."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#tools-introduced-in-chapter-1",
    "href": "weekly-overview/week-01/index.html#tools-introduced-in-chapter-1",
    "title": "Week 1 Guide",
    "section": "Tools introduced in Chapter 1",
    "text": "Tools introduced in Chapter 1\nChapter 1 introduces the main Python libraries used throughout the book:\n\n\n\n\n\n\n\nTool\nRole in the book\n\n\n\n\nNumPy\nProvides efficient numerical arrays and supports many operations used by other libraries\n\n\npandas\nUsed for working with structured, tabular data\n\n\nmatplotlib\nUsed for basic visualization and inspecting data\n\n\nscikit-learn\nProvides machine learning algorithms and workflows\n\n\n\nYou are not expected to master these tools in Week 1. You are expected to recognize their names and understand why each one exists."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#reading-expectations-and-habits",
    "href": "weekly-overview/week-01/index.html#reading-expectations-and-habits",
    "title": "Week 1 Guide",
    "section": "Reading expectations and habits",
    "text": "Reading expectations and habits\nChapter 1 is short but dense. You should read it slowly and actively.\nWhen you encounter unfamiliar terms:\n\npause and reread the surrounding paragraph,\nconnect the term to the example being shown,\navoid memorizing definitions in isolation.\n\nThe goal is to build conceptual familiarity, not technical mastery.\nIf you compare your output to the book’s examples later, remember that small differences in formatting or values are normal. Focus on whether the ideas and results make sense, not whether they look identical."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#week-1-tasks",
    "href": "weekly-overview/week-01/index.html#week-1-tasks",
    "title": "Week 1 Guide",
    "section": "Week 1 tasks",
    "text": "Week 1 tasks\nThis week, you should:\n\nRead Chapter 1 of the textbook carefully.\nFollow the course setup instructions and complete the Week 1 verification assignment.\nComplete the D2L quiz for Chapter 1.\nReflect on how machine learning problems are structured in the book."
  },
  {
    "objectID": "weekly-overview/week-01/index.html#knowledge-goals-for-week-1",
    "href": "weekly-overview/week-01/index.html#knowledge-goals-for-week-1",
    "title": "Week 1 Guide",
    "section": "Knowledge goals for Week 1",
    "text": "Knowledge goals for Week 1\nBy the end of this week, you should be able to explain:\n\nWhat the book means by machine learning\nThe difference between classification and regression\nWhat a dataset, feature, and target are\nWhy data is split into training and test sets\nWhat the basic machine learning workflow looks like\nWhy data representation matters before algorithm choice"
  },
  {
    "objectID": "weekly-overview/week-03/index.html",
    "href": "weekly-overview/week-03/index.html",
    "title": "Week 3 Guide",
    "section": "",
    "text": "This week marks a shift from instance-based learning (kNN) to model-based learning with linear models. Last week, you learned about k-Nearest Neighbors, which makes predictions by finding similar examples in the training data. kNN stores all the training data and searches through it at prediction time, but it doesn’t learn a formula or extract patterns.\nLinear models work differently. They learn a formula during training—a weighted combination of features—and use that same formula to make predictions for any new data point. This approach has several advantages: the models are fast, they work well with high-dimensional data, and they can tell you which features matter most by examining the learned weights.\nThe Week 3 demo walks through two fundamental linear models: Ridge Regression (for predicting numbers like final scores) and Logistic Regression (for predicting categories like pass/fail). You’ll learn how regularization controls model complexity, how to interpret performance metrics (R² for regression, accuracy for classification), and how to understand which features the model thinks are important.\nWeek 3 Demo:",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 3"
    ]
  },
  {
    "objectID": "weekly-overview/week-03/index.html#chapter-2-concepts-used-this-week-pages-45-70",
    "href": "weekly-overview/week-03/index.html#chapter-2-concepts-used-this-week-pages-45-70",
    "title": "Week 3 Guide",
    "section": "Chapter 2 concepts used this week (pages 45-70)",
    "text": "Chapter 2 concepts used this week (pages 45-70)\n\nWhat you’ll practice in the demo vs. what you’ll learn from the textbook\nThe Week 3 demo teaches you the core workflow by working hands-on with two linear models: Ridge Regression (for predicting numbers) and Logistic Regression (for predicting categories). Once you understand these two, the textbook shows you several variations and extensions that follow the same principles.\nYou’ll code in the demo: - Ridge Regression (regression with L2 regularization) - Logistic Regression (classification with L2 regularization)\nYou’ll read about in the textbook: - Lasso (like Ridge, but uses L1 instead of L2) - ElasticNet (combines both L1 and L2) - LinearSVC (like LogisticRegression, but with a different optimization approach) - Naive Bayes (a different classification approach entirely)\nThe demo gives you the foundation; the textbook shows you the variations.\n\n\nLinear models and the prediction formula\nLinear models make predictions using a weighted sum of features plus an intercept: ŷ = w₀×x₀ + w₁×x₁ + ... + wₚ×xₚ + b. The weights (w) show how much each feature contributes to the prediction, and the intercept (b) adjusts the baseline. During training, the model learns the optimal weights by minimizing prediction errors on the training data.\nThis weighted formula is the foundation of all linear models. Whether you’re using Ridge, Lasso, LogisticRegression, or LinearSVC, they all make predictions using this same basic structure—they just differ in how they learn the weights and control complexity.\n\n\nUnderstanding L1 and L2 regularization\nRegularization means constraining the weights to prevent overfitting. There are two main types:\nL2 regularization (used by Ridge, LogisticRegression by default): - Penalizes the sum of squared coefficients: w₁² + w₂² + w₃² + … - Think: “Punish large weights extra hard” (squaring makes big values even bigger) - Effect: Shrinks all coefficients toward zero, but never makes them exactly zero - Spreads influence across all features rather than relying heavily on any one feature\nL1 regularization (used by Lasso): - Penalizes the sum of absolute values: |w₁| + |w₂| + |w₃| + … - Think: “Punish any nonzero weight equally” - Effect: Can drive coefficients exactly to zero (automatic feature selection) - Forces the model to pick its favorite features and ignore the rest\nWhy this matters: L2 (Ridge) keeps all features but shrinks their weights. L1 (Lasso) eliminates some features entirely by setting their weights to zero. Both prevent overfitting, but in different ways.\n\n\nRidge Regression and the alpha parameter\nRidge Regression is a linear model for regression tasks. It learns weights for each feature, but uses L2 regularization to keep weights small. The alpha parameter controls regularization strength: larger alpha forces weights toward zero (simpler model), smaller alpha allows larger weights (more complex model). Ridge helps prevent overfitting, especially with many features.\nThe demo walks you through training Ridge models, interpreting the learned weights, and understanding what the alpha parameter does. This hands-on experience with Ridge gives you the foundation for understanding Lasso and ElasticNet in the textbook.\n\n\nR² scores for regression\nRidge uses R² (R-squared) as its performance metric, returned by the .score() method. R² ranges from 0 to 1 and measures how much variation in the data the model explains. Higher is better, and you want training and test R² to be reasonably close together (if training R² is much higher than test R², you’re overfitting).\n\n\nLasso and sparse models\nLasso works just like Ridge—it’s a linear regression model with regularization—but uses L1 regularization instead of L2. The key practical difference: Lasso can set coefficients exactly to zero, effectively ignoring some features. This produces sparse models that use only a subset of features, which can make models easier to interpret. Lasso is useful when you expect only a few features to be important.\nSince you’ll understand Ridge from the demo (L2 regularization, alpha parameter, interpreting weights), Lasso will make immediate sense—it’s the same idea with L1 instead of L2.\n\n\nElasticNet: combining both approaches\nElasticNet combines both L1 and L2 penalties, giving you both the feature selection of Lasso and the stability of Ridge. It requires tuning two parameters (one for L1, one for L2), but often provides the best performance in practice. Think of it as a hybrid that lets you dial in how much feature selection (L1) vs. weight shrinkage (L2) you want.\n\n\nLogistic Regression for classification\nLogistic Regression is a linear model for classification (despite its name containing “regression”). It learns weights just like Ridge, but converts the weighted sum into a probability between 0 and 1. If the probability is above 0.5, it predicts one class; otherwise, the other.\nThe C parameter controls regularization, but works opposite to alpha: larger C means less regularization (more complex), smaller C means more regularization (simpler). This backward relationship takes practice to remember.\nThe demo walks you through training Logistic Regression models, interpreting accuracy scores and learned weights, and understanding how the C parameter affects model complexity.\n\n\nLinear decision boundaries\nLinear classification models create decision boundaries that are straight lines (in 2D), planes (in 3D), or hyperplanes (in higher dimensions). Any point on one side of the boundary gets one prediction, any point on the other side gets the opposite prediction. This seems restrictive, but works surprisingly well in high-dimensional spaces.\n\n\nLinearSVC: an alternative linear classifier\nLinearSVC is another linear classification algorithm that’s very similar to LogisticRegression. It also uses the weighted formula, the C parameter for regularization, and creates linear decision boundaries. The main difference is the mathematical approach used to find the optimal weights (support vector machines vs. logistic loss). In practice, they often perform similarly.\nSince you’ll understand LogisticRegression from the demo (C parameter, decision boundaries, multiclass classification), LinearSVC will be familiar—it’s a variation on the same theme.\n\n\nOne-vs-rest for multiclass problems\nMany linear classifiers are inherently binary (two classes). To handle multiple classes, scikit-learn uses one-vs-rest (OvR): it trains one binary classifier per class (that class vs. all others), then at prediction time picks whichever classifier is most confident. This results in one set of weights per class. The demo shows this briefly; the textbook provides more detail and visualizations.\n\n\nNaive Bayes classifiers\nNaive Bayes is a family of fast, simple classifiers that work differently from the linear models above. Instead of learning a weighted formula, Naive Bayes calculates probabilities based on simple statistics: it counts how often each feature appears with each class, then uses those counts to predict which class a new data point belongs to.\nThere are three variants, each suited to different data types: - GaussianNB for continuous features (assumes features follow a normal distribution per class) - BernoulliNB for binary features (counts how often each feature is 0 or 1 for each class) - MultinomialNB for count data (like word counts in text - tracks average feature values per class)\nThe alpha parameter controls “smoothing”—adding virtual data points to avoid zero probabilities. Naive Bayes models are very fast and work well even with limited data, making them excellent baseline models.\nWhy “naive”? They assume features are independent (knowing one feature doesn’t tell you about others), which is often not true but works surprisingly well anyway.\n\n\nInterpreting coefficients and weights\nOne advantage of linear models is interpretability: you can examine the learned weights to see which features matter most. Larger absolute values mean that feature has more influence. However, be cautious: coefficients can change (even flip sign) when features are correlated or when you change regularization strength. Coefficient interpretation works best when features are relatively independent.\n\n\nLearning curves and regularization\nLearning curves show model performance as training set size increases. They reveal whether you’re overfitting (training score much higher than test score) or underfitting (both scores low), and whether more data would help. Regularization becomes less critical as dataset size increases—with enough data, regularized and unregularized models converge to similar performance.",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 3"
    ]
  },
  {
    "objectID": "weekly-overview/week-03/index.html#reading-expectations-for-week-3",
    "href": "weekly-overview/week-03/index.html#reading-expectations-for-week-3",
    "title": "Week 3 Guide",
    "section": "Reading expectations for Week 3",
    "text": "Reading expectations for Week 3\nAs you read Chapter 2 (pages 45-67) and work through the demo, check whether you can explain the following in your own words:\n\nHow do linear models make predictions? What does the formula look like?\nWhat is the difference between kNN (Week 2) and linear models (Week 3)?\nWhat is regularization, and why do we use it?\nHow does the alpha parameter in Ridge affect model complexity?\nHow does the C parameter in Logistic Regression affect model complexity?\nWhat is the difference between L1 regularization (Lasso) and L2 regularization (Ridge)?\nWhat does R² measure, and how do you interpret it?\nWhat is a decision boundary, and what shape does it have for linear classifiers?\nHow does one-vs-rest extend binary classifiers to multiple classes?\nWhich Naive Bayes variant should you use for which type of data?\nHow do you interpret the learned weights (coefficients) in a linear model?\nWhy might coefficient interpretations be misleading when features are correlated?",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 3"
    ]
  },
  {
    "objectID": "weekly-overview/week-03/index.html#week-3-tasks",
    "href": "weekly-overview/week-03/index.html#week-3-tasks",
    "title": "Week 3 Guide",
    "section": "Week 3 tasks",
    "text": "Week 3 tasks\n\nRead Chapter 2, pages 45-70 (Linear Models section, including Naive Bayes).\nWork through the Week 3 demo in your Jupyter environment.\nComplete the Week 3 D2L quiz (Linear Models concepts).",
    "crumbs": [
      "Home",
      "Weekly Overview",
      "Week 3"
    ]
  }
]